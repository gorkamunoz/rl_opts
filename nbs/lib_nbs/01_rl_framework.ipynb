{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classic version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook gathers the functions creating the RL framework proposed in our work. Namely, it can be use to generate both the foraging environment as well as the agents moving on them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp rl_framework.legacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Class that defines the foraging environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import numpy as np\n",
    "\n",
    "from rl_opts.utils import isBetween_c_Vec, coord_mod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4958686188975374"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.rand()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class TargetEnv():\n",
    "    def __init__(self,\n",
    "                 Nt,\n",
    "                 L,\n",
    "                 r,\n",
    "                 lc,\n",
    "                 agent_step = 1,\n",
    "                 boundary_condition = 'periodic',\n",
    "                 num_agents = 1,\n",
    "                 high_den = 5,\n",
    "                 destructive = False):\n",
    "        \n",
    "        \"\"\"Class defining the foraging environment. It includes the methods needed to place several agents to the world.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        Nt: int \n",
    "            Number of targets.\n",
    "        L: int\n",
    "            Size of the (squared) world.\n",
    "        r: int \n",
    "            Radius with center the target position. It defines the area in which agent detects the target.\n",
    "        lc: int\n",
    "            Cutoff length. Displacement away from target (to implement revisitable targets by displacing agent away from the visited target).\n",
    "        agent_step: int, optional \n",
    "            Displacement of one step. The default is 1.\n",
    "        boundary_conditions: str, optional\n",
    "            If there is an ensemble of agents, this is automatically set to periodic. The default is 'periodic'. \n",
    "        num_agents: int, optional \n",
    "            Number of agents that forage at the same time. The default is 1.\n",
    "        high_den: int, optional\n",
    "            Number of agents from which it is considered high density. Useful for the case with num_agents >> 1. The default is 5. \n",
    "        destructive: bool, optional\n",
    "            True if targets are destructive. The default is False.\n",
    "        \"\"\"\n",
    "        self.Nt = Nt\n",
    "        self.L = L\n",
    "        self.r = r\n",
    "        self.lc = lc\n",
    "        self.agent_step = agent_step \n",
    "        self.boundary_condition = (boundary_condition if num_agents == 1 else 'periodic')\n",
    "        self.num_agents = num_agents\n",
    "        self.high_den = high_den\n",
    "        self.destructive_targets = destructive\n",
    "        \n",
    "        self.init_env()\n",
    "        \n",
    "        \n",
    "    def init_env(self):\n",
    "        \"\"\"\n",
    "        Environment initialization.\n",
    "        \"\"\"\n",
    "        self.target_positions = np.random.rand(self.Nt, 2)*self.L\n",
    "        \n",
    "        #store who is/was rewarded\n",
    "        self.current_rewards = np.zeros(self.num_agents)\n",
    "        self.last_step_rewards = np.zeros(self.num_agents)\n",
    "        \n",
    "        #signal whether agent has been kicked\n",
    "        self.kicked = np.zeros(self.num_agents)\n",
    "        \n",
    "        #set positions and directions of the agents\n",
    "        self.current_directions = np.zeros(self.num_agents)\n",
    "        self.positions = np.zeros((self.num_agents, 2))\n",
    "        for ag in range(self.num_agents):\n",
    "            self.current_directions[ag] = np.random.rand()*2*np.pi\n",
    "            self.positions[ag] = np.random.rand(2)*(self.L) \n",
    "        self.previous_pos = self.positions.copy()\n",
    "          \n",
    "        \n",
    "    def update_pos(self, change_direction, agent_index=0):        \n",
    "        \"\"\"\n",
    "        Updates information of the agent depending on its decision.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        change_direction : bool\n",
    "            Whether the agent decided to turn or not.\n",
    "        agent_index : int, optional\n",
    "            Index of the given agent. The default is 0.\n",
    "        \"\"\"\n",
    "        # Save previous position to check if crossing happened\n",
    "        self.previous_pos[agent_index] = self.positions[agent_index].copy()\n",
    "        \n",
    "        if change_direction:\n",
    "            self.current_directions[agent_index] = np.random.rand(1)*2*np.pi\n",
    "        \n",
    "        #Update position\n",
    "        self.positions[agent_index][0] = self.positions[agent_index][0] + self.agent_step*np.cos(self.current_directions[agent_index])\n",
    "        self.positions[agent_index][1] = self.positions[agent_index][1] + self.agent_step*np.sin(self.current_directions[agent_index])\n",
    "        \n",
    "       \n",
    "    def check_encounter(self, agent_index=0):\n",
    "        \"\"\"\n",
    "        Checks whether the agent found a target, and updates the information accordingly.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        agent_index : int, optional\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        True if the agent found a target.\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        encounters = isBetween_c_Vec(self.previous_pos[agent_index], self.positions[agent_index], self.target_positions, self.r)\n",
    "        \n",
    "        self.last_step_rewards[agent_index] = self.current_rewards[agent_index].copy()\n",
    "        \n",
    "        if sum(encounters) > 0: \n",
    "            \n",
    "            #if there is more than 1 encounter, pick the closest to the agent.\n",
    "            if sum(encounters) == 1:\n",
    "                first_encounter = np.arange(len(self.target_positions))[encounters]\n",
    "            else:\n",
    "                # compute the distance from the previous position to each target            \n",
    "                distance_previous_pos = np.sqrt((self.previous_pos[agent_index][0]- self.target_positions[:, 0])**2 + (self.previous_pos[agent_index][1] - self.target_positions[:, 1])**2)            \n",
    "                \n",
    "                # checking which encountered point is closer to previous position\n",
    "                min_distance_masked = np.argmin(distance_previous_pos[encounters])\n",
    "                first_encounter = np.arange(len(self.target_positions))[encounters][min_distance_masked]\n",
    "            \n",
    "            #if targets are destructive, remove the found target\n",
    "            if self.destructive_targets:\n",
    "                self.target_positions[first_encounter] = np.random.rand(2)*self.L\n",
    "            else:\n",
    "                #----KICK----\n",
    "                # If there was encounter, we reset direction and change position of particle to (pos target + lc)\n",
    "                kick_direction = np.random.uniform(low = 0, high = 2*np.pi)\n",
    "                self.positions[agent_index][0] = self.target_positions[first_encounter, 0] + self.lc*np.cos(kick_direction)\n",
    "                self.positions[agent_index][1] = self.target_positions[first_encounter, 1] + self.lc*np.sin(kick_direction)\n",
    "                self.kicked[agent_index] = 1\n",
    "                #------------\n",
    "                \n",
    "            #...and we add the information that this agent got to the target\n",
    "            self.current_rewards[agent_index] = 1\n",
    "              \n",
    "            return 1\n",
    "        \n",
    "        else: \n",
    "            self.kicked[agent_index] = 0\n",
    "            self.current_rewards[agent_index] = 0\n",
    "            return 0\n",
    "        \n",
    "    def check_bc(self, agent_index=0):\n",
    "        \"\"\"\n",
    "        Updates position coordinates of agent agent_index to fulfill periodic boundary conditions.\n",
    "\n",
    "        \"\"\"\n",
    "        self.positions[agent_index] = (self.positions[agent_index])%self.L\n",
    "        \n",
    "\n",
    "    def get_neighbors_state(self, focal_agent, visual_cone, visual_radius):\n",
    "        \"\"\"\n",
    "        Gets the visual information of the agents surrounding the focal agent.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        focal_agent : int \n",
    "            Index of focal agent.\n",
    "        visual_cone : float \n",
    "            Angle (rad) of the visual cone in front of the agent.\n",
    "        visual_radius : int/float\n",
    "            Radius of the visual circular region around the agent. \n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        State: list\n",
    "            [density of rewarded agents in front, same at the back].\n",
    "            density values: 0 -- no rewarded agent, 1 -- low # of rewarded agents, 2 -- more than high_den rewarded agents.\n",
    "\n",
    "        \"\"\"\n",
    "        mask_in_sight, mask_behind = self.get_agents_in_sight(focal_agent, visual_cone, visual_radius)\n",
    "        \n",
    "        num_success_infront = np.sum(mask_in_sight * self.last_step_rewards)\n",
    "        num_success_behind = np.sum(mask_behind * self.last_step_rewards)\n",
    "              \n",
    "        return [np.argwhere((np.array([0, 1, self.high_den, self.num_agents]) - int(num_success_infront)) <= 0)[-1][0],\n",
    "                np.argwhere((np.array([0, 1, self.high_den, self.num_agents]) - int(num_success_behind)) <= 0)[-1][0]]\n",
    "        \n",
    "    \n",
    "    def get_agents_in_sight(self, focal_agent, visual_cone, visual_radius):\n",
    "        \"\"\"\n",
    "        Get which agents are within the front visual cone of the focal agent.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        focal_agent : int\n",
    "            Index of focal agent.\n",
    "        visual_cone : float\n",
    "            Angle (rad) of the visual cone in front of the agent.\n",
    "        visual_radius : int/float\n",
    "            Radius of the visual circular region around the agent. \n",
    "            \n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        mask_in_sight : np.array of boolean values\n",
    "            True at the indices of agents that are within the visual cone in front.\n",
    "        mask_behind : np.array of boolean values\n",
    "            True at the indices of agents that are within the visual range, but outside the front cone.\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        y = coord_mod(self.previous_pos[:,1], self.positions[focal_agent,1], self.L)\n",
    "        x = coord_mod(self.previous_pos[:,0], self.positions[focal_agent,0], self.L)\n",
    "        \n",
    "        mask_inside_radius = np.sqrt(x**2 + y**2) < visual_radius\n",
    "        \n",
    "        mask_in_sight = (np.abs((np.arctan2(y,x) + 2*np.pi) % (2*np.pi) - self.current_directions[focal_agent]) < visual_cone / 2 ) * mask_inside_radius\n",
    "        mask_behind = mask_inside_radius ^ mask_in_sight\n",
    "        \n",
    "        mask_in_sight[focal_agent] = False\n",
    "        mask_behind[focal_agent] = False\n",
    "        \n",
    "        return mask_in_sight, mask_behind\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projective Simulation agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class PSAgent():\n",
    "    \n",
    "    def __init__(self, num_actions, \n",
    "                 num_percepts_list, \n",
    "                 gamma_damping=0.0, \n",
    "                 eta_glow_damping=0.0, \n",
    "                 policy_type='standard', \n",
    "                 beta_softmax=3, \n",
    "                 initial_prob_distr=None, \n",
    "                 fixed_policy=None):\n",
    "        \"\"\"\n",
    "        Base class of a Reinforcement Learning agent based on Projective Simulation,\n",
    "        with two-layered network. This class has been adapted from https://github.com/qic-ibk/projectivesimulation\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        num_actions : int >=1\n",
    "            Number of actions.\n",
    "        num_percepts_list : list of integers >=1, not nested\n",
    "            Cardinality of each category/feature of percept space.\n",
    "        gamma_damping : float (between 0 and 1), optional\n",
    "            Forgetting/damping of h-values at the end of each interaction. The default is 0.0.\n",
    "        eta_glow_damping : float (between 0 and 1), optional\n",
    "            Controls the damping of glow; setting this to 1 effectively switches off glow. The default is 0.0.\n",
    "        policy_type : string, 'standard' or 'softmax', optional\n",
    "            Toggles the rule used to compute probabilities from h-values. See probability_distr. The default is 'standard'.\n",
    "        beta_softmax : float >=0, optional\n",
    "            Probabilities are proportional to exp(beta*h_value). If policy_type != 'softmax', then this is irrelevant. The default is 3.\n",
    "        initial_prob_distr : list of lists, optional\n",
    "            In case the user wants to change the initialization policy for the agent. This list contains, per percept, a list with the values of the initial h values for each action. The default is None.\n",
    "        fixed_policy : list of lists, optional\n",
    "            In case the user wants to fix a policy for the agent. This list contains, per percept, a list with the values of the probabilities for each action. \n",
    "            Example: Percept 0: fixed_policy[0] = [p(a0), p(a1), p(a2)] = [0.2, 0.3, 0.5], where a0, a1 and a2 are the three possible actions. The default is None.\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        self.num_actions = num_actions\n",
    "        self.num_percepts_list = num_percepts_list\n",
    "        self.gamma_damping = gamma_damping\n",
    "        self.eta_glow_damping = eta_glow_damping\n",
    "        self.policy_type = policy_type\n",
    "        self.beta_softmax = beta_softmax\n",
    "        self.initial_prob_distr = initial_prob_distr\n",
    "        self.fixed_policy = fixed_policy\n",
    "        \n",
    "        self.num_percepts = int(np.prod(np.array(self.num_percepts_list).astype(np.float64))) # total number of possible percepts\n",
    "        \n",
    "        self.h_matrix = np.ones((self.num_actions, self.num_percepts), dtype=np.float64) #Note: the first index specifies the action, the second index specifies the percept.\n",
    "        \n",
    "        self.g_matrix = np.zeros((self.num_actions, self.num_percepts), dtype=np.float64) #glow matrix, for processing delayed rewards\n",
    "        \n",
    "        #initialize h matrix with different values\n",
    "        if self.initial_prob_distr:\n",
    "            self.h_0 = np.ones((self.num_actions, self.num_percepts), dtype=np.float64)\n",
    "            \n",
    "            for percept_index, this_percept_prob_distr in enumerate(self.initial_prob_distr):\n",
    "                self.h_0[:, percept_index] = this_percept_prob_distr\n",
    "                \n",
    "            self.h_matrix = np.copy(self.h_0)\n",
    "            \n",
    "        \n",
    "    def percept_preprocess(self, observation):\n",
    "        \"\"\"\n",
    "        Takes a multi-feature percept and reduces it to a single integer index.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        observation : list of integers >=0, of the same length as self.num_percepts_list\n",
    "            List that describes the observation. Each entry is the value that each feature takes in the observation.\n",
    "            observation[i] < num_percepts_list[i] (strictly)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        percept : int\n",
    "            Percept index that corresponds to the input observation.\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        percept = 0\n",
    "        for which_feature in range(len(observation)):\n",
    "            percept += int(observation[which_feature] * np.prod(self.num_percepts_list[:which_feature]))\n",
    "        return percept\n",
    "    \n",
    "    def deliberate(self, observation):\n",
    "        \"\"\"\n",
    "        Given an observation , this method chooses the next action and records that choice in the g_matrix.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        observation : list\n",
    "            List that describes the observation, as specified in percept_preprocess.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        action : int\n",
    "            Index of the chosen action.\n",
    "\n",
    "        \"\"\"\n",
    "        percept = self.percept_preprocess(observation) \n",
    "        action = np.random.choice(self.num_actions, p=self.probability_distr(percept)) #deliberate once\n",
    "        self.g_matrix = (1 - self.eta_glow_damping) * self.g_matrix\n",
    "        self.g_matrix[action, percept] += 1 #record latest decision in g_matrix\n",
    "        return action\n",
    "    \n",
    "    def learn(self, reward):\n",
    "        \"\"\"\n",
    "        Given a reward, this method updates the h matrix.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        reward : float\n",
    "            Value of the obtained reward.\n",
    "        \"\"\"\n",
    "        if self.initial_prob_distr:\n",
    "            self.h_matrix =  self.h_matrix - self.gamma_damping * (self.h_matrix - self.h_0) + reward * self.g_matrix\n",
    "        else:\n",
    "            self.h_matrix =  self.h_matrix - self.gamma_damping * (self.h_matrix - 1.) + reward * self.g_matrix\n",
    "    \n",
    "    def probability_distr(self, percept):\n",
    "        \"\"\"\n",
    "        Given a percept index, this method returns a probability distribution over actions.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        percept : int\n",
    "            Index of the given percept.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        probability_distr : np.array, length = num_actions\n",
    "            Probability for each action (normalized to unit sum), computed according to policy_type.\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        if self.policy_type == 'standard':\n",
    "            h_vector = self.h_matrix[:, percept]\n",
    "            probability_distr = h_vector / np.sum(h_vector)\n",
    "        elif self.policy_type == 'softmax':\n",
    "            h_vector = self.beta_softmax * self.h_matrix[:, percept]\n",
    "            h_vector_mod = h_vector - np.max(h_vector)\n",
    "            probability_distr = np.exp(h_vector_mod) / np.sum(np.exp(h_vector_mod))\n",
    "        return probability_distr\n",
    "    \n",
    "    def reset_g(self):\n",
    "        \"\"\"\n",
    "        Resets the g_matrix.\n",
    "        \"\"\"\n",
    "        self.g_matrix = np.zeros((self.num_actions, self.num_percepts), dtype=np.float64)\n",
    "        \n",
    "    def deliberate_fixed_policy(self, observation):\n",
    "        \"\"\"\n",
    "        Given an observation , this method chooses the next action according to the fixed policy specified as attribute of the class.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        observation : list\n",
    "            List that describes the observation, as specified in percept_preprocess.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        action : int\n",
    "            Index of the chosen action.\n",
    "\n",
    "        \"\"\"\n",
    "        percept = self.percept_preprocess(observation) \n",
    "        if self.fixed_policy:\n",
    "            action = np.random.choice(self.num_actions, p=self.fixed_policy[percept])\n",
    "        else:\n",
    "            print('No fixed policy was given to the agent. The action will be selected randomly.')\n",
    "            action = np.random.choice(self.num_actions)\n",
    "    \n",
    "        self.g_matrix = (1 - self.eta_glow_damping) * self.g_matrix\n",
    "        self.g_matrix[action, percept] += 1 #record latest decision in g_matrix\n",
    "    \n",
    "        return action\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General forager agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "\n",
    "class Forager(PSAgent):\n",
    "    \n",
    "    def __init__(self, state_space, num_actions, visual_cone= np.pi, visual_radius=1.0, **kwargs):\n",
    "        \"\"\"\n",
    "        This class extends the general `PSAgent` class and adapts it to the foraging scenarioÂ·\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        state_space : list\n",
    "            List where each entry is the state space of each perceptual feature.\n",
    "            E.g. [state space of step counter, state space of density of successful neighbours].\n",
    "        num_actions : int\n",
    "            Number of actions.\n",
    "        visual_cone : float, optional\n",
    "            Visual cone (angle, in radians) of the forager, useful in scenarios with ensembles of agents. The default is np.pi.\n",
    "        visual_radius : float, optional\n",
    "            Radius of the visual region, useful in scenarious with ensembles of agents. The default is 1.0.\n",
    "        **kwargs : multiple\n",
    "            Parameters of the class that defines the learning agent.\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        self.state_space = state_space\n",
    "        self.visual_cone = visual_cone\n",
    "        self.visual_radius = visual_radius\n",
    "        \n",
    "        num_states_list = [len(i) for i in self.state_space]\n",
    "        \n",
    "        super().__init__(num_actions, num_states_list, **kwargs)\n",
    "        \n",
    "        #initialize the step counter n\n",
    "        self.agent_state = 0\n",
    "    \n",
    "    def act(self, action):\n",
    "        \"\"\"\n",
    "        Agent performs the given action.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        action : int (0, 1)\n",
    "            1 if it changes direction, 0 otherwise\n",
    "        \"\"\"\n",
    "        \n",
    "        # If the agent changes direction   \n",
    "        if action == 1:\n",
    "            self.agent_state = 0\n",
    "        else:\n",
    "            self.agent_state += 1        \n",
    "        \n",
    "\n",
    "    \n",
    "    def get_state(self, visual_perception=[0,0]):\n",
    "        \"\"\"\n",
    "        Gets the total state of the agent, combining the internal perception (#steps in same direction)\n",
    "        and the external information of the other agents.\n",
    "                                                                             \n",
    "        Parameters\n",
    "        ----------\n",
    "        visual_perception : list, optional \n",
    "            List with the visual perception of surrounding agents,\n",
    "            [density rewarded agents in front, density of rewarded agents at the back]. \n",
    "            The default is [0,0], for when there is only one agent.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Final state: list\n",
    "            [internal state, external visual information]\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        #state related to step counter\n",
    "        internal_state = list(np.argwhere((self.state_space[0] - self.agent_state) <= 0)[-1])\n",
    "        \n",
    "        return internal_state + visual_perception\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev import nbdev_export ; nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl_opts",
   "language": "python",
   "name": "rl_opts"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
