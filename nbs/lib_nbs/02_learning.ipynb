{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning and benchmarking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook gathers the functions needed to train agents to forage optimally, as well as tools to calculate their foraging efficiency as well as their comparison to benchmark foraging strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp learn_and_bench"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import numpy as np\n",
    "import pathlib\n",
    "\n",
    "from rl_opts.rl_framework import TargetEnv, Forager\n",
    "from rl_opts.utils import get_encounters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def learning(config, results_path, run):\n",
    "    \"\"\"\n",
    "    Training of the RL agent\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    config : dict\n",
    "        Dictionary with all the parameters\n",
    "    results_path : str\n",
    "        Path to save the results\n",
    "    run : int\n",
    "        Agent identifier\n",
    "    \"\"\"\n",
    "    \n",
    "    #Simulation parameters\n",
    "    TIME_EP = config['MAX_STEP_L'] #time steps per episode\n",
    "    EPISODES = config['NUM_EPISODES'] #number of episodes\n",
    "    \n",
    "    #initialize environment\n",
    "    env = TargetEnv(Nt=config['NUM_TARGETS'], L=config['WORLD_SIZE'], r=config['r'], lc=config['lc'])\n",
    "    \n",
    "    #initialize agent \n",
    "    STATE_SPACE = [np.linspace(0, config['MAX_STEP_L']-1, config['NUM_BINS']), np.arange(1), np.arange(1)]\n",
    "    NUM_STATES = np.prod([len(i) for i in STATE_SPACE])\n",
    "    \n",
    "    #default initialization policy\n",
    "    if config['PI_INIT'] == 0.5:\n",
    "        INITIAL_DISTR = None\n",
    "    #change initialization policy\n",
    "    elif config['PI_INIT'] == 0.99:\n",
    "        INITIAL_DISTR = []\n",
    "        for percept in range(NUM_STATES):\n",
    "            INITIAL_DISTR.append([0.99, 0.01])\n",
    "            \n",
    "    agent = Forager(num_actions=config['NUM_ACTIONS'],\n",
    "                    state_space=STATE_SPACE,\n",
    "                    gamma_damping=config['GAMMA'],\n",
    "                    eta_glow_damping=config['ETA_GLOW'],\n",
    "                    initial_prob_distr=INITIAL_DISTR)\n",
    "    \n",
    "    for e in range(EPISODES):\n",
    "        \n",
    "        #initialize environment and agent's counter and g matrix\n",
    "        env.init_env()\n",
    "        agent.agent_state = 0\n",
    "        agent.reset_g()\n",
    "    \n",
    "        for t in range(TIME_EP):\n",
    "            \n",
    "            #step to set counter to its min value n=1\n",
    "            if t == 0 or env.kicked[0]:\n",
    "                #do one step with random direction (no learning in this step)\n",
    "                env.update_pos(1)\n",
    "                #check boundary conditions\n",
    "                env.check_bc()\n",
    "                #reset counter\n",
    "                agent.agent_state = 0\n",
    "                #set kicked value to false again\n",
    "                env.kicked[0] = 0\n",
    "                \n",
    "            else:\n",
    "                #get perception\n",
    "                state = agent.get_state()\n",
    "                #decide\n",
    "                action = agent.deliberate(state)\n",
    "                #act (update counter)\n",
    "                agent.act(action)\n",
    "                \n",
    "                #update positions\n",
    "                env.update_pos(action)\n",
    "                #check if target was found + kick if it is\n",
    "                reward = env.check_encounter()\n",
    "                    \n",
    "                #check boundary conditions\n",
    "                env.check_bc()\n",
    "                #learn\n",
    "                agent.learn(reward)\n",
    "                \n",
    "                \n",
    "        if (e+1)%500 == 0:\n",
    "            #save h matrix of the agent at this stage of the learning process\n",
    "            np.save(results_path+'memory_agent_'+str(run)+'_episode_'+str(e+1)+'.npy', agent.h_matrix)\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate walk from a policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "\n",
    "def walk_from_policy(policy, time_ep, n, L, Nt, r, lc, destructive=False, with_bound=False, bound=100):\n",
    "    \"\"\"\n",
    "    Walk of foragers given a policy. Performance is evaluated as the number of targets found in a fixed time time_ep.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    policy : list\n",
    "        Starting from counter=1, prob of continuing for each counter value.\n",
    "    time_ep : int\n",
    "        Number of steps (decisions).\n",
    "    n : int\n",
    "        Number of agents that walk in parallel (all with the same policy, they do not interact). This is \"number of walks\" in the paper.\n",
    "    L : int\n",
    "        World size.\n",
    "    Nt : int\n",
    "        Number of targets.\n",
    "    r : float\n",
    "        Target radius.\n",
    "    lc : float\n",
    "        Cutoff length. Agent is displaced a distance lc from the target when it finds it.\n",
    "    destructive : bool, optional\n",
    "        True if targets are destructive. The default is False.\n",
    "    with_bound : bool, optional\n",
    "        True if policy is cut. The default is False.\n",
    "    bound : int, optional\n",
    "        Bound of the policy (maximum value for the counter). The default is 20.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    reward : list, len(rewards)=n\n",
    "        Number of targets found by each agent in time_ep steps of d=1.\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    #initialize agents clocks, positions and directions, as well as targets in the env.\n",
    "    pos = np.zeros((time_ep, n, 2)) \n",
    "    pos[0] = np.random.rand(n,2)*L\n",
    "    \n",
    "    current_pos = np.random.rand(n,2)*L\n",
    "    \n",
    "    direction = np.random.rand(n)*2*np.pi \n",
    "    internal_counter = [0]*n\n",
    "    target_positions = np.random.rand(Nt,2) * L\n",
    "    reward = [0]*n\n",
    "    \n",
    "    #cut policy\n",
    "    if with_bound:\n",
    "        policy[bound:] = [0] * (len(policy)-bound)\n",
    "        \n",
    "    for t in range(1, time_ep):   \n",
    "        \n",
    "        #update position\n",
    "        previous_pos = np.copy(current_pos)\n",
    "        current_pos[:,0] = previous_pos[:, 0] + np.cos(direction)\n",
    "        current_pos[:,1] = previous_pos[:, 1] + np.sin(direction)\n",
    "        \n",
    "        #check reward\n",
    "        encounters = get_encounters(previous_pos, current_pos, target_positions, L, r)\n",
    "        \n",
    "        for ag, num_encounters in enumerate(np.sum(encounters,axis=0)):\n",
    "            kick = False\n",
    "            \n",
    "            if num_encounters > 0: \n",
    "                \n",
    "                first_encounter = np.arange(len(target_positions))[encounters[:,ag]]\n",
    "                \n",
    "                if destructive:\n",
    "                    #target is destroyed, sample position for a new target.\n",
    "                    target_positions[first_encounter] = np.random.rand(2) * L\n",
    "                else:\n",
    "                    #----KICK----\n",
    "                    # If there was encounter, we reset direction and change position of particle to (pos target + lc)\n",
    "                    kick_direction = np.random.rand()*2*np.pi  \n",
    "                    \n",
    "                    current_pos[ag, 0] = target_positions[first_encounter, 0] + lc*np.cos(kick_direction)\n",
    "                    current_pos[ag, 1] = target_positions[first_encounter, 1] + lc*np.sin(kick_direction)\n",
    "                    \n",
    "                    #------------\n",
    "                internal_counter[ag] = 0\n",
    "                reward[ag] += 1\n",
    "                kick = True\n",
    "                \n",
    "            \n",
    "            current_pos[ag] %= L\n",
    "            \n",
    "            if np.random.rand() > policy[internal_counter[ag]] or kick:\n",
    "                internal_counter[ag] = 0\n",
    "                direction[ag] = np.random.rand()*2*np.pi  \n",
    "                \n",
    "            else:\n",
    "                internal_counter[ag] += 1\n",
    "                \n",
    "    return reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Efficiency computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "from rl_opts.utils import get_config, get_policy\n",
    "\n",
    "def agent_efficiency(results_path, config, run, num_walks, episode_interval):\n",
    "    \"\"\"\n",
    "    Computes the agent's average search efficiency over a number of walks where the agent follows a fixed policy. \n",
    "    This is repeated with the policies at different stages of the training to analyze the evolution of its performance.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    results_path : str\n",
    "        Path to the results folder, from which to extract the agent's policies\n",
    "    config : dict\n",
    "        Dictionary with all the parameters. It needs to be the same configuration file as the one used to train the agent.\n",
    "    run : int\n",
    "        Id of the agent\n",
    "    num_walks : int\n",
    "        Number of (independent) walks\n",
    "    episode_interval : int\n",
    "        Every 'episode_interval' training episodes, the policy of the agent is taken and its performance is analyzed.\n",
    "        \n",
    "    \"\"\"\n",
    "\n",
    "    print('Statistics postlearning of agent', run, '\\nData obtained from folder: ', results_path)\n",
    "    \n",
    "    \n",
    "    for training_episode in [i for i in range(0, config['NUM_EPISODES'] + 1, episode_interval)]:\n",
    "        \n",
    "        if training_episode == 0 and config['PI_INIT'] == 0.99:\n",
    "            frozen_policy = [0.99 for percept in range(config['MAX_STEP_L'])] #initial policy\n",
    "            \n",
    "        elif training_episode == 0 and config['PI_INIT'] == 0.5:\n",
    "            frozen_policy  = [0.5 for percept in range(config['MAX_STEP_L'])] #initial policy\n",
    "            \n",
    "        else:\n",
    "            #get policy from the stored h matrix at the given training_episode\n",
    "            frozen_policy = get_policy(results_path, run, training_episode)\n",
    "            \n",
    "        #run the 10^4 walks (in parallel) with the same policy\n",
    "        rewards = walk_from_policy(policy=frozen_policy,\n",
    "                                   time_ep=config['MAX_STEP_L'],\n",
    "                                   n=num_walks,\n",
    "                                   L=config['WORLD_SIZE'],\n",
    "                                   Nt=config['NUM_TARGETS'],\n",
    "                                   r=config['r'],\n",
    "                                   lc=config['lc'])\n",
    "        \n",
    "        #save results\n",
    "        np.save(results_path+'performance_post_training_agent_'+str(run)+'_episode_'+str(training_episode)+'.npy', rewards)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmarks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code to get the search efficiency of the benchmark models. We consider LÃ©vy and bi-exponential distributions and obtain the model parameters that achieve the highest search efficiency. We use the library `Tune` for the efficiency optimization within given parameter ranges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import pathlib\n",
    "\n",
    "from ray import tune\n",
    "from ray.tune.search.bayesopt import BayesOptSearch\n",
    "from ray.tune.search import ConcurrencyLimiter\n",
    "\n",
    "from rl_opts.analytics import get_policy_from_dist, pdf_powerlaw, pdf_multimode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def average_search_efficiency(config):\n",
    "    \"\"\"\n",
    "    Get the average search efficiency, considering the benchmark model defined in config.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    config : dict\n",
    "        Dictionary with the configuration of the benchmark model.\n",
    "    \"\"\"\n",
    "    \n",
    "    #get parameters of the distributions depending on the chosen model\n",
    "    if config['model'] == 'powerlaw':\n",
    "        #get policy from benchmark model\n",
    "        policy = get_policy_from_dist(n_max = config['time_ep'], \n",
    "                                      func = pdf_powerlaw,\n",
    "                                      beta = config['beta']\n",
    "                                     )\n",
    "    \n",
    "    elif config['model'] == 'double_exp':\n",
    "        #get policy from benchmark model\n",
    "        policy = get_policy_from_dist(n_max=config['time_ep'],\n",
    "                                      func = pdf_multimode,\n",
    "                                      lambdas = np.array([config['d_int'], config['d_ext']]),\n",
    "                                      probs = np.array([config['p'], 1-config['p']])\n",
    "                                  )\n",
    "    \n",
    "    \n",
    "    #run the walks in parallel\n",
    "    efficiencies = walk_from_policy(policy=policy,\n",
    "                                    time_ep=config['time_ep'],\n",
    "                                    n=config['n'],\n",
    "                                    L=config['L'],\n",
    "                                    Nt=config['Nt'],\n",
    "                                    r=config['r'],\n",
    "                                    lc=config['lc'])\n",
    "    \n",
    "    #get the mean search efficiency over the walks\n",
    "    mean_eff = np.mean(efficiencies) \n",
    "    tune.report(mean_eff = mean_eff)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up the configuration, run and type of search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rl_opts.learn_and_bench import average_search_efficiency\n",
    "from ray import tune\n",
    "from ray.tune.search.bayesopt import BayesOptSearch\n",
    "from ray.tune.search import ConcurrencyLimiter\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Minimal example #####\n",
    "run = '0'\n",
    "search_type = 'Bayesian'\n",
    "config = {'num_raytune_samples': 3,\n",
    "          'd_int': tune.uniform(0.00001, 20.0),\n",
    "          'd_ext': 100.0,\n",
    "          'p': tune.uniform(0.0, 1.0),\n",
    "          'beta': None,\n",
    "          'model': 'double_exp',\n",
    "          'time_ep': 20,\n",
    "          'n': 10,\n",
    "          'lc': 3.0,\n",
    "          'Nt': 100,\n",
    "          'L': 100,\n",
    "          'r': 0.5,\n",
    "          'destructive': False,\n",
    "         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# Full simulations as in manuscript\n",
    "run = '1'\n",
    "search_type = 'Bayesian'\n",
    "config = {'d_int': tune.uniform(0.00001, 20.0),\n",
    "          'd_ext': 100.0,\n",
    "          'p': tune.uniform(0.0, 1.0),\n",
    "          'beta': None,\n",
    "          'model': 'double_exp',\n",
    "          'time_ep': 20000,\n",
    "          'n': 10000,\n",
    "          'lc': 3.0,\n",
    "          'Nt': 100,\n",
    "          'L': 100,\n",
    "          'r': 0.5,\n",
    "          'destructive': False\n",
    "          }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize `Tune`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if search_type == 'Bayesian': #Bayesian optimization\n",
    "    \n",
    "    bayesopt = BayesOptSearch(metric=\"mean_eff\", mode=\"max\")\n",
    "    bayesopt = ConcurrencyLimiter(bayesopt, max_concurrent=3)\n",
    "    tuner = tune.Tuner(average_search_efficiency, \n",
    "                        tune_config=tune.TuneConfig(search_alg=bayesopt, num_samples=config['num_raytune_samples']), \n",
    "                        param_space=config)\n",
    "    \n",
    "elif search_type == 'Grid':#Grid search\n",
    "\n",
    "    tuner = tune.Tuner(average_search_efficiency,\n",
    "                        tune_config=tune.TuneConfig(num_samples=1),\n",
    "                        param_space=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_grid = tuner.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the results and the configuration in the corresponding folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results path\n",
    "results_path = 'results/benchmark_models/' + config['model'] + '/'+ str(config['lc']) + '/run_'+run+'/'\n",
    "pathlib.Path(results_path).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Configuration path\n",
    "config_path = 'configurations/benchmark_models/'\n",
    "pathlib.Path(config_path).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save results\n",
    "results_df = result_grid.get_dataframe()\n",
    "results_df.to_csv(results_path+'df'+run+'_'+config['model']+'_lc_'+str(config['lc'])+'.csv')\n",
    "np.save(config_path+'config_'+config['model']+'_lc_'+str(config['lc'])+'_run_'+run+'.npy', config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev import nbdev_export; nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
