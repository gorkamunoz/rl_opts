{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement learning agents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook gathers the functions creating different kinds of agents for foraging and target search in various scenarios, adapted for their use in the reinforcement learning paradigm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp rl_framework.numba.agents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#| hide\n",
    "# Libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import numpy as np\n",
    "from numba.experimental import jitclass\n",
    "from numba import float64, int64,  njit, prange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helpers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random sampling from array with probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "@njit\n",
    "def rand_choice_nb(arr, prob):\n",
    "    \"\"\"\n",
    "    :param arr: A 1D numpy array of values to sample from.\n",
    "    :param prob: A 1D numpy array of probabilities for the given samples.\n",
    "    :return: A random sample from the given array with a given probability.\n",
    "    \"\"\"\n",
    "    return arr[np.searchsorted(np.cumsum(prob), np.random.random(), side=\"right\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "@jitclass([(\"size_state_space\", int64[:]),           \n",
    "           (\"initial_prob_distr\", float64[:,:]),           \n",
    "           (\"fixed_policy\", float64[:,:]) ,\n",
    "           (\"h_matrix\", float64[:,:]) ,\n",
    "           (\"g_matrix\", float64[:,:]) ,\n",
    "           (\"h_0\", float64[:,:]),\n",
    "           (\"prefactor_1\", float64[:]),\n",
    "           (\"prefactor_2\", float64[:]),\n",
    "           (\"last_upd_G\", float64[:,:])\n",
    "          ])\n",
    "class Forager():\n",
    "    num_actions : int\n",
    "    gamma_damping : float\n",
    "    eta_glow_damping : float\n",
    "    policy_type : str\n",
    "    beta_softmax : float\n",
    "    num_percepts : int\n",
    "    agent_state : int\n",
    "    size_state_space : np.array\n",
    "    initial_prob_distr : np.array\n",
    "    fixed_policy : np.array    \n",
    "    h_matrix : np.array\n",
    "    g_matrix : np.array\n",
    "    h_0 : np.array\n",
    "    g_update : str\n",
    "    # Efficient H update\n",
    "    prefactor_1: np.array\n",
    "    prefactor_2: np.array\n",
    "    max_no_H_update : int\n",
    "    N_upd_H : int\n",
    "    # Efficient G update\n",
    "    last_upd_G: np.array\n",
    "    N_upd_G: int\n",
    "    \n",
    "    def __init__(self, \n",
    "                 # Number of actions the agent can take\n",
    "                 num_actions : int, \n",
    "                 # Size of the state space, given as an array where each entry is the dimension of each environmental feature \n",
    "                 size_state_space : np.array, \n",
    "                 # Gamma damping from PS\n",
    "                 gamma_damping=0.0, \n",
    "                 # Eta damping from PS\n",
    "                 eta_glow_damping=0.0,\n",
    "                 # Policy type. Can be 'standard' or 'softmax'\n",
    "                 policy_type='standard', \n",
    "                 # Beta parameter for softmax policy\n",
    "                 beta_softmax=3, \n",
    "                 # Initial probability distribution for the H matrix\n",
    "                 initial_prob_distr = np.array([[],[]]), \n",
    "                 # Fixed policy for the agent to follow\n",
    "                 fixed_policy=np.array([[],[]]),\n",
    "                 # Max number of steps without updating the H matrix. After this number, the full H matrix is updated\n",
    "                 max_no_H_update = int(1e4),\n",
    "                 # Type of update for the G matrix. Can be 's' (sum) or 'r' (reset)\n",
    "                 # Works as follows: s (sum) -> g_mat += 1 or r (reset) -> gmat = 1 when updating gmat\n",
    "                 g_update = 's', \n",
    "                ):\n",
    "        \"\"\"\n",
    "\n",
    "        This class defines a Forager agent, able to perform actions and learn from rewards based on the PS paradigm.\n",
    "\n",
    "        This is an updated version from the one used in the original paper (https://doi.org/10.1088/1367-2630/ad19a8), \n",
    "        taking into account the improvements made to the H and G matrices proposed by Michele Caraglio in our paper\n",
    "        (https://doi.org/10.1039/D3SM01680C).\n",
    "        \"\"\"\n",
    "        \n",
    "        self.agent_state = 0\n",
    "        \n",
    "        self.num_actions = num_actions\n",
    "        self.size_state_space = size_state_space\n",
    "        self.num_percepts = int(np.prod(self.size_state_space)) # total number of possible percepts\n",
    "        \n",
    "        self.gamma_damping = gamma_damping\n",
    "        self.eta_glow_damping = eta_glow_damping\n",
    "        self.policy_type = policy_type\n",
    "        self.beta_softmax = beta_softmax\n",
    "        self.initial_prob_distr = initial_prob_distr\n",
    "        self.fixed_policy = fixed_policy    \n",
    "        self.g_update = g_update\n",
    "        \n",
    "        self.init_matrices()\n",
    "        \n",
    "        # For H update\n",
    "        self.max_no_H_update = max_no_H_update      \n",
    "        self.N_upd_H = 0\n",
    "        self.prefactor_1 = (1-self.gamma_damping)**(np.arange(1,self.max_no_H_update+1)) \n",
    "        self.prefactor_2 = np.zeros(self.max_no_H_update)\n",
    "        for i in range(max_no_H_update):\n",
    "            self.prefactor_2[i] = self.gamma_damping*np.sum((1-self.gamma_damping)**np.arange(i+1))\n",
    "            \n",
    "        # For G update\n",
    "        self.last_upd_G = np.zeros((self.num_actions, self.num_percepts))\n",
    "        self.N_upd_G = 0\n",
    "                              \n",
    "        \n",
    "    def init_matrices(self):\n",
    "\n",
    "        self.g_matrix = np.zeros((self.num_actions, self.num_percepts)) #glow matrix, for processing delayed rewards\n",
    "\n",
    "        # initialize h matrix with different values\n",
    "        if len(self.initial_prob_distr[0]) > 0:          \n",
    "            self.h_0 = self.initial_prob_distr\n",
    "            self.h_matrix = self.h_0.copy()\n",
    "        else: \n",
    "            self.h_matrix = np.ones((self.num_actions, self.num_percepts), dtype=np.float64) #Note: the first index specifies the action, the second index specifies the percept.\n",
    "            \n",
    "    def _learn_post_reward(self, reward):\n",
    "        '''Given a reward, updates the whole H-matrix taking into account that we did not have updates\n",
    "        for the last N_upd_H steps.'''\n",
    "        # Update the full G matrix\n",
    "        self._G_upd_full()\n",
    "        \n",
    "        if self.N_upd_H == 0:\n",
    "            print('Counter for h_matrix is zero, check that your are properly updating it!')\n",
    "        if len(self.initial_prob_distr[0]) > 0:\n",
    "            self.h_matrix = self.prefactor_1[self.N_upd_H-1] * self.h_matrix + self.prefactor_2[self.N_upd_H-1] * self.h_0 + reward * self.g_matrix\n",
    "        else:\n",
    "            self.h_matrix = self.prefactor_1[self.N_upd_H-1] * self.h_matrix + self.prefactor_2[self.N_upd_H-1] + reward * self.g_matrix\n",
    "        self.N_upd_H = 0\n",
    "        \n",
    "    def _H_upd_single_percept(self, t, percept):\n",
    "        '''Given a percept and the time t passed since the last H-matrix update,\n",
    "        returns the corresponding --updated-- column of the H-matrix for all actions.\n",
    "        This updated is local and does no affect the H-matrix.'''\n",
    "        if len(self.initial_prob_distr[0]) > 0:\n",
    "            return self.prefactor_1[t-1] * self.h_matrix[:, percept] + self.prefactor_2[t-1] * self.h_0[:, percept]\n",
    "        else:\n",
    "            return self.prefactor_1[t-1] * self.h_matrix[:, percept] + self.prefactor_2[t-1] \n",
    "        \n",
    "    def _G_upd_single_percept(self, percept, action):\n",
    "        '''Given a percept-action tuple, updates that element of the G-matrix. Updates the last_upd_G\n",
    "        to keep track of when was the matrix updated.''' \n",
    "\n",
    "        if self.g_update == 's': # For the current (a,s) tuple, we damp and sum one\n",
    "            if self.eta_glow_damping == 1:\n",
    "                # We do this because below we would have 0**0 = 1\n",
    "                self.g_matrix[action, percept] = 1\n",
    "            else:\n",
    "                self.g_matrix[action, percept] = (1 - self.eta_glow_damping)**(self.N_upd_G - self.last_upd_G[action, percept])*self.g_matrix[action, percept] + 1\n",
    "        elif self.g_update == 'r':\n",
    "            self.g_matrix[action, percept] = 1\n",
    "        \n",
    "        \n",
    "        # Then update the last_upd matrix\n",
    "        self.last_upd_G[action, percept] = self.N_upd_G\n",
    "        \n",
    "    def _G_upd_full(self):\n",
    "        '''Given the current number of steps without an update, updates the whole G-matrix.\n",
    "        Then, resets all counters.'''\n",
    "        self.g_matrix = (1 - self.eta_glow_damping)**(self.N_upd_G - self.last_upd_G) * self.g_matrix\n",
    "        self.N_upd_G = 0\n",
    "        self.last_upd_G = np.zeros((self.num_actions, self.num_percepts))\n",
    "            \n",
    "            \n",
    "    def percept_preprocess(self, observation):\n",
    "        \"\"\"\n",
    "        Takes a multi-feature percept and reduces it to a single integer index.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        observation : ARRAY of integers >=0, of the same length as self.num_percepts_list\n",
    "            List that describes the observation. Each entry is the value that each feature takes in the observation.\n",
    "            observation[i] < num_percepts_list[i] (strictly)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        percept : int\n",
    "            Percept index that corresponds to the input observation.\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        percept = 0\n",
    "        for idx_obs, obs_feature in enumerate(observation):\n",
    "            percept += int(obs_feature * np.prod(self.size_state_space[:idx_obs]))  \n",
    "        return percept\n",
    "    \n",
    "    def deliberate(self, observation):\n",
    "        \"\"\"\n",
    "        Given an observation , this method chooses the next action and records that choice in the g_matrix.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        observation : list\n",
    "            List that describes the observation, as specified in percept_preprocess.\n",
    "        action : optional, bool\n",
    "            Mostly for debugging, we can input the action and no deliberation takes place, but g_matrix is updated\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        action : int\n",
    "            Index of the chosen action.\n",
    "\n",
    "        \"\"\"\n",
    "        percept = self.percept_preprocess(observation) \n",
    "        \n",
    "        \n",
    "        # Probabilities must be of update h_matrix. We feed the prob distr the update h_matrix\n",
    "        # for the percept, but don't update the h_matrix\n",
    "        if self.N_upd_H == 0:\n",
    "            current_h_mat = self.h_matrix[:, percept]\n",
    "        else:\n",
    "            current_h_mat = self._H_upd_single_percept(self.N_upd_H, percept)\n",
    "        probs = self.probability_distr(percept, h_matrix = current_h_mat)        \n",
    "        action = rand_choice_nb(arr = np.arange(self.num_actions), prob = probs)\n",
    "        \n",
    "        # Update the G matrix for current (s,a) tuple\n",
    "        self._G_upd_single_percept(percept, action)\n",
    "        \n",
    "        return action\n",
    "    \n",
    "    def probability_distr(self, percept, h_matrix = None):\n",
    "        \"\"\"\n",
    "        UPDATE (added the optional input)\n",
    "         \n",
    "        Given a percept index, this method returns a probability distribution over actions.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        percept : int\n",
    "            Index of the given percept.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        probability_distr : np.array, length = num_actions\n",
    "            Probability for each action (normalized to unit sum), computed according to policy_type.\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        if self.policy_type == 'standard':\n",
    "            h_vector = self.h_matrix[:, percept] if h_matrix is None else h_matrix\n",
    "            probability_distr = h_vector / np.sum(h_vector)\n",
    "        elif self.policy_type == 'softmax':\n",
    "            h_vector = self.beta_softmax * self.h_matrix[:, percept] if h_matrix is None else h_matrix\n",
    "            h_vector_mod = h_vector - np.max(h_vector)\n",
    "            probability_distr = np.exp(h_vector_mod) / np.sum(np.exp(h_vector_mod))\n",
    "        return probability_distr\n",
    "    \n",
    "    def learn(self, reward):\n",
    "        \"\"\"\n",
    "        Given a reward, this method updates the h matrix.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        reward : float\n",
    "            Value of the obtained reward.\n",
    "        \"\"\"\n",
    "        if len(self.initial_prob_distr[0]) > 0:\n",
    "            self.h_matrix =  self.h_matrix - self.gamma_damping * (self.h_matrix - self.h_0) + reward * self.g_matrix\n",
    "        else:\n",
    "            self.h_matrix =  self.h_matrix - self.gamma_damping * (self.h_matrix - 1.) + reward * self.g_matrix\n",
    "            \n",
    "    def reset_g(self):\n",
    "        \"\"\"\n",
    "        Resets the g_matrix.\n",
    "        \"\"\"\n",
    "        self.g_matrix = np.zeros((self.num_actions, self.num_percepts), dtype=np.float64)\n",
    "        \n",
    "    def deliberate_fixed_policy(self, observation):\n",
    "        \"\"\"\n",
    "        Given an observation , this method chooses the next action according to the fixed policy specified as attribute of the class.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        observation : list\n",
    "            List that describes the observation, as specified in percept_preprocess.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        action : int\n",
    "            Index of the chosen action.\n",
    "\n",
    "        \"\"\"\n",
    "        percept = self.percept_preprocess(observation) \n",
    "        if len(self.fixed_policy[0]) > 0:\n",
    "            action = rand_choice_nb(arr = np.arange(self.num_actions), prob = self.fixed_policy[percept])\n",
    "        else:\n",
    "            print('No fixed policy was given to the agent. The action will be selected randomly.')\n",
    "            action = np.random.choice(self.num_actions)\n",
    "    \n",
    "        self.g_matrix = (1 - self.eta_glow_damping) * self.g_matrix\n",
    "        self.g_matrix[action, percept] += 1 #record latest decision in g_matrix\n",
    "    \n",
    "        return action\n",
    "    \n",
    "    def act(self, action):\n",
    "        \"\"\"\n",
    "        Agent performs the given action.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        action : int (0, 1)\n",
    "            1 if it changes direction, 0 otherwise\n",
    "        \"\"\"\n",
    "        \n",
    "        # If the agent changes direction   \n",
    "        if action == 1:\n",
    "            self.agent_state = 0\n",
    "        else:\n",
    "            self.agent_state += 1  \n",
    "            \n",
    "    \n",
    "    def get_state(self):  \n",
    "        ''' simplified to case of single forager. Returns list because is what deliberate needs'''\n",
    "        return np.array([self.agent_state])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parallel training launchers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For ResetEnv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               \n",
    "@njit\n",
    "def train_loop_reset(episodes, time_ep, agent, env, h_mat_allT = False, when_save_h_mat = 1, reset_after_reward = True):  \n",
    "\n",
    "    if h_mat_allT: \n",
    "        policy_t = np.zeros((int(np.ceil(episodes/when_save_h_mat)), \n",
    "                             agent.h_matrix.shape[-1]))\n",
    "        idx_policy_save = 0\n",
    "        \n",
    "    save_rewards = np.zeros(episodes)\n",
    "    \n",
    "    for ep in range(episodes):\n",
    "        \n",
    "        #initialize environment and agent's counter and g matrix\n",
    "        env.init_env()\n",
    "        agent.agent_state = 0\n",
    "        agent.reset_g()\n",
    "\n",
    "        for t in range(time_ep):\n",
    "            agent.N_upd_H += 1\n",
    "            agent.N_upd_G += 1\n",
    "\n",
    "            #get perception\n",
    "            state = agent.get_state()\n",
    "            \n",
    "            # if we reached the maximum state space, we perform turn action\n",
    "            if state == agent.h_matrix.shape[-1]:\n",
    "                action = 1\n",
    "            # else we do as normal    \n",
    "            else: \n",
    "                action = agent.deliberate(state)\n",
    "                \n",
    "            #act (update counter)\n",
    "            agent.act(action)\n",
    "\n",
    "            #update positions\n",
    "            reward = env.update_pos(action)            \n",
    "\n",
    "            if reward == 1 or agent.N_upd_H == agent.max_no_H_update-1:\n",
    "                agent._learn_post_reward(reward)\n",
    "            \n",
    "            if reset_after_reward == True and reward != 0:\n",
    "                agent.agent_state = 0\n",
    "\n",
    "            # Saving\n",
    "            save_rewards[ep] += reward\n",
    "        if h_mat_allT and ep % when_save_h_mat == 0:\n",
    "            policy_t[idx_policy_save] = agent.h_matrix[0,:] / agent.h_matrix.sum(0)\n",
    "            idx_policy_save += 1\n",
    "      \n",
    "    return (save_rewards/time_ep, policy_t) if h_mat_allT else (save_rewards/time_ep, agent.h_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      "  1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      "  1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      "  1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      "  1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      "  1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      "  1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      "  1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      "  1. 1. 1. 1.]]\n"
     ]
    }
   ],
   "source": [
    "#| hide\n",
    "#|eval: false\n",
    "\n",
    "# from rl_opts.rl_framework.numba.environments import ResetEnv_1D\n",
    "# from rl_opts.rl_framework.numba.agents import Forager\n",
    "# import numpy as np \n",
    "\n",
    "env = ResetEnv_1D(L = 5, D = 1/2)\n",
    "agent = Forager(num_actions = 2,\n",
    "                          size_state_space = np.array([100]))\n",
    "\n",
    "res = train_loop_reset(100, 100, agent, env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Launchers\n",
    "\n",
    "> Note: we have to separate the launchers in 1D and 2D because of `numba` compilation, which would give errors due to the enviroments asking for different inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from rl_opts.rl_framework.numba.environments import ResetEnv_1D\n",
    "\n",
    "\n",
    "\n",
    "@njit(parallel = True)\n",
    "def run_agents_reset_1D(episodes, time_ep, N_agents,\n",
    "                       # Environment props\n",
    "                       D = 1/2, L = 10.0, \n",
    "                       # Agent props\n",
    "                       num_actions = 2,\n",
    "                       size_state_space = np.array([100]),\n",
    "                       gamma_damping = 0.00001,\n",
    "                       eta_glow_damping = 0.1,\n",
    "                       g_update = 's',\n",
    "                       initial_prob_distr = np.array([[],[]]),\n",
    "                       policy_type = 'standard', \n",
    "                       beta_softmax = 3,  \n",
    "                       fixed_policy = np.array([[],[]]),\n",
    "                       max_no_H_update = int(1e3),\n",
    "                       h_mat_allT = False, \n",
    "                       reset_after_reward = True,\n",
    "                       # When we want N_agent != number of max cores, we use this to make few runs\n",
    "                       # over the selected number of cores, given by N_agents.\n",
    "                       num_runs = None \n",
    "                      ):\n",
    "\n",
    "    if num_runs is None:\n",
    "        total_agents = N_agents\n",
    "    else:\n",
    "        total_agents = N_agents*num_runs\n",
    "    \n",
    "    save_rewards = np.zeros((total_agents, episodes))\n",
    "    if h_mat_allT:\n",
    "        save_h_matrix = np.zeros((total_agents, episodes, size_state_space.prod()))  \n",
    "    else:        \n",
    "        save_h_matrix = np.zeros((total_agents, 2, size_state_space.prod())) \n",
    "\n",
    "    # if N_agents is an integer, we consider that we run this at full cores\n",
    "    if num_runs is None:\n",
    "    \n",
    "        for n_agent in prange(N_agents):\n",
    "            \n",
    "            agent = Forager(num_actions, size_state_space,\n",
    "                                      gamma_damping, eta_glow_damping, \n",
    "                                      policy_type, beta_softmax,\n",
    "                                      initial_prob_distr,fixed_policy,max_no_H_update,g_update)\n",
    "            env = ResetEnv_1D(L, D)\n",
    "            \n",
    "            rews, mat = train_loop_reset(episodes, time_ep, agent, env, h_mat_allT, reset_after_reward)            \n",
    "     \n",
    "            # print(rews.shape, rews[0], rews[0].dtype)\n",
    "            \n",
    "            save_rewards[n_agent] = rews\n",
    "            save_h_matrix[n_agent] = mat\n",
    "\n",
    "    # If it is a list, the first number is the number of parallel agents and the second the \n",
    "    # times we run those parallel agents\n",
    "    else:\n",
    "\n",
    "        n_run = -1        \n",
    "        for run in range(num_runs):\n",
    "            n_run += 1\n",
    "            for idxa in prange(N_agents):\n",
    "            \n",
    "                agent = Forager(num_actions, size_state_space,\n",
    "                                gamma_damping, eta_glow_damping, \n",
    "                                policy_type, beta_softmax,\n",
    "                                initial_prob_distr,fixed_policy,max_no_H_update, g_update)\n",
    "                env = ResetEnv_1D(L, D)\n",
    "                \n",
    "                rews, mat = train_loop_reset(episodes, time_ep, agent, env, h_mat_allT, reset_after_reward)            \n",
    "                         \n",
    "                save_rewards[idxa*num_runs+n_run] = rews\n",
    "                save_h_matrix[idxa*num_runs+n_run] = mat\n",
    "        \n",
    "        \n",
    "    return save_rewards, save_h_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "#|eval: false\n",
    "# from rl_opts.rl_framework.numba.environments import ResetEnv_1D\n",
    "# from rl_opts.rl_framework.numba.agents import Forager, train_loop_reset\n",
    "\n",
    "\n",
    "rews, mats = run_agents_reset_1D(5, 10, 5, L = 2, num_runs=2, eta_glow_damping=0);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from rl_opts.rl_framework.numba.environments import ResetEnv_2D\n",
    "\n",
    "\n",
    "@njit(parallel = True)\n",
    "def run_agents_reset_2D(episodes, time_ep, N_agents,\n",
    "                     # Environment props\n",
    "                     dist_target = 10.0, radius_target = 1.0, D = 1/2,\n",
    "                     # Agent props\n",
    "                     num_actions = 2,\n",
    "                     size_state_space = np.array([100]),\n",
    "                     gamma_damping = 0.00001,\n",
    "                     eta_glow_damping = 0.1,\n",
    "                     initial_prob_distr = np.array([[],[]]),\n",
    "                     policy_type = 'standard', \n",
    "                     beta_softmax = 3,  \n",
    "                     fixed_policy = np.array([[],[]]),\n",
    "                     max_no_H_update = int(1e3),\n",
    "                     h_mat_allT = False, when_save_h_mat = 1,\n",
    "                     reset_after_reward = True,\n",
    "                     g_update = 's',\n",
    "                    # When we want N_agent != number of max cores, we use this to make few runs\n",
    "                    # over the selected number of cores, given by N_agents.\n",
    "                    num_runs = None                         \n",
    "                      ):\n",
    "    \n",
    "    save_rewards = np.zeros((N_agents, episodes))\n",
    "    if h_mat_allT:\n",
    "        save_h_matrix = np.zeros((N_agents, \n",
    "                                  int(np.ceil(episodes/when_save_h_mat)), \n",
    "                                  size_state_space.prod()))  \n",
    "    else:        \n",
    "        save_h_matrix = np.zeros((N_agents, 2, size_state_space.prod())) \n",
    "\n",
    "    # if N_agents is an integer, we consider that we run this at full cores\n",
    "    if num_runs is None:\n",
    "    \n",
    "        for n_agent in prange(N_agents):\n",
    "            \n",
    "            agent = Forager(num_actions, size_state_space,\n",
    "                            gamma_damping, eta_glow_damping, \n",
    "                            policy_type, beta_softmax,\n",
    "                            initial_prob_distr,fixed_policy,max_no_H_update,g_update)\n",
    "            \n",
    "            env = ResetEnv_2D(dist_target, radius_target, D)\n",
    "            \n",
    "            rews, mat = train_loop_reset(episodes, time_ep, agent, env, h_mat_allT, when_save_h_mat, reset_after_reward)            \n",
    "    \n",
    "        \n",
    "            save_rewards[n_agent] = rews\n",
    "            save_h_matrix[n_agent] = mat\n",
    "\n",
    "    # If it is a list, the first number is the number of parallel agents and the second the \n",
    "    # times we run those parallel agents\n",
    "    else:\n",
    "\n",
    "        n_run = -1        \n",
    "        for run in range(num_runs):\n",
    "            n_run += 1\n",
    "            for idxa in prange(N_agents):\n",
    "            \n",
    "                agent = Forager(num_actions, size_state_space,\n",
    "                                gamma_damping, eta_glow_damping, \n",
    "                                policy_type, beta_softmax,\n",
    "                                initial_prob_distr,fixed_policy,max_no_H_update, g_update)\n",
    "                env = ResetEnv_2D(dist_target, radius_target, D)\n",
    "                \n",
    "                rews, mat = train_loop_reset(episodes, time_ep, agent, env, h_mat_allT, reset_after_reward)            \n",
    "                         \n",
    "                save_rewards[idxa*num_runs+n_run] = rews\n",
    "                save_h_matrix[idxa*num_runs+n_run] = mat\n",
    "        \n",
    "    return save_rewards, save_h_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "#|eval: false\n",
    "\n",
    "# from rl_opts.rl_framework.numba.environments import ResetEnv_2D\n",
    "# from rl_opts.rl_framework.numba.agents import Forager, train_loop_reset\n",
    "\n",
    "\n",
    "run_agents_reset_2D(10,10, 15, dist_target = 10, radius_target = 1, D = 1, \n",
    "                           size_state_space=np.array([3]),\n",
    "                          h_mat_allT=True, when_save_h_mat=5);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# nbdev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev import nbdev_export ; nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
