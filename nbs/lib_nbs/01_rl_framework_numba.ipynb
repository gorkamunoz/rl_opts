{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numba version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we update and improve the agents and environments presented in `rl_opts.rl_framework`. Two main changes:\n",
    "\n",
    "- we use `numba` to improve speed.\n",
    "- we implement more efficient ways of updating the H and G matrix (contribution by Dr. Michele Caraglio).\n",
    "- we consider as base case `num_agents = 1`. In previous versions we had this as an input which overcomplicated all functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp numba.rl_framework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#| hide\n",
    "# Libs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Class that defines the foraging environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import numpy as np\n",
    "import numba\n",
    "from numba.experimental import jitclass\n",
    "from numba import jit, float64, int64, bool_, prange, njit\n",
    "import math\n",
    "import random\n",
    "#from rl_opts.utils import isBetween_c_Vec, coord_mod\n",
    "NOPYTHON = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "#| exec: false\n",
    "# for debugging\n",
    "NOPYTHON = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helpers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## isBetween"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "@jit(nopython = NOPYTHON)\n",
    "def isBetween_c_Vec_numba(a, b, c, r):\n",
    "        \"\"\"\n",
    "        Checks whether point c is crossing the line formed with point a and b.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        a : tensor, shape = (1,2)\n",
    "            Previous position.\n",
    "        b : tensor, shape = (1,2)\n",
    "            Current position.\n",
    "        c : tensor, shape = (Nt,2)\n",
    "            Positions of all targets.\n",
    "        r : int/float\n",
    "            Target radius.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        mask : array of boolean values\n",
    "            True at the indices of found targets.\n",
    "\n",
    "        \"\"\"\n",
    "        if (a == b).all():\n",
    "            return np.array([False]*c.shape[0])\n",
    "\n",
    "        mask = np.array([True]*c.shape[0])\n",
    "        \n",
    "        dotproduct = (c[:, 0] - a[0]) * (b[0] - a[0]) + (c[:, 1] - a[1])*(b[1] - a[1])\n",
    "        squaredlengthba = (b[0] - a[0])*(b[0] - a[0]) + (b[1] - a[1])*(b[1] - a[1])\n",
    "        \n",
    "        #exclude the targets whose vertical projection of the vector c-a w.r.t. the vector b-a is larger than the target radius.\n",
    "        idx = np.argwhere(np.abs(numba.np.arraymath.cross2d(b-a, c-a))/np.linalg.norm(b-a) > r) \n",
    "        for i1 in idx:\n",
    "            mask[i1] = False        \n",
    "        \n",
    "        #exclude the targets whose scalar product is negative (they are on the other side of the step direction)\n",
    "        for i2 in np.argwhere(dotproduct < 0):\n",
    "            mask[i2] = False\n",
    "\n",
    "        #exclude the targets that are beyond the step.\n",
    "        for i3 in np.argwhere(dotproduct > squaredlengthba):\n",
    "            mask[i3] = False\n",
    "            \n",
    "        return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "compiling = isBetween_c_Vec_numba(np.array([0.1,1]), np.array([1,3]), np.random.rand(100,2), 0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.24 µs ± 7.62 ns per loop (mean ± std. dev. of 7 runs, 100,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit isBetween_c_Vec_numba(np.array([0.1,1]), np.array([1,5]), np.random.rand(100,2), 0.00001)\n",
    "# Run time of new version:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from rl_opts.utils import isBetween_c_Vec as oldbetween"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45.2 µs ± 2.9 µs per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit oldbetween(np.array([0.1,1]), np.array([1,3]), np.random.rand(100,2), 0.00001)\n",
    "# Run time of older version:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pareto sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "@jit(nopython = NOPYTHON)\n",
    "def pareto_sample(alpha : float, # Exponent of the power law\n",
    "                  xm : float, # Minimun value of the distribution\n",
    "                  size : int=1 # Number of samples\n",
    "                 )-> np.array : # Samples from the distribution\n",
    "    ''' Generates samples from a Pareto distribution of given parameters '''\n",
    "    samples = np.zeros(size)\n",
    "    for ii in range(size):\n",
    "        u = random.random()  # Uniform random variable between 0 and 1\n",
    "        x = xm / (u ** (1 / alpha))\n",
    "        samples[ii] = x\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random sampling from array with probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "@jit(nopython = NOPYTHON)\n",
    "def rand_choice_nb(arr : np.array, # 1D numpy array of values to sample from.\n",
    "                   prob : np.array # 1D numpy array of probabilities for the given samples.\n",
    "                  ): # Random sample from the given array with a given probability.    \n",
    "    return arr[np.searchsorted(np.cumsum(prob), np.random.random(), side=\"right\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TargetEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "@jitclass([(\"target_positions\", float64[:,:]) ,\n",
    "           (\"current_rewards\", float64[:]) ,\n",
    "           (\"kicked\", float64[:]) ,\n",
    "           (\"current_directions\", float64[:]) ,\n",
    "           (\"positions\", float64[:,:]),\n",
    "           (\"previous_pos\", float64[:,:]),\n",
    "           (\"lc\", float64[:,:]),\n",
    "           (\"mask\", bool_[:]),\n",
    "           (\"first_encounter\", float64[:,:])])\n",
    "class TargetEnv():\n",
    "    Nt : int\n",
    "    L : float\n",
    "    r : float\n",
    "    lc : np.array\n",
    "    agent_step : float\n",
    "    num_agents : int\n",
    "    destructive_targets : bool\n",
    "    target_positions : np.ndarray\n",
    "    current_rewards : np.array\n",
    "    kicked : np.array\n",
    "    current_directions : np.array\n",
    "    positions : np.array\n",
    "    previous_pos : np.array\n",
    "    mask : np.array\n",
    "    first_encounter : np.array\n",
    "    lc_distribution : str\n",
    "    \n",
    "    \n",
    "    def __init__(self,\n",
    "                 Nt = 10, # blabla\n",
    "                 L = 1.3,\n",
    "                 r = 1.5,\n",
    "                 lc = np.array([[1.0],[1]]),\n",
    "                 agent_step = 1,\n",
    "                 num_agents = 1,\n",
    "                 destructive = False,\n",
    "                 lc_distribution = 'constant'):\n",
    "        \n",
    "        \"\"\"        \n",
    "        Class defining the foraging environment. It includes the methods needed to place several agents to the world.\n",
    "        \n",
    "        Updated from `rl_framework.TargetEnv`:        \n",
    "            > `lc_distribution`: now allows to consider different distributions. lc now means different things depending on the distribution.\n",
    "            > `TargetEnv.update_pos_disp`: allows to update the position of the agent with a given displacement.            \n",
    "            \n",
    "        **Inputs**\n",
    "        \n",
    "        `Nt` : (int) \n",
    "            Number of targets.\n",
    "            \n",
    "        `L` : (int)\n",
    "            Size of the (squared) world.\n",
    "            \n",
    "        `r` : (int) \n",
    "            Radius with center the target position. It defines the area in which agent detects the target.\n",
    "            \n",
    "        `lc` \n",
    "            Cutoff length. Displacement away from target (to implement revisitable targets by displacing agent away from the visited target).\n",
    "            \n",
    "        `agent_step`: (int, optional)\n",
    "            Displacement of one step. The default is 1.\n",
    "            \n",
    "        `num_agents`: (int, optional)\n",
    "            Number of agents that forage at the same time. The default is 1.\n",
    "            \n",
    "        `destructive`: (bool, optional)\n",
    "            True if targets are destructive. The default is False.\n",
    "            \n",
    "        `lc_distribution`: (str) Chosee between 'power_law', 'pareto' and None. Depending on the previous, lc has different meanings:\n",
    "        \n",
    "        > `power_law` : lc is sampled from a power law x^{-1-alpha} where alpha = self.lc.flatten()[0] \n",
    "        \n",
    "        > `pareto` : lc is sampled from a Pareto distribution with alpha = self.lc.flatten()[0] and x_minim = self.lc.flatten()[0]\n",
    "        \n",
    "        > `None` : if len(lc) == 1, then that's the lc. If len(lc) > 1, then samples an lc considering vals = lc[0] and probabilities = lc[1]\n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "        self.Nt = Nt\n",
    "        self.L = L\n",
    "        self.r = r\n",
    "        self.lc = lc\n",
    "        self.agent_step = agent_step \n",
    "        self.num_agents = num_agents\n",
    "        self.destructive_targets = destructive\n",
    "        self.lc_distribution = lc_distribution\n",
    "        \n",
    "\n",
    "        self.init_env()\n",
    "        \n",
    "    def init_env(self):\n",
    "        \"\"\"\n",
    "        Environment initialization.\n",
    "        \"\"\"\n",
    "        # self.target_positions = np.random.rand(self.Nt, 2)*self.L\n",
    "        if self.r < self.L:\n",
    "            self.target_positions = np.random.uniform(self.r/2, self.L-self.r/2, size = (self.Nt, 2))\n",
    "        else: # This is just a rare, test case that puts all targets in the center if the target radius is huge\n",
    "            self.target_positions = np.ones((self.Nt, 2))*self.L/2\n",
    "            \n",
    "        \n",
    "        #store who is/was rewarded\n",
    "        self.current_rewards = np.zeros(self.num_agents)\n",
    "        \n",
    "        #signal whether agent has been kicked\n",
    "        self.kicked = np.zeros(self.num_agents)\n",
    "        \n",
    "        #set positions and directions of the agents\n",
    "        self.current_directions = np.random.rand(self.num_agents)*2*np.pi\n",
    "        self.positions = np.random.rand(self.num_agents, 2)*self.L\n",
    "        self.previous_pos = self.positions.copy()       \n",
    "\n",
    "        \n",
    "\n",
    "    def update_pos(self, change_direction, agent_index = 0):        \n",
    "        \"\"\"\n",
    "        Updates information of the agent depending on its decision.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        change_direction : bool\n",
    "            Whether the agent decided to turn or not.\n",
    "        agent_index : int, optional\n",
    "            Index of the given agent. Default is 0.\n",
    "        \"\"\"        \n",
    "        # Save previous position to check if crossing happened\n",
    "        self.previous_pos[agent_index] = self.positions[agent_index].copy()\n",
    "        \n",
    "        if change_direction:\n",
    "            self.current_directions[agent_index] = random.uniform(0,1)*2*math.pi\n",
    "        \n",
    "        #Update position\n",
    "        self.positions[agent_index][0] = self.positions[agent_index][0] + self.agent_step*np.cos(self.current_directions[agent_index])\n",
    "        self.positions[agent_index][1] = self.positions[agent_index][1] + self.agent_step*np.sin(self.current_directions[agent_index])\n",
    "        \n",
    "    def update_pos_disp(self, \n",
    "                        displacement, # tuple or array stating (disp_x, disp_y)                        \n",
    "                       ):\n",
    "        \"\"\"\n",
    "        Updates the position of the agent based on the input displacement\n",
    "        \"\"\"\n",
    "        agent_index = 0 # index of the agent. For collective experiments we should put this as input\n",
    "        \n",
    "        # Save previous position to check if crossing happened\n",
    "        self.previous_pos[agent_index] = self.positions[agent_index].copy()\n",
    "        \n",
    "        #Update position\n",
    "        self.positions[agent_index][0] = self.positions[agent_index][0] + displacement[0]\n",
    "        self.positions[agent_index][1] = self.positions[agent_index][1] + displacement[1]\n",
    "        \n",
    "\n",
    "\n",
    "    \n",
    "    def check_encounter(self):\n",
    "        \"\"\"\n",
    "        Checks whether the agent found a target, and updates the information accordingly.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        agent_index : int, optional\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        True if the agent found a target.\n",
    "\n",
    "        \"\"\"       \n",
    "        agent_index = 0\n",
    "        encounters = isBetween_c_Vec_numba(self.previous_pos[agent_index], self.positions[agent_index], self.target_positions, self.r)\n",
    "        \n",
    "        if sum(encounters) > 0: \n",
    "            \n",
    "            #if there is more than 1 encounter, pick the closest to the agent.\n",
    "            if sum(encounters) == 1:\n",
    "                first_encounter = np.argwhere(encounters == True).flatten()\n",
    "            else:\n",
    "                # compute the distance from the previous position to each target            \n",
    "                distance_previous_pos = np.sqrt((self.previous_pos[agent_index][0]- self.target_positions[:, 0])**2 + (self.previous_pos[agent_index][1] - self.target_positions[:, 1])**2)            \n",
    "                \n",
    "                # checking which encountered point is closer to previous position\n",
    "                min_distance_masked = np.argmin(distance_previous_pos[encounters])\n",
    "                first_encounter = np.argwhere(encounters == True)[min_distance_masked].flatten()\n",
    "            if self.destructive_targets:\n",
    "                self.target_positions[first_encounter] = np.random.rand(2)*self.L\n",
    "            else:\n",
    "                #----KICK----\n",
    "                # If there was encounter, we reset direction and change position of particle to (pos target + lc)\n",
    "                kick_direction = np.random.uniform(low = 0, high = 2*np.pi)\n",
    "                for idx_first in first_encounter: # This is super weird!\n",
    "                    if self.lc_distribution == 'power_law':\n",
    "                        # when we have the power law, the first value of lc is considered to be the exponent.\n",
    "                        # The following samples from a power law x^{-1-alpha} where alpha = self.lc.flatten()[0]                        \n",
    "                        current_lc = (1-random.uniform(0,1))**(-1/self.lc.flatten()[0])\n",
    "\n",
    "                    elif self.lc_distribution == 'pareto':\n",
    "                        # Sampling from Pareto. Here alpha = self.lc.flatten()[0] and x_minim = self.lc.flatten()[0]\n",
    "                        current_lc = pareto_sample(self.lc[0,0], self.lc[1,0])[0]\n",
    "                    else:\n",
    "                        # if lc has a single element, take that one as lc, if not sample\n",
    "                        current_lc = self.lc.flatten()[0] if len(self.lc.flatten()) == 2 else rand_choice_nb(arr = self.lc[0], prob = self.lc[1])\n",
    "                    self.positions[agent_index][0] = self.target_positions[idx_first, 0] + current_lc*np.cos(kick_direction)\n",
    "                    self.positions[agent_index][1] = self.target_positions[idx_first, 1] + current_lc*np.sin(kick_direction)\n",
    "                self.kicked[agent_index] = 1\n",
    "                #------------\n",
    "                \n",
    "            #...and we add the information that this agent got to the target\n",
    "            self.current_rewards[agent_index] = 1              \n",
    "            return 1\n",
    "        \n",
    "        else: \n",
    "            self.kicked[agent_index] = 0\n",
    "            self.current_rewards[agent_index] = 0\n",
    "            return 0   \n",
    "        \n",
    "    def check_bc(self):\n",
    "        \"\"\"\n",
    "        Updates position coordinates of agent agent_index to fulfill periodic boundary conditions.\n",
    "\n",
    "        \"\"\"\n",
    "        agent_index=0\n",
    "        self.positions[agent_index] = (self.positions[agent_index])%self.L\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Runtime testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from rl_opts.numba.rl_framework import TargetEnv "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = TargetEnv(Nt = 1000,\n",
    "                      L = 123,\n",
    "                      r = 50,\n",
    "                      lc = np.array([[0.1],[1]]),\n",
    "                      lc_distribution = 'pareto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.19 µs ± 7.96 ns per loop (mean ± std. dev. of 7 runs, 100,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit env.check_encounter()\n",
    "# Runtime of env.check_encounter():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rl_opts.rl_framework import TargetEnv as TargetEnv_classic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oenv = TargetEnv_classic(Nt = 100,\n",
    "                         L = 123,\n",
    "                         r = 0.2,\n",
    "                         lc = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "283 µs ± 211 ns per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit oenv.check_encounter()\n",
    "# Runtime of oenv.check_encounter():"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Walk from policy\n",
    "\n",
    "These replicate what we were doing in `rl_opts.learn_and_bench.walk_from_policy` and help get efficiencies for fixed policies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               \n",
    "@jit(nopython = NOPYTHON)\n",
    "def single_agent_walk(N_runs : int, # Total number of runs / episodes to evaluate\n",
    "                      time_ep : int, # Length of each run / episode\n",
    "                      policy : np.array, # Policy of the walker\n",
    "                      env : object# Environment where the walker moves\n",
    "                     )-> np.array :  # Array containing the number of targets from in each run\n",
    "\n",
    "    \"\"\"\n",
    "    Walk of a single in env of type TargetEnv given a policy. Performance is evaluated as the number of targets found in a fixed time time_ep.\n",
    "    \"\"\"\n",
    "    \n",
    "    save_rewards = np.zeros(N_runs)\n",
    "    \n",
    "    for ep in range(N_runs):\n",
    "        #initialize environment and agent's counter and g matrix\n",
    "        env.init_env()\n",
    "        agent_state = 0\n",
    "\n",
    "        for t in range(time_ep):\n",
    "            \n",
    "            if t == 0 or env.kicked[0]:\n",
    "                # change direction\n",
    "                env.update_pos(1)\n",
    "                #check boundary conditions\n",
    "                env.check_bc()\n",
    "                #reset counter\n",
    "                agent_state = 0\n",
    "                #set kicked value to false again\n",
    "                env.kicked[0] = 0\n",
    "                \n",
    "            else: \n",
    "                # decide\n",
    "                action = 0 if policy[0, agent_state] > np.random.rand() else 1\n",
    "                #update positions\n",
    "                env.update_pos(action)\n",
    "                #check if target was found + kick if it is\n",
    "                reward = env.check_encounter()\n",
    "                #check boundary conditions\n",
    "                env.check_bc()\n",
    "                # update agent_state\n",
    "                agent_state += 1\n",
    "                \n",
    "                save_rewards[ep] += reward\n",
    "                \n",
    "    return save_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@jit(nopython = NOPYTHON, parallel = True)\n",
    "def multi_agents_walk(N_runs : int, # Total number of runs / episodes to evaluate\n",
    "                      time_ep : int, # Length of each run / episode\n",
    "                      N_agents : int, # Number of agents to consider\n",
    "                      Nt = 100, # Number of targets in the environment\n",
    "                      L = 100, # Size of the environment\n",
    "                      r = 0.5, # Radius of the targets\n",
    "                      lc = 1.0, # Parameters of lc distribution or lc itself \n",
    "                      agent_step = 1, # Length of agent's step\n",
    "                      destructive_targets = False, # True if targets are destructive. The default is False. \n",
    "                      lc_distribution = 'constant', # lc distribution\n",
    "                      policy = [[1,1], [0,0]] # Policy of the agents\n",
    "              )-> np.array : # Array containing number of targets found for each agent at each run.\n",
    "    \"\"\"\n",
    "    Runs in parallel single_agent_walk. Due to numba props, we need to give all parameters as inputs (see source).\n",
    "    \"\"\"\n",
    "    \n",
    "    save_rewards = np.zeros((N_agents, N_runs))\n",
    "    \n",
    "    for n_agent in prange(N_agents):\n",
    "        \n",
    "        env = TargetEnv(Nt,L,r,lc,agent_step,1,destructive_targets,lc_distribution)\n",
    "        \n",
    "        rews = single_agent_walk(N_runs, time_ep, policy, env) \n",
    "    \n",
    "        save_rewards[n_agent] = rews\n",
    "        \n",
    "    return save_rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projective Simulation agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#| hide\n",
    "## Base Forager\n",
    "Here we do the numba implementation of `rl_opts.rl_framework.Forager`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "#| hide\n",
    "@jitclass([(\"num_percepts_list\", int64[:]),           \n",
    "           (\"initial_prob_distr\", float64[:,:]),           \n",
    "           (\"fixed_policy\", float64[:,:]) ,\n",
    "           (\"h_matrix\", float64[:,:]) ,\n",
    "           (\"g_matrix\", float64[:,:]) ,\n",
    "           (\"h_0\", float64[:,:]),\n",
    "           ])\n",
    "class _Forager_original():\n",
    "    num_actions : int\n",
    "    gamma_damping : float\n",
    "    eta_glow_damping : float\n",
    "    policy_type : str\n",
    "    beta_softmax : float\n",
    "    num_percepts : int\n",
    "    agent_state : int\n",
    "    num_percepts_list : np.array\n",
    "    initial_prob_distr : np.array\n",
    "    fixed_policy : np.array    \n",
    "    h_matrix : np.array\n",
    "    g_matrix : np.array\n",
    "    h_0 : np.array\n",
    "    \n",
    "    def __init__(self,\n",
    "                 num_actions, # Number of actions\n",
    "                 state_space, \n",
    "                 # List where each entry is the state space of each perceptual feature. \n",
    "                 # In general we only consider one perceptual feature (counter)\n",
    "                 gamma_damping=0.0, # Gamma of PS\n",
    "                 eta_glow_damping=0.0, # Glow of PS\n",
    "                 policy_type='standard', # Sampling of policy\n",
    "                 beta_softmax=3, # Parameters if policy is softmax\n",
    "                 initial_prob_distr = np.array([[],[]]), # Initial h-matrix\n",
    "                 fixed_policy=np.array([[],[]]) # If considering a fixed policy\n",
    "                ):\n",
    "        \"\"\"\n",
    "        Adapted to numba from rl_framework.Forager. This is an intermediate step to Forager with no efficient H and G updates.\n",
    "        To improve clarity we have changed num_percepts_list variable to state_space\n",
    "        \"\"\"\n",
    "        \n",
    "        self.agent_state = 0\n",
    "        \n",
    "        self.num_actions = num_actions       \n",
    "\n",
    "        \n",
    "        self.num_percepts_list = np.array([len(i) for i in state_space], dtype = np.int64) # change w.r.t PSAGENT\n",
    "        self.gamma_damping = gamma_damping\n",
    "        self.eta_glow_damping = eta_glow_damping\n",
    "        self.policy_type = policy_type\n",
    "        self.beta_softmax = beta_softmax\n",
    "        self.initial_prob_distr = initial_prob_distr\n",
    "        self.fixed_policy = fixed_policy\n",
    "        \n",
    "        self.num_percepts = int(np.prod(self.num_percepts_list)) # total number of possible percepts\n",
    "        \n",
    "        self.init_matrices()\n",
    "        \n",
    "    def init_matrices(self):\n",
    "\n",
    "        self.g_matrix = np.zeros((self.num_actions, self.num_percepts)) #glow matrix, for processing delayed rewards\n",
    "\n",
    "        # initialize h matrix with different values\n",
    "        if len(self.initial_prob_distr[0]) > 0:          \n",
    "            self.h_0 = self.initial_prob_distr\n",
    "            self.h_matrix = self.h_0.copy()\n",
    "        else: \n",
    "            self.h_matrix = np.ones((self.num_actions, self.num_percepts), dtype=np.float64) #Note: the first index specifies the action, the second index specifies the percept.\n",
    "            \n",
    "    def percept_preprocess(self, observation):\n",
    "        \"\"\"\n",
    "        Takes a multi-feature percept and reduces it to a single integer index.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        observation : ARRAY of integers >=0, of the same length as self.num_percepts_list\n",
    "            List that describes the observation. Each entry is the value that each feature takes in the observation.\n",
    "            observation[i] < num_percepts_list[i] (strictly)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        percept : int\n",
    "            Percept index that corresponds to the input observation.\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        percept = 0\n",
    "        for which_feature in range(len(observation)):\n",
    "            percept += int(observation[which_feature] * np.prod(self.num_percepts_list[:which_feature]))\n",
    "        return percept\n",
    "    \n",
    "    def deliberate(self, observation):\n",
    "        \"\"\"\n",
    "        Given an observation , this method chooses the next action and records that choice in the g_matrix.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        observation : list\n",
    "            List that describes the observation, as specified in percept_preprocess.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        action : int\n",
    "            Index of the chosen action.\n",
    "\n",
    "        \"\"\"\n",
    "        percept = self.percept_preprocess(observation) \n",
    "        action = rand_choice_nb(arr = np.arange(self.num_actions), prob = self.probability_distr(percept))\n",
    "        self.g_matrix = (1 - self.eta_glow_damping) * self.g_matrix\n",
    "        self.g_matrix[action, percept] += 1 #record latest decision in g_matrix\n",
    "        return action\n",
    "    \n",
    "    def probability_distr(self, percept):\n",
    "        \"\"\"\n",
    "        Given a percept index, this method returns a probability distribution over actions.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        percept : int\n",
    "            Index of the given percept.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        probability_distr : np.array, length = num_actions\n",
    "            Probability for each action (normalized to unit sum), computed according to policy_type.\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        if self.policy_type == 'standard':\n",
    "            h_vector = self.h_matrix[:, percept]\n",
    "            probability_distr = h_vector / np.sum(h_vector)\n",
    "        elif self.policy_type == 'softmax':\n",
    "            h_vector = self.beta_softmax * self.h_matrix[:, percept]\n",
    "            h_vector_mod = h_vector - np.max(h_vector)\n",
    "            probability_distr = np.exp(h_vector_mod) / np.sum(np.exp(h_vector_mod))\n",
    "        return probability_distr\n",
    "    \n",
    "    def learn(self, reward):\n",
    "        \"\"\"\n",
    "        Given a reward, this method updates the h matrix.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        reward : float\n",
    "            Value of the obtained reward.\n",
    "        \"\"\"\n",
    "        if len(self.initial_prob_distr[0]) > 0:\n",
    "            self.h_matrix =  self.h_matrix - self.gamma_damping * (self.h_matrix - self.h_0) + reward * self.g_matrix\n",
    "        else:\n",
    "            self.h_matrix =  self.h_matrix - self.gamma_damping * (self.h_matrix - 1.) + reward * self.g_matrix\n",
    "            \n",
    "    def reset_g(self):\n",
    "        \"\"\"\n",
    "        Resets the g_matrix.\n",
    "        \"\"\"\n",
    "        self.g_matrix = np.zeros((self.num_actions, self.num_percepts), dtype=np.float64)\n",
    "        \n",
    "    def deliberate_fixed_policy(self, observation):\n",
    "        \"\"\"\n",
    "        Given an observation , this method chooses the next action according to the fixed policy specified as attribute of the class.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        observation : list\n",
    "            List that describes the observation, as specified in percept_preprocess.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        action : int\n",
    "            Index of the chosen action.\n",
    "\n",
    "        \"\"\"\n",
    "        percept = self.percept_preprocess(observation) \n",
    "        if len(self.fixed_policy[0]) > 0:\n",
    "            action = rand_choice_nb(arr = np.arange(self.num_actions), prob = self.fixed_policy[percept])\n",
    "        else:\n",
    "            print('No fixed policy was given to the agent. The action will be selected randomly.')\n",
    "            action = np.random.choice(self.num_actions)\n",
    "    \n",
    "        self.g_matrix = (1 - self.eta_glow_damping) * self.g_matrix\n",
    "        self.g_matrix[action, percept] += 1 #record latest decision in g_matrix\n",
    "    \n",
    "        return action\n",
    "    \n",
    "    def act(self, action):\n",
    "        \"\"\"\n",
    "        Agent performs the given action.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        action : int (0, 1)\n",
    "            1 if it changes direction, 0 otherwise\n",
    "        \"\"\"\n",
    "        \n",
    "        # If the agent changes direction   \n",
    "        if action == 1:\n",
    "            self.agent_state = 0\n",
    "        else:\n",
    "            self.agent_state += 1  \n",
    "            \n",
    "    \n",
    "    def get_state(self):  \n",
    "        ''' simplified to case of single forager. Returns list because is what deliberate needs'''\n",
    "        return np.array([self.agent_state])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No fixed policy was given to the agent. The action will be selected randomly.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| hide\n",
    "#| exec : false\n",
    "agent = _Forager_original(num_actions = 2, state_space = np.array([np.arange(100)]))\n",
    "agent.percept_preprocess([0]*agent.num_percepts_list)\n",
    "agent.probability_distr(0)\n",
    "observation = [0]*agent.num_percepts_list[0]\n",
    "agent.deliberate(np.array(observation))\n",
    "agent.learn(1)\n",
    "agent.reset_g()\n",
    "agent.deliberate_fixed_policy(np.array(observation))\n",
    "agent.act(0)\n",
    "agent.get_state()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#| hide\n",
    "\n",
    "## Forager with efficient H update\n",
    "We use the formula $H_{t+i} = (1-\\gamma)^i H_t + \\gamma H_0 \\sum_{j=1}^i(1-\\gamma)^{j-1}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "#| hide\n",
    "@jitclass([(\"num_percepts_list\", int64[:]),           \n",
    "           (\"initial_prob_distr\", float64[:,:]),           \n",
    "           (\"fixed_policy\", float64[:,:]) ,\n",
    "           (\"h_matrix\", float64[:,:]) ,\n",
    "           (\"g_matrix\", float64[:,:]) ,\n",
    "           (\"h_0\", float64[:,:]),\n",
    "           (\"prefactor_1\", float64[:]),\n",
    "           (\"prefactor_2\", float64[:])\n",
    "          ])\n",
    "class _Forager_efficient_H():\n",
    "    num_actions : int\n",
    "    gamma_damping : float\n",
    "    eta_glow_damping : float\n",
    "    policy_type : str\n",
    "    beta_softmax : float\n",
    "    num_percepts : int\n",
    "    agent_state : int\n",
    "    max_no_update : int\n",
    "    counter_upd : int\n",
    "    num_percepts_list : np.array\n",
    "    initial_prob_distr : np.array\n",
    "    fixed_policy : np.array    \n",
    "    h_matrix : np.array\n",
    "    g_matrix : np.array\n",
    "    h_0 : np.array\n",
    "    prefactor_1: np.array\n",
    "    prefactor_2: np.array\n",
    "    \n",
    "    def __init__(self, num_actions, \n",
    "                 state_space, \n",
    "                 gamma_damping=0.0, \n",
    "                 eta_glow_damping=0.0, \n",
    "                 policy_type='standard', \n",
    "                 beta_softmax=3, \n",
    "                 initial_prob_distr = np.array([[],[]]), \n",
    "                 fixed_policy=np.array([[],[]]),\n",
    "                 max_no_update = int(1e4)\n",
    "                ):\n",
    "        \"\"\"\n",
    "        Improved agent from _Agent_original with efficient H update implemented\n",
    "        \"\"\"\n",
    "        \n",
    "        self.agent_state = 0\n",
    "        \n",
    "        self.num_actions = num_actions\n",
    "        self.num_percepts_list = np.array([len(i) for i in state_space], dtype = np.int64) # change w.r.t PSAGENT\n",
    "        self.gamma_damping = gamma_damping\n",
    "        self.eta_glow_damping = eta_glow_damping\n",
    "        self.policy_type = policy_type\n",
    "        self.beta_softmax = beta_softmax\n",
    "        self.initial_prob_distr = initial_prob_distr\n",
    "        self.fixed_policy = fixed_policy\n",
    "        \n",
    "        self.num_percepts = int(np.prod(self.num_percepts_list)) # total number of possible percepts\n",
    "        \n",
    "        self.init_matrices()\n",
    "        \n",
    "        self.max_no_update = max_no_update      \n",
    "        self.counter_upd = 0\n",
    "        self.prefactor_1 = (1-self.gamma_damping)**(np.arange(self.max_no_update+1)) \n",
    "        # This is the slow / easy to understand way of computing prefactor_2\n",
    "        # self.prefactor_2 = np.zeros(self.max_no_H_update+1)       \n",
    "        # for i in range(self.max_no_H_update):\n",
    "        #     self.prefactor_2[i+1] = self.gamma_damping*np.sum((1-self.gamma_damping)**np.arange(i+1))\n",
    "        # and this it the efficient way\n",
    "        sum_term = (1-self.gamma_damping)**np.arange(self.max_no_update+1)\n",
    "        self.prefactor_2 = self.gamma_damping*(np.cumsum(sum_term)-sum_term)\n",
    "        \n",
    "    def init_matrices(self):\n",
    "\n",
    "        self.g_matrix = np.zeros((self.num_actions, self.num_percepts)) #glow matrix, for processing delayed rewards\n",
    "\n",
    "        # initialize h matrix with different values\n",
    "        if len(self.initial_prob_distr[0]) > 0:          \n",
    "            self.h_0 = self.initial_prob_distr\n",
    "            self.h_matrix = self.h_0.copy()\n",
    "        else: \n",
    "            self.h_matrix = np.ones((self.num_actions, self.num_percepts), dtype=np.float64) #Note: the first index specifies the action, the second index specifies the percept.\n",
    "            \n",
    "    def _learn_post_reward(self, reward):\n",
    "        if self.counter_upd == 0:\n",
    "            print('Counter for h_matrix is zero, check that your are properly updating it!')\n",
    "        if len(self.initial_prob_distr[0]) > 0:\n",
    "            self.h_matrix = self.prefactor_1[self.counter_upd ] * self.h_matrix + self.prefactor_2[self.counter_upd] * self.h_0 + reward * self.g_matrix\n",
    "        else:\n",
    "            self.h_matrix = self.prefactor_1[self.counter_upd ] * self.h_matrix + self.prefactor_2[self.counter_upd] + reward * self.g_matrix\n",
    "        self.counter_upd = 0\n",
    "        \n",
    "    def _hmat_upd_single_percept(self, t, percept):\n",
    "        if len(self.initial_prob_distr[0]) > 0:\n",
    "            return self.prefactor_1[t] * self.h_matrix[:, percept] + self.prefactor_2[t] * self.h_0[:, percept]\n",
    "        else:\n",
    "            return self.prefactor_1[t] * self.h_matrix[:, percept] + self.prefactor_2[t] \n",
    "            \n",
    "            \n",
    "    def percept_preprocess(self, observation):\n",
    "        \"\"\"\n",
    "        Takes a multi-feature percept and reduces it to a single integer index.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        observation : ARRAY of integers >=0, of the same length as self.num_percepts_list\n",
    "            List that describes the observation. Each entry is the value that each feature takes in the observation.\n",
    "            observation[i] < num_percepts_list[i] (strictly)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        percept : int\n",
    "            Percept index that corresponds to the input observation.\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        percept = 0\n",
    "        for which_feature in range(len(observation)):\n",
    "            percept += int(observation[which_feature] * np.prod(self.num_percepts_list[:which_feature]))\n",
    "        return percept\n",
    "    \n",
    "    def deliberate(self, observation):\n",
    "        \"\"\"\n",
    "        Given an observation , this method chooses the next action and records that choice in the g_matrix.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        observation : list\n",
    "            List that describes the observation, as specified in percept_preprocess.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        action : int\n",
    "            Index of the chosen action.\n",
    "\n",
    "        \"\"\"\n",
    "        percept = self.percept_preprocess(observation) \n",
    "        \n",
    "        # Probabilities must be of update h_matrix. We feed the prob distr the update h_matrix\n",
    "        # for the percept, but don't update the h_matrix\n",
    "        current_h_mat = self._hmat_upd_single_percept(self.counter_upd, percept)\n",
    "        probs = self.probability_distr(percept, h_matrix = current_h_mat)\n",
    "        \n",
    "        action = rand_choice_nb(arr = np.arange(self.num_actions), prob = probs)\n",
    "        self.g_matrix = (1 - self.eta_glow_damping) * self.g_matrix\n",
    "        self.g_matrix[action, percept] += 1 #record latest decision in g_matrix\n",
    "        return action\n",
    "    \n",
    "    def probability_distr(self, percept, h_matrix = None):\n",
    "        \"\"\"\n",
    "        UPDATE (added the optional input)\n",
    "         \n",
    "        Given a percept index, this method returns a probability distribution over actions.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        percept : int\n",
    "            Index of the given percept.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        probability_distr : np.array, length = num_actions\n",
    "            Probability for each action (normalized to unit sum), computed according to policy_type.\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        if self.policy_type == 'standard':\n",
    "            h_vector = self.h_matrix[:, percept] if h_matrix is None else h_matrix\n",
    "            probability_distr = h_vector / np.sum(h_vector)\n",
    "        elif self.policy_type == 'softmax':\n",
    "            h_vector = self.beta_softmax * self.h_matrix[:, percept] if h_matrix is None else h_matrix\n",
    "            h_vector_mod = h_vector - np.max(h_vector)\n",
    "            probability_distr = np.exp(h_vector_mod) / np.sum(np.exp(h_vector_mod))\n",
    "        return probability_distr\n",
    "    \n",
    "    def learn(self, reward):\n",
    "        \"\"\"\n",
    "        Given a reward, this method updates the h matrix.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        reward : float\n",
    "            Value of the obtained reward.\n",
    "        \"\"\"\n",
    "        if len(self.initial_prob_distr[0]) > 0:\n",
    "            self.h_matrix =  self.h_matrix - self.gamma_damping * (self.h_matrix - self.h_0) + reward * self.g_matrix\n",
    "        else:\n",
    "            self.h_matrix =  self.h_matrix - self.gamma_damping * (self.h_matrix - 1.) + reward * self.g_matrix\n",
    "            \n",
    "    def reset_g(self):\n",
    "        \"\"\"\n",
    "        Resets the g_matrix.\n",
    "        \"\"\"\n",
    "        self.g_matrix = np.zeros((self.num_actions, self.num_percepts), dtype=np.float64)\n",
    "        \n",
    "    def deliberate_fixed_policy(self, observation):\n",
    "        \"\"\"\n",
    "        Given an observation , this method chooses the next action according to the fixed policy specified as attribute of the class.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        observation : list\n",
    "            List that describes the observation, as specified in percept_preprocess.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        action : int\n",
    "            Index of the chosen action.\n",
    "\n",
    "        \"\"\"\n",
    "        percept = self.percept_preprocess(observation) \n",
    "        if len(self.fixed_policy[0]) > 0:\n",
    "            action = rand_choice_nb(arr = np.arange(self.num_actions), prob = self.fixed_policy[percept])\n",
    "        else:\n",
    "            print('No fixed policy was given to the agent. The action will be selected randomly.')\n",
    "            action = np.random.choice(self.num_actions)\n",
    "    \n",
    "        self.g_matrix = (1 - self.eta_glow_damping) * self.g_matrix\n",
    "        self.g_matrix[action, percept] += 1 #record latest decision in g_matrix\n",
    "    \n",
    "        return action\n",
    "    \n",
    "    def act(self, action):\n",
    "        \"\"\"\n",
    "        Agent performs the given action.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        action : int (0, 1)\n",
    "            1 if it changes direction, 0 otherwise\n",
    "        \"\"\"\n",
    "        \n",
    "        # If the agent changes direction   \n",
    "        if action == 1:\n",
    "            self.agent_state = 0\n",
    "        else:\n",
    "            self.agent_state += 1  \n",
    "            \n",
    "    \n",
    "    def get_state(self):  \n",
    "        ''' simplified to case of single forager. Returns list because is what deliberate needs'''\n",
    "        return np.array([self.agent_state])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#| hide\n",
    "**Testing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "def test_train_loop_Heff(efficient, agent, episodes):\n",
    "    \n",
    "    for i in range(episodes):\n",
    "        \n",
    "        if efficient:\n",
    "            agent.counter_upd += 1\n",
    "        \n",
    "        state = np.array([i])\n",
    "        \n",
    "        if i % 2 == 0:\n",
    "            action = 0\n",
    "        else: 1\n",
    "        \n",
    "        # here is where glow matrix updates:\n",
    "        agent.g_matrix = (1 - agent.eta_glow_damping) * agent.g_matrix\n",
    "        agent.g_matrix[action, i] += 1 #record latest decision in g_matrix\n",
    "        \n",
    "        if i == 2 or i == 6:\n",
    "            reward = 1\n",
    "        else: reward = 0\n",
    "        \n",
    "        if efficient:\n",
    "            if reward == 1:\n",
    "                agent._learn_post_reward(reward)\n",
    "                agent.counter_upd = 0\n",
    "        else:\n",
    "            agent.learn(reward)\n",
    "\n",
    "    if efficient:\n",
    "        agent._learn_post_reward(reward)\n",
    "            \n",
    "    return agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#| hide\n",
    "**Value testing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from rl_opts.rl_framework import Forager as Forager_classic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "eps = 10\n",
    "gamma_damping = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.90236435, 2.90236435, 2.90236435, 1.970299  , 1.970299  ,\n",
       "        1.970299  , 1.970299  , 1.        , 1.        , 1.        ],\n",
       "       [1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| hide\n",
    "agent_noopt = Forager_classic(num_actions = 2,\n",
    "                              state_space = np.array([np.arange(eps)]),\n",
    "                              gamma_damping = gamma_damping)\n",
    "\n",
    "trained_noopt = test_train_loop_Heff(efficient = False, agent = agent_noopt, episodes = eps)\n",
    "trained_noopt.h_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.90236435, 2.90236435, 2.90236435, 1.970299  , 1.970299  ,\n",
       "        1.970299  , 1.970299  , 1.        , 1.        , 1.        ],\n",
       "       [1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| hide\n",
    "agent_opt = _Forager_efficient_H(num_actions = 2,\n",
    "                                 state_space = np.array([np.arange(eps)]),\n",
    "                                 gamma_damping = gamma_damping)\n",
    "\n",
    "trained = test_train_loop_Heff(efficient=True, agent = agent_opt, episodes = eps)\n",
    "trained.h_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'comparison old and efficient: -7.438494264988549e-15 ||||| IF value != 0, something is wrong!!!'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| hide\n",
    "f'comparison old and efficient: {(trained.h_matrix-trained_noopt.h_matrix).sum()} ||||| IF value != 0, something is wrong!!!'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#| hide\n",
    "Manual testing: we define an h-matrix and let it damp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "eps = 10\n",
    "len_hmat = 5\n",
    "gamma_damping = 0.001\n",
    "rand_h = [[2.0,2,4,5,1],\n",
    "          [3,3,1,1,1]]\n",
    "rand_h = np.array(rand_h)\n",
    "\n",
    "\n",
    "#np.random.rand(2, len_hmat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "agent_noopt = Forager_classic(num_actions = 2,\n",
    "                              state_space = np.array([np.arange(len_hmat)]),\n",
    "                              gamma_damping = gamma_damping)\n",
    "agent_noopt.h_matrix = rand_h.copy()\n",
    "\n",
    "for e in range(eps):\n",
    "    agent_noopt.learn(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.99004488, 1.99004488, 3.97013464, 4.96017952, 1.        ],\n",
       "       [2.98008976, 2.98008976, 1.        , 1.        , 1.        ]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| hide\n",
    "agent_noopt.h_matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "agent_opt = _Forager_efficient_H(num_actions = 2,\n",
    "                                 state_space = np.array([np.arange(len_hmat)]),\n",
    "                                 gamma_damping = gamma_damping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.99004488, 1.99004488, 3.97013464, 4.96017952, 1.        ],\n",
       "       [2.98008976, 2.98008976, 1.        , 1.        , 1.        ]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| hide\n",
    "agent_opt.h_matrix = rand_h.copy()\n",
    "\n",
    "agent_opt.counter_upd = 10\n",
    "agent_opt._learn_post_reward(0)\n",
    "agent_opt.h_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forager \n",
    "This agent is an efficient version of rl_opts.rl_framework.Forager with:\n",
    "\n",
    "- `numba` implementation\n",
    "-  H and G matrix efficient updates (contribution by Michele Caraglio)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "@jitclass([(\"size_state_space\", int64[:]),           \n",
    "           (\"initial_prob_distr\", float64[:,:]),           \n",
    "           (\"fixed_policy\", float64[:,:]) ,\n",
    "           (\"h_matrix\", float64[:,:]) ,\n",
    "           (\"g_matrix\", float64[:,:]) ,\n",
    "           (\"h_0\", float64[:,:]),\n",
    "           (\"prefactor_1\", float64[:]),\n",
    "           (\"prefactor_2\", float64[:]),\n",
    "           (\"last_upd_G\", float64[:,:])\n",
    "          ])\n",
    "class Forager():\n",
    "    num_actions : int\n",
    "    gamma_damping : float\n",
    "    eta_glow_damping : float\n",
    "    policy_type : str\n",
    "    beta_softmax : float\n",
    "    num_percepts : int\n",
    "    agent_state : int\n",
    "    size_state_space : np.array\n",
    "    initial_prob_distr : np.array\n",
    "    fixed_policy : np.array    \n",
    "    h_matrix : np.array\n",
    "    g_matrix : np.array\n",
    "    h_0 : np.array\n",
    "    # Efficient H update\n",
    "    prefactor_1: np.array\n",
    "    prefactor_2: np.array\n",
    "    max_no_H_update : int\n",
    "    N_upd_H : int\n",
    "    # Efficient G update\n",
    "    last_upd_G: np.array\n",
    "    N_upd_G: int\n",
    "    \n",
    "    def __init__(self, num_actions, # Number of actions\n",
    "                 size_state_space, \n",
    "                 # List where each entry is the state space of each perceptual feature. \n",
    "                 # In general we only consider one perceptual feature (counter)\n",
    "                 gamma_damping=0.0, # Gamma of PS\n",
    "                 eta_glow_damping=0.0, # Glow of PS\n",
    "                 policy_type='standard', # Sampling of policy\n",
    "                 beta_softmax=3, # Parameters if policy is softmax\n",
    "                 initial_prob_distr = np.array([[],[]]), # Initial h-matrix\n",
    "                 fixed_policy=np.array([[],[]]), # If considering a fixed policy\n",
    "                 max_no_H_update = int(1e4) # maximum number of steps before an update of H and G matrices.\n",
    "                ):\n",
    "\n",
    "        \n",
    "        \"\"\"\n",
    "        Updated version of the `rl_framework.Forager` class, with an efficient update both for the H-matrix and the G-matrix.\n",
    "\n",
    "        **Inputs**\n",
    "        \n",
    "        `num_actions` : \n",
    "            Number of actions\n",
    "            \n",
    "        `size_state_space` : \n",
    "             List where each entry is the state space of each perceptual feature. In general we only consider one perceptual feature (counter)\n",
    "        \n",
    "        `gamma_damping` :\n",
    "            Gamma of PS\n",
    "        \n",
    "        `eta_glow_damping` :\n",
    "            Glow of PS\n",
    "        \n",
    "        `policy_type` :\n",
    "            Sampling of policy\n",
    "        \n",
    "        `beta_softmax` :\n",
    "            Parameters if policy is softmax\n",
    "        \n",
    "        `initial_prob_distr` :\n",
    "            Initial h-matrix\n",
    "        \n",
    "        `fixed_policy` :\n",
    "            If considering a fixed policy\n",
    "        \n",
    "        `max_no_H_update` :\n",
    "            Maximum number of steps before an update of H and G matrices.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.agent_state = 0\n",
    "        \n",
    "        self.num_actions = num_actions\n",
    "        self.size_state_space = size_state_space\n",
    "        self.num_percepts = int(np.prod(self.size_state_space)) # total number of possible percepts\n",
    "        \n",
    "        self.gamma_damping = gamma_damping\n",
    "        self.eta_glow_damping = eta_glow_damping\n",
    "        self.policy_type = policy_type\n",
    "        self.beta_softmax = beta_softmax\n",
    "        self.initial_prob_distr = initial_prob_distr\n",
    "        self.fixed_policy = fixed_policy       \n",
    "        \n",
    "        self.init_matrices()\n",
    "        \n",
    "        # # For H update\n",
    "        self.max_no_H_update = max_no_H_update      \n",
    "        self.N_upd_H = 0\n",
    "        self.prefactor_1 = (1-self.gamma_damping)**(np.arange(self.max_no_H_update+1)) \n",
    "\n",
    "        # This is the slow / easy to understand way of computing prefactor_2\n",
    "        # self.prefactor_2 = np.zeros(self.max_no_H_update+1)       \n",
    "        # for i in range(self.max_no_H_update):\n",
    "        #     self.prefactor_2[i+1] = self.gamma_damping*np.sum((1-self.gamma_damping)**np.arange(i+1))\n",
    "        # and this it the efficient way\n",
    "        sum_term = (1-self.gamma_damping)**np.arange(self.max_no_H_update+1)\n",
    "        self.prefactor_2 = self.gamma_damping*(np.cumsum(sum_term)-sum_term)\n",
    "        \n",
    "        \n",
    "            \n",
    "        # For G update\n",
    "        self.last_upd_G = np.zeros((self.num_actions, self.num_percepts))\n",
    "        self.N_upd_G = 0\n",
    "                              \n",
    "        \n",
    "    def init_matrices(self):\n",
    "\n",
    "        self.g_matrix = np.zeros((self.num_actions, self.num_percepts)) #glow matrix, for processing delayed rewards\n",
    "\n",
    "        # initialize h matrix with different values\n",
    "        if len(self.initial_prob_distr[0]) > 0:          \n",
    "            self.h_0 = self.initial_prob_distr\n",
    "            self.h_matrix = self.h_0.copy()\n",
    "        else: \n",
    "            self.h_matrix = np.ones((self.num_actions, self.num_percepts), dtype=np.float64) #Note: the first index specifies the action, the second index specifies the percept.\n",
    "            \n",
    "    def _learn_post_reward(self, reward):\n",
    "        '''Given a reward, updates the whole H-matrix taking into account that we did not have updates\n",
    "        for the last N_upd_H steps.'''\n",
    "        # Update the full G matrix\n",
    "        self._G_upd_full()\n",
    "        \n",
    "        if self.N_upd_H == 0:\n",
    "            print('Counter for h_matrix is zero, check that your are properly updating it!')\n",
    "        if len(self.initial_prob_distr[0]) > 0:\n",
    "            self.h_matrix = self.prefactor_1[self.N_upd_H ] * self.h_matrix + self.prefactor_2[self.N_upd_H] * self.h_0 + reward * self.g_matrix\n",
    "        else:\n",
    "            self.h_matrix = self.prefactor_1[self.N_upd_H ] * self.h_matrix + self.prefactor_2[self.N_upd_H] + reward * self.g_matrix\n",
    "        self.N_upd_H = 0\n",
    "        \n",
    "    def _H_upd_single_percept(self, t, percept):\n",
    "        '''Given a percept and the time t passed since the last H-matrix update,\n",
    "        returns the corresponding --updated-- column of the H-matrix for all actions.\n",
    "        This updated is local and does no affect the H-matrix.'''\n",
    "        if len(self.initial_prob_distr[0]) > 0:\n",
    "            return self.prefactor_1[t] * self.h_matrix[:, percept] + self.prefactor_2[t] * self.h_0[:, percept]\n",
    "        else:\n",
    "            return self.prefactor_1[t] * self.h_matrix[:, percept] + self.prefactor_2[t] \n",
    "        \n",
    "    def _G_upd_single_percept(self, percept, action):\n",
    "        '''Given a percept-action tuple, updates that element of the G-matrix. Updates the last_upd_G\n",
    "        to keep track of when was the matrix updated.'''\n",
    "        # For the current (a,s) tuple, we damp and sum one\n",
    "        self.g_matrix[action, percept] = (1 - self.eta_glow_damping)**(self.N_upd_G - self.last_upd_G[action, percept])*self.g_matrix[action, percept] + 1\n",
    "        # Then update the last_upd matrix\n",
    "        self.last_upd_G[action, percept] = self.N_upd_G\n",
    "        \n",
    "    def _G_upd_full(self):\n",
    "        '''Given the current number of steps without an update, updates the whole G-matrix.\n",
    "        Then, resets all counters.'''\n",
    "        self.g_matrix = (1 - self.eta_glow_damping)**(self.N_upd_G - self.last_upd_G) * self.g_matrix\n",
    "        self.N_upd_G = 0\n",
    "        self.last_upd_G = np.zeros((self.num_actions, self.num_percepts))\n",
    "            \n",
    "            \n",
    "    def percept_preprocess(self, observation):\n",
    "        \"\"\"\n",
    "        Takes a multi-feature percept and reduces it to a single integer index.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        observation : ARRAY of integers >=0, of the same length as self.num_percepts_list\n",
    "            List that describes the observation. Each entry is the value that each feature takes in the observation.\n",
    "            observation[i] < num_percepts_list[i] (strictly)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        percept : int\n",
    "            Percept index that corresponds to the input observation.\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        percept = 0\n",
    "        for idx_obs, obs_feature in enumerate(observation):\n",
    "            percept += int(obs_feature * np.prod(self.size_state_space[:idx_obs]))  \n",
    "        return percept\n",
    "    \n",
    "    def deliberate(self, observation):\n",
    "        \"\"\"\n",
    "        Given an observation , this method chooses the next action and records that choice in the g_matrix.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        observation : list\n",
    "            List that describes the observation, as specified in percept_preprocess.\n",
    "        action : optional, bool\n",
    "            Mostly for debugging, we can input the action and no deliberation takes place, but g_matrix is updated\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        action : int\n",
    "            Index of the chosen action.\n",
    "\n",
    "        \"\"\"\n",
    "        percept = self.percept_preprocess(observation) \n",
    "        \n",
    "        \n",
    "        # Probabilities must be of update h_matrix. We feed the prob distr the update h_matrix\n",
    "        # for the percept, but don't update the h_matrix\n",
    "        current_h_mat = self._H_upd_single_percept(self.N_upd_H, percept)\n",
    "        probs = self.probability_distr(percept, h_matrix = current_h_mat)        \n",
    "        action = rand_choice_nb(arr = np.arange(self.num_actions), prob = probs)\n",
    "        \n",
    "        # Update the G matrix for current (s,a) tuple\n",
    "        self._G_upd_single_percept(percept, action)\n",
    "        \n",
    "        return action\n",
    "    \n",
    "    def probability_distr(self, percept, h_matrix = None):\n",
    "        \"\"\"\n",
    "        UPDATE (added the optional input)\n",
    "         \n",
    "        Given a percept index, this method returns a probability distribution over actions.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        percept : int\n",
    "            Index of the given percept.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        probability_distr : np.array, length = num_actions\n",
    "            Probability for each action (normalized to unit sum), computed according to policy_type.\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        if self.policy_type == 'standard':\n",
    "            h_vector = self.h_matrix[:, percept] if h_matrix is None else h_matrix\n",
    "            probability_distr = h_vector / np.sum(h_vector)\n",
    "        elif self.policy_type == 'softmax':\n",
    "            h_vector = self.beta_softmax * self.h_matrix[:, percept] if h_matrix is None else h_matrix\n",
    "            h_vector_mod = h_vector - np.max(h_vector)\n",
    "            probability_distr = np.exp(h_vector_mod) / np.sum(np.exp(h_vector_mod))\n",
    "        return probability_distr\n",
    "    \n",
    "    def learn(self, reward):\n",
    "        \"\"\"\n",
    "        Given a reward, this method updates the h matrix.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        reward : float\n",
    "            Value of the obtained reward.\n",
    "        \"\"\"\n",
    "        if len(self.initial_prob_distr[0]) > 0:\n",
    "            self.h_matrix =  self.h_matrix - self.gamma_damping * (self.h_matrix - self.h_0) + reward * self.g_matrix\n",
    "        else:\n",
    "            self.h_matrix =  self.h_matrix - self.gamma_damping * (self.h_matrix - 1.) + reward * self.g_matrix\n",
    "            \n",
    "    def reset_g(self):\n",
    "        \"\"\"\n",
    "        Resets the g_matrix.\n",
    "        \"\"\"\n",
    "        self.g_matrix = np.zeros((self.num_actions, self.num_percepts), dtype=np.float64)\n",
    "        \n",
    "    def deliberate_fixed_policy(self, observation):\n",
    "        \"\"\"\n",
    "        Given an observation , this method chooses the next action according to the fixed policy specified as attribute of the class.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        observation : list\n",
    "            List that describes the observation, as specified in percept_preprocess.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        action : int\n",
    "            Index of the chosen action.\n",
    "\n",
    "        \"\"\"\n",
    "        percept = self.percept_preprocess(observation) \n",
    "        if len(self.fixed_policy[0]) > 0:\n",
    "            action = rand_choice_nb(arr = np.arange(self.num_actions), prob = self.fixed_policy[percept])\n",
    "        else:\n",
    "            print('No fixed policy was given to the agent. The action will be selected randomly.')\n",
    "            action = np.random.choice(self.num_actions)\n",
    "    \n",
    "        self.g_matrix = (1 - self.eta_glow_damping) * self.g_matrix\n",
    "        self.g_matrix[action, percept] += 1 #record latest decision in g_matrix\n",
    "    \n",
    "        return action\n",
    "    \n",
    "    def act(self, action):\n",
    "        \"\"\"\n",
    "        Agent performs the given action.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        action : int (0, 1)\n",
    "            1 if it changes direction, 0 otherwise\n",
    "        \"\"\"\n",
    "        \n",
    "        # If the agent changes direction   \n",
    "        if action == 1:\n",
    "            self.agent_state = 0\n",
    "        else:\n",
    "            self.agent_state += 1  \n",
    "            \n",
    "    \n",
    "    def get_state(self):  \n",
    "        ''' simplified to case of single forager. Returns list because is what deliberate needs'''\n",
    "        return np.array([self.agent_state])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#| hide\n",
    "**Testing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# @jit\n",
    "def test_train_loop(efficient, agent, episodes):\n",
    "    \n",
    "    for i in range(episodes):\n",
    "        \n",
    "        if efficient:\n",
    "            agent.N_upd_H += 1\n",
    "            agent.N_upd_G += 1\n",
    "        \n",
    "        state = np.array([i])\n",
    "        \n",
    "        # Do determinist action to keep track\n",
    "        if i % 2 == 0:\n",
    "            action = 0\n",
    "        else: action = 1       \n",
    "        \n",
    "        # Because deterministic action, do update of G matrix by hand copying what is in agent.deliberate\n",
    "        agent.deliberate(state)\n",
    "        \n",
    "        if i == 2 or i == 6:\n",
    "            reward = 1\n",
    "        else: reward = 0\n",
    "        \n",
    "        if efficient:\n",
    "            if reward == 1:\n",
    "                agent._learn_post_reward(reward)\n",
    "        else:\n",
    "            agent.learn(reward)\n",
    "    \n",
    "    if efficient:\n",
    "        agent._learn_post_reward(reward)\n",
    "\n",
    "    \n",
    "            \n",
    "    return agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Value testing**\n",
    "\n",
    "Because actions are random, the row of the value will change. But in each column, you should have consistent values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rl_opts.rl_framework import Forager as Forager_classic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0.91, 0.  , 0.93, 0.94, 0.95, 0.96, 0.97, 0.98, 0.  , 1.  ],\n",
       "        [0.  , 0.92, 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.99, 0.  ]]),\n",
       " array([[2.91192, 1.     , 2.95074, 1.96739, 1.97716, 1.98703, 1.997  ,\n",
       "         1.     , 1.     , 1.     ],\n",
       "        [1.     , 2.93123, 1.     , 1.     , 1.     , 1.     , 1.     ,\n",
       "         1.     , 1.     , 1.     ]]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eps = 10\n",
    "agent_noopt = Forager_classic(num_actions = 2,\n",
    "                              state_space = np.array([np.arange(eps)]), eta_glow_damping = 0.01, gamma_damping = 0.001)\n",
    "trained_noopt = test_train_loop(efficient = False, agent = agent_noopt, episodes = eps)\n",
    "trained_noopt.g_matrix.round(2), trained_noopt.h_matrix.round(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0.  , 0.92, 0.  , 0.  , 0.  , 0.96, 0.  , 0.98, 0.99, 0.  ],\n",
       "        [0.91, 0.  , 0.93, 0.94, 0.95, 0.  , 0.97, 0.  , 0.  , 1.  ]]),\n",
       " array([[1.     , 2.93123, 1.     , 1.     , 1.     , 1.98703, 1.     ,\n",
       "         1.     , 1.     , 1.     ],\n",
       "        [2.91192, 1.     , 2.95074, 1.96739, 1.97716, 1.     , 1.997  ,\n",
       "         1.     , 1.     , 1.     ]]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_opt = Forager(num_actions = 2,\n",
    "                    size_state_space = np.array([eps]), eta_glow_damping = 0.01, gamma_damping = 0.001)\n",
    "trained = test_train_loop(efficient=True, agent = agent_opt, episodes = eps)\n",
    "\n",
    "trained.g_matrix.round(2), trained.h_matrix.round(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Runtime testing only H vs. full efficient**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps = int(1e4); eta = 0.1\n",
    "agent_noopt = _Forager_original(num_actions = 2,\n",
    "                                state_space = np.array([np.arange(eps)]), \n",
    "                                eta_glow_damping = eta)\n",
    "agent_opt = Forager(num_actions = 2,\n",
    "                    size_state_space = np.array([eps]), \n",
    "                    eta_glow_damping = eta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22.6 ms ± 122 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit test_train_loop(efficient=True, agent = agent_opt, episodes = eps)\n",
    "# Runtime new Forager (efficient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "401 ms ± 32.5 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit test_train_loop(efficient=False, agent = agent_noopt, episodes = eps)\n",
    "# Runtime old Forager "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Launch multi agent learning\n",
    "We now make use of the parallel option of `numba` to create launchers where many agents are trained at the same time:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "#| export                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               \n",
    "@jit(nopython = NOPYTHON)\n",
    "def _train_loop_original(episodes, # Number of episodes to train\n",
    "               time_ep, # Length of episode\n",
    "               agent, # Agent class\n",
    "               env # Environment class\n",
    "              ): # Rewards in each episode and time step and h_matrix of the trained agent \n",
    "    '''\n",
    "    Training loop for _Forager_original.\n",
    "    '''\n",
    "    \n",
    "    save_rewards = np.zeros((episodes, time_ep))\n",
    "    \n",
    "    for ep in range(episodes):\n",
    "        # print(f'starting episode {ep} for agent {n_agent}')\n",
    "        #initialize environment and agent's counter and g matrix\n",
    "        env.init_env()\n",
    "        agent.agent_state = 0\n",
    "        agent.reset_g()\n",
    "\n",
    "        for t in range(time_ep):\n",
    "\n",
    "            #step to set counter to its min value n=1\n",
    "            if t == 0 or env.kicked[0]:\n",
    "                #do one step with random direction (no learning in this step)\n",
    "                env.update_pos(1)\n",
    "                #check boundary conditions\n",
    "                env.check_bc()\n",
    "                #reset counter\n",
    "                agent.agent_state = 0\n",
    "                #set kicked value to false again\n",
    "                env.kicked[0] = 0\n",
    "                reward = 0\n",
    "\n",
    "            else:\n",
    "                #get perception\n",
    "                state = agent.get_state()\n",
    "                #decide\n",
    "                action = agent.deliberate(state)\n",
    "                #act (update counter)\n",
    "                agent.act(action)\n",
    "\n",
    "                #update positions\n",
    "                env.update_pos(action)\n",
    "                #check if target was found + kick if it is\n",
    "                reward = env.check_encounter()\n",
    "\n",
    "                #check boundary conditions\n",
    "                env.check_bc()\n",
    "                #learn\n",
    "\n",
    "                agent.learn(reward)                \n",
    "                    \n",
    "            save_rewards[ep, t] = reward\n",
    "      \n",
    "    \n",
    "    return save_rewards, agent.h_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "#| export                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               \n",
    "@jit(nopython = NOPYTHON)\n",
    "def _train_loop_h_efficient(episodes, time_ep, agent, env, h_mat_allT = False):  \n",
    "    '''\n",
    "    Training loop for _Forager_h_efficient.\n",
    "    '''\n",
    "\n",
    "    if h_mat_allT: policy_t = np.zeros((episodes, agent.h_matrix.shape[-1]))\n",
    "    save_rewards = np.zeros((episodes, time_ep))\n",
    "    \n",
    "    for ep in range(episodes):\n",
    "        # print(f'starting episode {ep} for agent {n_agent}')\n",
    "        #initialize environment and agent's counter and g matrix\n",
    "        env.init_env()\n",
    "        agent.agent_state = 0\n",
    "        agent.reset_g()\n",
    "\n",
    "        for t in range(time_ep):\n",
    "            agent.counter_upd += 1\n",
    "            \n",
    "            #step to set counter to its min value n=1\n",
    "            if t == 0 or env.kicked[0]:\n",
    "                #do one step with random direction (no learning in this step)\n",
    "                env.update_pos(1)\n",
    "                #check boundary conditions\n",
    "                env.check_bc()\n",
    "                #reset counter\n",
    "                agent.agent_state = 0\n",
    "                #set kicked value to false again\n",
    "                env.kicked[0] = 0\n",
    "                reward = 0\n",
    "\n",
    "            else:\n",
    "                #get perception\n",
    "                state = agent.get_state()\n",
    "                #decide\n",
    "                action = agent.deliberate(state)\n",
    "                #act (update counter)\n",
    "                agent.act(action)\n",
    "\n",
    "                #update positions\n",
    "                env.update_pos(action)\n",
    "                #check if target was found + kick if it is\n",
    "                reward = env.check_encounter()\n",
    "\n",
    "                #check boundary conditions\n",
    "                env.check_bc()\n",
    "                #learn\n",
    "                if reward == 1 or agent.counter_upd > agent.max_no_update:\n",
    "                    agent._learn_post_reward(reward)\n",
    "                    \n",
    "            # Saving\n",
    "            save_rewards[ep, t] = reward\n",
    "            if h_mat_allT: policy_t[ep] = agent.h_matrix[0,:] / agent.h_matrix.sum(0)\n",
    "      \n",
    "    return (save_rewards, policy_t) if h_mat_allT else (save_rewards, agent.h_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               \n",
    "@jit(nopython = NOPYTHON)\n",
    "def train_loop(episodes : int, # Number of episodes to train\n",
    "               time_ep : int, # Length of episode\n",
    "               agent : object, # Agent class\n",
    "               env : object, # Environment class\n",
    "               h_mat_allT : bool = False # If True, returns the h_matrix at all times\n",
    "              )-> tuple: # Rewards and h-matrix of the trained agent      \n",
    "    '''    \n",
    "    Given an agent and environment, performs a loop train for the `TargetEnv` type of environment, by adequatly\n",
    "    updating the H and G counters, considering boundaries, etc... \n",
    "    '''\n",
    "\n",
    "    if h_mat_allT: policy_t = np.zeros((episodes, agent.h_matrix.shape[-1]))\n",
    "    save_rewards = np.zeros((episodes, time_ep))\n",
    "    \n",
    "    for ep in range(episodes):\n",
    "        # print(f'starting episode {ep} for agent {n_agent}')\n",
    "        #initialize environment and agent's counter and g matrix\n",
    "        env.init_env()\n",
    "        agent.agent_state = 0\n",
    "        agent.reset_g()\n",
    "\n",
    "        for t in range(time_ep):\n",
    "            agent.N_upd_H += 1\n",
    "            agent.N_upd_G += 1\n",
    "            \n",
    "            #step to set counter to its min value n=1\n",
    "            if t == 0 or env.kicked[0]:\n",
    "                #do one step with random direction (no learning in this step)\n",
    "                env.update_pos(1)\n",
    "                #check boundary conditions\n",
    "                env.check_bc()\n",
    "                #reset counter\n",
    "                agent.agent_state = 0\n",
    "                #set kicked value to false again\n",
    "                env.kicked[0] = 0\n",
    "                reward = 0\n",
    "\n",
    "            else:\n",
    "                #get perception\n",
    "                state = agent.get_state()\n",
    "                #decide\n",
    "                action = agent.deliberate(state)\n",
    "                #act (update counter)\n",
    "                agent.act(action)\n",
    "\n",
    "                #update positions\n",
    "                env.update_pos(action)\n",
    "                #check if target was found + kick if it is\n",
    "                reward = env.check_encounter()\n",
    "\n",
    "                #check boundary conditions\n",
    "                env.check_bc()\n",
    "                #learn\n",
    "                if reward == 1:\n",
    "                    agent._learn_post_reward(reward)\n",
    "\n",
    "            if agent.N_upd_H == agent.max_no_H_update-1:\n",
    "                agent._learn_post_reward(reward)\n",
    "\n",
    "            # Saving\n",
    "            save_rewards[ep, t] = reward\n",
    "\n",
    "        # updating H and G at the last episode if there was no reward on it.\n",
    "        if ep == episodes-1 and reward != 1:\n",
    "            agent._learn_post_reward(reward)            \n",
    "            \n",
    "        if h_mat_allT: policy_t[ep] = agent.h_matrix[0,:] / agent.h_matrix.sum(0)\n",
    "\n",
    "    \n",
    "      \n",
    "    return (save_rewards, policy_t) if h_mat_allT else (save_rewards, agent.h_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Runtime testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = TargetEnv(Nt = 100,\n",
    "               L = 100, \n",
    "               r = 0.5, \n",
    "               lc = np.array([[1.0],[1]]), \n",
    "               agent_step = 1, \n",
    "               destructive = False, \n",
    "               lc_distribution = 'constant')\n",
    "\n",
    "\n",
    "agent = Forager(num_actions = 2, # From here are props of the agent (see Forager for details)\n",
    "               size_state_space = np.array([100]))\n",
    "\n",
    "agent_nopt = _Forager_original(num_actions = 2, # From here are props of the agent (see Forager for details)\n",
    "                     state_space = np.array([np.arange(100)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24.7 ms ± 18.8 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit _train_loop_original(episodes = 100, time_ep = 100, agent = agent_nopt, env = env)\n",
    "# Runtime without h-matrix efficient update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21 ms ± 57.6 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit train_loop(episodes = 100, time_ep = 100, agent = agent, env = env)\n",
    "# Runtime with h-matrix efficient update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run multiple agents in parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@jit(nopython = NOPYTHON, parallel = True)\n",
    "def run_agents(episodes, # Number of episodes\n",
    "               time_ep, # Length of episode\n",
    "               N_agents, # Number of agents               \n",
    "               h_mat_allT = False, # If to save the h_matrix at all times\n",
    "               efficient_agent = True, # If to consider efficient H and G agent (other options: 'only H' or False)\n",
    "               Nt = 100, # From here are props of the environment (see TargetEnv for details) \n",
    "               L = 100, \n",
    "               r = 0.5, \n",
    "               lc = np.array([[1.0],[1]]), \n",
    "               agent_step = 1, \n",
    "               destructive_targets = False, \n",
    "               lc_distribution = 'constant', \n",
    "               num_actions = 2, # From here are props of the agent (see Forager for details)\n",
    "               size_state_space = np.array([100]), \n",
    "               gamma_damping = 0.00001,\n",
    "               eta_glow_damping = 0.1,\n",
    "               initial_prob_distr = np.array([[],[]]),\n",
    "               policy_type = 'standard', \n",
    "               beta_softmax = 3,  \n",
    "               fixed_policy = np.array([[],[]]),\n",
    "               max_no_H_update = int(1e3) \n",
    "              ):\n",
    "    \n",
    "    save_rewards = np.zeros((N_agents, episodes))\n",
    "    if h_mat_allT:\n",
    "        save_h_matrix = np.zeros((N_agents, episodes, size_state_space[0]))  \n",
    "    else:        \n",
    "        save_h_matrix = np.zeros((N_agents, 2, size_state_space[0])) \n",
    "    \n",
    "    for n_agent in prange(N_agents):\n",
    "        \n",
    "        env = TargetEnv(Nt,L,r,lc,agent_step,1,destructive_targets,lc_distribution)\n",
    "\n",
    "\n",
    "        if efficient_agent == True: # Both G and H efficient update\n",
    "            agent = Forager(num_actions,size_state_space,gamma_damping,\n",
    "                            eta_glow_damping,policy_type,beta_softmax,\n",
    "                            initial_prob_distr,fixed_policy,max_no_H_update)\n",
    "            rews, mat = train_loop(episodes, time_ep, agent, env, h_mat_allT) \n",
    "        \n",
    "        elif efficient_agent == 'only H': # Only H efficient update\n",
    "            state_space = np.arange(size_state_space[0]).reshape(1, size_state_space[0])\n",
    "            agent = _Forager_efficient_H(num_actions,state_space,gamma_damping,\n",
    "                                        eta_glow_damping,policy_type,beta_softmax,\n",
    "                                        initial_prob_distr,fixed_policy,max_no_H_update)\n",
    "            rews, mat = _train_loop_h_efficient(episodes, time_ep, agent, env, h_mat_allT)  \n",
    "            \n",
    "        elif efficient_agent == False: # Old version without efficient updates            \n",
    "            state_space = np.arange(size_state_space[0]).reshape(1, size_state_space[0])\n",
    "            agent = _Forager_original(num_actions,state_space,gamma_damping,\n",
    "                            eta_glow_damping,policy_type,beta_softmax,\n",
    "                            initial_prob_distr,fixed_policy)\n",
    "            rews, mat = _train_loop_original(episodes, time_ep, agent, env)    \n",
    "    \n",
    "        for t in range(episodes):\n",
    "            save_rewards[n_agent, t] = np.mean(rews[t])\n",
    "        save_h_matrix[n_agent] = mat\n",
    "        \n",
    "    return save_rewards, save_h_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Runtime testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gorka/miniconda3/envs/rl_opts_main/lib/python3.10/site-packages/numba/np/ufunc/parallel.py:371: NumbaWarning: The TBB threading layer requires TBB version 2021 update 6 or later i.e., TBB_INTERFACE_VERSION >= 12060. Found TBB_INTERFACE_VERSION = 12050. The TBB threading layer is disabled.\n",
      "  warnings.warn(problem)\n"
     ]
    }
   ],
   "source": [
    "#| hide\n",
    "# For compiling and checking\n",
    "time_ep = 12000\n",
    "run_agents(episodes = 10, time_ep = time_ep, N_agents = 5, efficient_agent=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.26 s ± 35.7 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit run_agents(episodes = 100, time_ep = time_ep, N_agents = 10, size_state_space = np.array([100]), efficient_agent=False)\n",
    "# Runtime original in numba:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.66 s ± 11.4 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit k = run_agents(episodes = 100, time_ep = time_ep, N_agents = 10, size_state_space = np.array([100]), efficient_agent='only H')\n",
    "# Runtime original with only H efficient:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.66 s ± 20.6 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit k = run_agents(episodes = 100, time_ep = time_ep, N_agents = 10, size_state_space = np.array([100]), efficient_agent=True)\n",
    "# Runtime original with full H and G efficient:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Virtual environment \n",
    "This is a wrapper that merges the forager and agent whose only input is the displacement of a real particle at each step. The output is the action to be taken."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "#@jitclass\n",
    "class virtual_ABM:\n",
    "\n",
    "    def __init__(self, \n",
    "                 num_episodes : int = None, # If int, number of episodes. If None, don't consider episodes\n",
    "                 time_ep : int = None, # If int, length of an episodes. If episodes = None, it is not considered.\n",
    "                 L : float = 5, # Size of the environment\n",
    "                 r : float = 1, # Radius of the target       \n",
    "                 max_counter : int = 500, # Maximum number for the agent's phase counter (i.e. what it gets as station)\n",
    "                 gamma_damping : float = 0.001, # Gamma of PS\n",
    "                 eta_glow_damping : float = 0.001, # Glow of PS                 \n",
    "                 max_no_H_update : int = int(1e4) # maximum number of steps before an update of H and G matrices.\n",
    "                 ):\n",
    "        \n",
    "        # Arguments for TargetEnv\n",
    "        Nt = 1 # Number of targets\n",
    "        destructive = True\n",
    "        lc = np.array([[1.0],[1]]) # Won't enter into effect if destructive = True\n",
    "        lc_distribution = 'constant' # Won't enter into effect if destructive = True\n",
    "        agent_step = 1\n",
    "\n",
    "        self.env = TargetEnv(Nt, L, r, lc, agent_step, 1, destructive, lc_distribution)\n",
    "        \n",
    "        self.particle_position = self.env.positions[0]\n",
    "\n",
    "        # Arguments for Forager\n",
    "        num_actions = 2\n",
    "        self.max_counter = max_counter\n",
    "        size_state_space = np.array([2, self.max_counter]) # first is phase (either active or passive). Second is the counter till last change\n",
    "        policy_type='standard' # Sampling of policy\n",
    "        beta_softmax=3 # Parameters if policy is softmax\n",
    "        initial_prob_distr = np.array([[],[]]) # Initial h-matrix\n",
    "        fixed_policy=np.array([[],[]]) # If considering a fixed policy\n",
    "        \n",
    "        self.agent = Forager(num_actions,size_state_space,gamma_damping,\n",
    "                             eta_glow_damping,policy_type,beta_softmax,\n",
    "                             initial_prob_distr,fixed_policy,max_no_H_update)       \n",
    "        \n",
    "        self.num_episodes = num_episodes\n",
    "        self.time_ep = time_ep\n",
    "\n",
    "        self.init_training()\n",
    "        \n",
    "\n",
    "    def _particle_position(self):\n",
    "        return self.env.positions[0]\n",
    "    def _target_position(self):\n",
    "        return self.env.target_positions[0] \n",
    "    \n",
    "    def init_training(self):\n",
    "        ''' Initializes the environment and epochs '''\n",
    "        self.init_virtual_env()\n",
    "        \n",
    "        if self.num_episodes: \n",
    "            self.epoch = -1 # we start at -1 because init_epoch will sum one and set it at zero.\n",
    "            self.init_epoch()\n",
    "\n",
    "    def init_virtual_env(self):\n",
    "        # Current phase is: 0 for passive, 1 for active\n",
    "        self.current_phase = 0 # Starting always in the passive phase\n",
    "\n",
    "        # Initialize the environment (puts agent and target in random position)\n",
    "        self.env.init_env()\n",
    "\n",
    "        # Initialize / Reset agent's props\n",
    "        self.agent.agent_state = 0\n",
    "        self.agent.reset_g()\n",
    "        self.agent.N_upd_H = 0\n",
    "        self.agent.N_upd_G = 0\n",
    "\n",
    "    def init_epoch(self):\n",
    "        ''' Initializes a new epoch '''\n",
    "        self.init_virtual_env()\n",
    "        self.epoch += 1\n",
    "        self.t_ep = 0\n",
    "\n",
    "    def step(self, disp, return_reward = False):\n",
    "        ''' Makes a step in the virtual environment and subsequently learns'''\n",
    "\n",
    "        # Given the new displacement, update the environment and get the reward\n",
    "        self.env.update_pos_disp(disp)        \n",
    "             \n",
    "        # If we are in the passive phase, we check encounters with targets\n",
    "        if self.current_phase == 0:\n",
    "            reward = self.env.check_encounter()         \n",
    "        # If in active phase, we can't get targets hence reward is 0\n",
    "        else:\n",
    "            reward = 0\n",
    "\n",
    "        # Checking boundary conditions\n",
    "        self.env.check_bc()\n",
    "            \n",
    "        # Learn\n",
    "        # Now that we collected the reward, we have s,a,R and can learn        \n",
    "        # First we update the H update counter\n",
    "        self.agent.N_upd_H += 1        \n",
    "        # If the rewards are not zero or we reach the maximum no upd counters, we update\n",
    "        if (reward != 0) or (self.agent.N_upd_H == self.agent.max_no_H_update-1):\n",
    "            self.agent._learn_post_reward(reward)\n",
    "        \n",
    "        \n",
    "        # Acting\n",
    "        # Create a state based on current phase and agent state\n",
    "        state = np.array([self.current_phase, self.agent.agent_state])\n",
    "        # Get an action based on this state\n",
    "        action = self.agent.deliberate(state)\n",
    "        \n",
    "        if action == 0: # Continuing in the same phase\n",
    "            self.agent.agent_state += 1 \n",
    "        elif action == 1:\n",
    "            self.agent.agent_state = 0\n",
    "            self.current_phase = 1 - self.current_phase\n",
    "\n",
    "        if self.agent.agent_state == self.max_counter - 1:\n",
    "            self.agent.agent_state = 0\n",
    "\n",
    "        if self.num_episodes:\n",
    "            self.t_ep += 1\n",
    "            if self.t_ep == self.time_ep:\n",
    "                self.init_epoch()\n",
    "            \n",
    "\n",
    "        if return_reward:\n",
    "            return action, reward\n",
    "        else:\n",
    "            return action\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing on ABM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "#| hide\n",
    "def get_ABM_motion(x, y, theta, phi, vdt, sigma, sigma_theta, L):\n",
    "       \n",
    "    x += phi * vdt * np.cos(theta) + sigma * np.random.randn() \n",
    "    x = x % L\n",
    "\n",
    "    y += phi * vdt * np.sin(theta) + sigma * np.random.randn() \n",
    "    y = y % L\n",
    "\n",
    "    theta += sigma_theta * np.random.randn() \n",
    "    return x, y, theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from rl_opts.numba.rl_framework import TargetEnv, Forager\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "D = 2 # mum² s⁻¹\n",
    "L = 50 # mu² \n",
    "tau = 1.25 * 1e3 # s\n",
    "Pe = 100\n",
    "v = Pe * L / tau\n",
    "dt = tau*1e-4\n",
    "vdt = v*dt\n",
    "ell = 10\n",
    "D_theta = v/ell\n",
    "\n",
    "sigma = np.sqrt(2*D*dt);\n",
    "sigma_theta = np.sqrt(2*D_theta*dt)\n",
    "\n",
    "r = 2\n",
    "# venv = virtual_ABM(L = L, r = r, num_episodes = 50, time_ep = 5)\n",
    "venv = virtual_ABM(L = L, r = r, num_episodes = None)\n",
    "# manually creating longer passive phases:\n",
    "venv.agent.h_matrix[:500,0] *= 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "theta = 2*np.pi*np.random.rand()\n",
    "phi = 0\n",
    "T = 500\n",
    "\n",
    "\n",
    "venv.init_virtual_env()\n",
    "\n",
    "pos = np.zeros((T, 2))\n",
    "phase = np.zeros(T)\n",
    "rewards = np.zeros(T)\n",
    "pos[0] = venv.env.positions[0]\n",
    "pos_f = pos.copy()\n",
    "\n",
    "\n",
    "targets_pos = [venv.env.target_positions[0]]\n",
    "for t in range(1, T):    \n",
    "    # if t == int(T/2): phi = 1\n",
    "\n",
    "    x0, y0 = venv.env.positions[0].copy()\n",
    "    x1, y1, theta = get_ABM_motion(x0, y0, theta, phi, vdt, sigma, sigma_theta, L)\n",
    "    \n",
    "    \n",
    "    action, reward = venv.step((x1-x0, y1-y0), return_reward=True)\n",
    "    \n",
    "    if action == 1:\n",
    "        phi = 1 - phi\n",
    "        \n",
    "    \n",
    "    pos[t] = venv.env.positions[0].copy()\n",
    "    \n",
    "    phase[t] = venv.current_phase\n",
    "    if reward == 1: \n",
    "        print('sdf')\n",
    "        targets_pos.append(venv.env.target_positions[0])\n",
    "    rewards[t] = reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#| hide\n",
    "import matplotlib.patches as patches\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.plot(pos[:,0], pos[:,1])\n",
    "ax.scatter(pos[phase == 0,0], pos[phase == 0,1], c = 'r')\n",
    "ax.scatter(pos[phase == 1,0], pos[phase == 1,1], c = 'g')\n",
    "\n",
    "for tp in targets_pos:\n",
    "    circle = patches.Circle(tp, radius=r, linewidth=1, edgecolor='k', facecolor='none')\n",
    "    ax.add_patch(circle)\n",
    "\n",
    "ax.set_xlim(0, L)\n",
    "ax.set_ylim(0, L)\n",
    "ax.set_aspect('equal', 'box')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#| hide\n",
    "# nbdev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev import nbdev_export ; nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl_opts_origin",
   "language": "python",
   "name": "rl_opts_origin"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
