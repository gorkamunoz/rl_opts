{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numba version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we update and improve the agents and environments presented in `rl_opts.rl_framework`. Two main changes:\n",
    "\n",
    "- we use `numba` to improve speed.\n",
    "- we implement more efficient ways of updating the H and G matrix (contribution by Dr. Michele Caraglio).\n",
    "- we consider as base case `num_agents = 1`. In previous versions we had this as an input which overcomplicated all functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp numba.rl_framework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#| hide\n",
    "# Libs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Class that defines the foraging environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import numpy as np\n",
    "import numba\n",
    "from numba.experimental import jitclass\n",
    "from numba import jit, float64, int64, bool_, prange, njit\n",
    "import math\n",
    "import random\n",
    "#from rl_opts.utils import isBetween_c_Vec, coord_mod\n",
    "NOPYTHON = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "#| exec: false\n",
    "# for debugging\n",
    "NOPYTHON = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helpers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## isBetween"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "@jit(nopython = NOPYTHON)\n",
    "def isBetween_c_Vec_numba(a, b, c, r):\n",
    "        \"\"\"\n",
    "        Checks whether point c is crossing the line formed with point a and b.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        a : tensor, shape = (1,2)\n",
    "            Previous position.\n",
    "        b : tensor, shape = (1,2)\n",
    "            Current position.\n",
    "        c : tensor, shape = (Nt,2)\n",
    "            Positions of all targets.\n",
    "        r : int/float\n",
    "            Target radius.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        mask : array of boolean values\n",
    "            True at the indices of found targets.\n",
    "\n",
    "        \"\"\"\n",
    "        if (a == b).all():\n",
    "            return np.array([False]*c.shape[0])\n",
    "\n",
    "        mask = np.array([True]*c.shape[0])\n",
    "        \n",
    "        dotproduct = (c[:, 0] - a[0]) * (b[0] - a[0]) + (c[:, 1] - a[1])*(b[1] - a[1])\n",
    "        squaredlengthba = (b[0] - a[0])*(b[0] - a[0]) + (b[1] - a[1])*(b[1] - a[1])\n",
    "        \n",
    "        #exclude the targets whose vertical projection of the vector c-a w.r.t. the vector b-a is larger than the target radius.\n",
    "        idx = np.argwhere(np.abs(numba.np.arraymath.cross2d(b-a, c-a))/np.linalg.norm(b-a) > r) \n",
    "        for i1 in idx:\n",
    "            mask[i1] = False        \n",
    "        \n",
    "        #exclude the targets whose scalar product is negative (they are on the other side of the step direction)\n",
    "        for i2 in np.argwhere(dotproduct < 0):\n",
    "            mask[i2] = False\n",
    "\n",
    "        #exclude the targets that are beyond the step.\n",
    "        for i3 in np.argwhere(dotproduct > squaredlengthba):\n",
    "            mask[i3] = False\n",
    "            \n",
    "        return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "compiling = isBetween_c_Vec_numba(np.array([0.1,1]), np.array([1,3]), np.random.rand(100,2), 0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.24 µs ± 7.62 ns per loop (mean ± std. dev. of 7 runs, 100,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit isBetween_c_Vec_numba(np.array([0.1,1]), np.array([1,5]), np.random.rand(100,2), 0.00001)\n",
    "# Run time of new version:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from rl_opts.utils import isBetween_c_Vec as oldbetween"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45.2 µs ± 2.9 µs per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit oldbetween(np.array([0.1,1]), np.array([1,3]), np.random.rand(100,2), 0.00001)\n",
    "# Run time of older version:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pareto sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "@jit(nopython = NOPYTHON)\n",
    "def pareto_sample(alpha : float, # Exponent of the power law\n",
    "                  xm : float, # Minimun value of the distribution\n",
    "                  size : int=1 # Number of samples\n",
    "                 )-> np.array : # Samples from the distribution\n",
    "    ''' Generates samples from a Pareto distribution of given parameters '''\n",
    "    samples = np.zeros(size)\n",
    "    for ii in range(size):\n",
    "        u = random.random()  # Uniform random variable between 0 and 1\n",
    "        x = xm / (u ** (1 / alpha))\n",
    "        samples[ii] = x\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random sampling from array with probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "@jit(nopython = NOPYTHON)\n",
    "def rand_choice_nb(arr : np.array, # 1D numpy array of values to sample from.\n",
    "                   prob : np.array # 1D numpy array of probabilities for the given samples.\n",
    "                  ): # Random sample from the given array with a given probability.    \n",
    "    return arr[np.searchsorted(np.cumsum(prob), np.random.random(), side=\"right\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TargetEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "@jitclass([(\"target_positions\", float64[:,:]) ,\n",
    "           (\"current_rewards\", float64[:]) ,\n",
    "           (\"kicked\", float64[:]) ,\n",
    "           (\"current_directions\", float64[:]) ,\n",
    "           (\"positions\", float64[:,:]),\n",
    "           (\"previous_pos\", float64[:,:]),\n",
    "           (\"lc\", float64[:,:]),\n",
    "           (\"mask\", bool_[:]),\n",
    "           (\"first_encounter\", float64[:,:])])\n",
    "class TargetEnv():\n",
    "    Nt : int\n",
    "    L : float\n",
    "    r : float\n",
    "    lc : np.array\n",
    "    agent_step : float\n",
    "    num_agents : int\n",
    "    destructive_targets : bool\n",
    "    target_positions : np.ndarray\n",
    "    current_rewards : np.array\n",
    "    kicked : np.array\n",
    "    current_directions : np.array\n",
    "    positions : np.array\n",
    "    previous_pos : np.array\n",
    "    mask : np.array\n",
    "    first_encounter : np.array\n",
    "    lc_distribution : str\n",
    "    \n",
    "    \n",
    "    def __init__(self,\n",
    "                 Nt = 10, # blabla\n",
    "                 L = 1.3,\n",
    "                 r = 1.5,\n",
    "                 lc = np.array([[1.0],[1]]),\n",
    "                 agent_step = 1,\n",
    "                 num_agents = 1,\n",
    "                 destructive = False,\n",
    "                 lc_distribution = 'constant'):\n",
    "        \n",
    "        \"\"\"        \n",
    "        Class defining the foraging environment. It includes the methods needed to place several agents to the world.\n",
    "        \n",
    "        Updated from `rl_framework.TargetEnv`:        \n",
    "            > `lc_distribution`: now allows to consider different distributions. lc now means different things depending on the distribution.\n",
    "            > `TargetEnv.update_pos_disp`: allows to update the position of the agent with a given displacement.            \n",
    "            \n",
    "        **Inputs**\n",
    "        \n",
    "        `Nt` : (int) \n",
    "            Number of targets.\n",
    "            \n",
    "        `L` : (int)\n",
    "            Size of the (squared) world.\n",
    "            \n",
    "        `r` : (int) \n",
    "            Radius with center the target position. It defines the area in which agent detects the target.\n",
    "            \n",
    "        `lc` \n",
    "            Cutoff length. Displacement away from target (to implement revisitable targets by displacing agent away from the visited target).\n",
    "            \n",
    "        `agent_step`: (int, optional)\n",
    "            Displacement of one step. The default is 1.\n",
    "            \n",
    "        `num_agents`: (int, optional)\n",
    "            Number of agents that forage at the same time. The default is 1.\n",
    "            \n",
    "        `destructive`: (bool, optional)\n",
    "            True if targets are destructive. The default is False.\n",
    "            \n",
    "        `lc_distribution`: (str) Chosee between 'power_law', 'pareto' and None. Depending on the previous, lc has different meanings:\n",
    "        \n",
    "        > `power_law` : lc is sampled from a power law x^{-1-alpha} where alpha = self.lc.flatten()[0] \n",
    "        \n",
    "        > `pareto` : lc is sampled from a Pareto distribution with alpha = self.lc.flatten()[0] and x_minim = self.lc.flatten()[0]\n",
    "        \n",
    "        > `None` : if len(lc) == 1, then that's the lc. If len(lc) > 1, then samples an lc considering vals = lc[0] and probabilities = lc[1]\n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "        self.Nt = Nt\n",
    "        self.L = L\n",
    "        self.r = r\n",
    "        self.lc = lc\n",
    "        self.agent_step = agent_step \n",
    "        self.num_agents = num_agents\n",
    "        self.destructive_targets = destructive\n",
    "        self.lc_distribution = lc_distribution\n",
    "        \n",
    "\n",
    "        self.init_env()\n",
    "        \n",
    "    def init_env(self):\n",
    "        \"\"\"\n",
    "        Environment initialization.\n",
    "        \"\"\"\n",
    "        # self.target_positions = np.random.rand(self.Nt, 2)*self.L\n",
    "        self.target_positions = np.random.uniform(self.r, self.L-self.r, size = (self.Nt, 2))\n",
    "        \n",
    "        #store who is/was rewarded\n",
    "        self.current_rewards = np.zeros(self.num_agents)\n",
    "        \n",
    "        #signal whether agent has been kicked\n",
    "        self.kicked = np.zeros(self.num_agents)\n",
    "        \n",
    "        #set positions and directions of the agents\n",
    "        self.current_directions = np.random.rand(self.num_agents)*2*np.pi\n",
    "        self.positions = np.random.rand(self.num_agents, 2)*self.L\n",
    "        self.previous_pos = self.positions.copy()       \n",
    "\n",
    "        \n",
    "\n",
    "    def update_pos(self, change_direction, agent_index = 0):        \n",
    "        \"\"\"\n",
    "        Updates information of the agent depending on its decision.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        change_direction : bool\n",
    "            Whether the agent decided to turn or not.\n",
    "        agent_index : int, optional\n",
    "            Index of the given agent. Default is 0.\n",
    "        \"\"\"        \n",
    "        # Save previous position to check if crossing happened\n",
    "        self.previous_pos[agent_index] = self.positions[agent_index].copy()\n",
    "        \n",
    "        if change_direction:\n",
    "            self.current_directions[agent_index] = random.uniform(0,1)*2*math.pi\n",
    "        \n",
    "        #Update position\n",
    "        self.positions[agent_index][0] = self.positions[agent_index][0] + self.agent_step*np.cos(self.current_directions[agent_index])\n",
    "        self.positions[agent_index][1] = self.positions[agent_index][1] + self.agent_step*np.sin(self.current_directions[agent_index])\n",
    "        \n",
    "    def update_pos_disp(self, \n",
    "                        displacement, # tuple or array stating (disp_x, disp_y)                        \n",
    "                       ):\n",
    "        \"\"\"\n",
    "        Updates the position of the agent based on the input displacement\n",
    "        \"\"\"\n",
    "        agent_index = 0 # index of the agent. For collective experiments we should put this as input\n",
    "        \n",
    "        # Save previous position to check if crossing happened\n",
    "        self.previous_pos[agent_index] = self.positions[agent_index].copy()\n",
    "        \n",
    "        #Update position\n",
    "        self.positions[agent_index][0] = self.positions[agent_index][0] + displacement[0]\n",
    "        self.positions[agent_index][1] = self.positions[agent_index][1] + displacement[1]\n",
    "        \n",
    "\n",
    "\n",
    "    \n",
    "    def check_encounter(self):\n",
    "        \"\"\"\n",
    "        Checks whether the agent found a target, and updates the information accordingly.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        agent_index : int, optional\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        True if the agent found a target.\n",
    "\n",
    "        \"\"\"       \n",
    "        agent_index = 0\n",
    "        encounters = isBetween_c_Vec_numba(self.previous_pos[agent_index], self.positions[agent_index], self.target_positions, self.r)\n",
    "        \n",
    "        if sum(encounters) > 0: \n",
    "            \n",
    "            #if there is more than 1 encounter, pick the closest to the agent.\n",
    "            if sum(encounters) == 1:\n",
    "                first_encounter = np.argwhere(encounters == True).flatten()\n",
    "            else:\n",
    "                # compute the distance from the previous position to each target            \n",
    "                distance_previous_pos = np.sqrt((self.previous_pos[agent_index][0]- self.target_positions[:, 0])**2 + (self.previous_pos[agent_index][1] - self.target_positions[:, 1])**2)            \n",
    "                \n",
    "                # checking which encountered point is closer to previous position\n",
    "                min_distance_masked = np.argmin(distance_previous_pos[encounters])\n",
    "                first_encounter = np.argwhere(encounters == True)[min_distance_masked].flatten()\n",
    "            if self.destructive_targets:\n",
    "                self.target_positions[first_encounter] = np.random.rand(2)*self.L\n",
    "            else:\n",
    "                #----KICK----\n",
    "                # If there was encounter, we reset direction and change position of particle to (pos target + lc)\n",
    "                kick_direction = np.random.uniform(low = 0, high = 2*np.pi)\n",
    "                for idx_first in first_encounter: # This is super weird!\n",
    "                    if self.lc_distribution == 'power_law':\n",
    "                        # when we have the power law, the first value of lc is considered to be the exponent.\n",
    "                        # The following samples from a power law x^{-1-alpha} where alpha = self.lc.flatten()[0]                        \n",
    "                        current_lc = (1-random.uniform(0,1))**(-1/self.lc.flatten()[0])\n",
    "\n",
    "                    elif self.lc_distribution == 'pareto':\n",
    "                        # Sampling from Pareto. Here alpha = self.lc.flatten()[0] and x_minim = self.lc.flatten()[0]\n",
    "                        current_lc = pareto_sample(self.lc[0,0], self.lc[1,0])[0]\n",
    "                    else:\n",
    "                        # if lc has a single element, take that one as lc, if not sample\n",
    "                        current_lc = self.lc.flatten()[0] if len(self.lc.flatten()) == 2 else rand_choice_nb(arr = self.lc[0], prob = self.lc[1])\n",
    "                    self.positions[agent_index][0] = self.target_positions[idx_first, 0] + current_lc*np.cos(kick_direction)\n",
    "                    self.positions[agent_index][1] = self.target_positions[idx_first, 1] + current_lc*np.sin(kick_direction)\n",
    "                self.kicked[agent_index] = 1\n",
    "                #------------\n",
    "                \n",
    "            #...and we add the information that this agent got to the target\n",
    "            self.current_rewards[agent_index] = 1              \n",
    "            return 1\n",
    "        \n",
    "        else: \n",
    "            self.kicked[agent_index] = 0\n",
    "            self.current_rewards[agent_index] = 0\n",
    "            return 0   \n",
    "        \n",
    "    def check_bc(self):\n",
    "        \"\"\"\n",
    "        Updates position coordinates of agent agent_index to fulfill periodic boundary conditions.\n",
    "\n",
    "        \"\"\"\n",
    "        agent_index=0\n",
    "        self.positions[agent_index] = (self.positions[agent_index])%self.L\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Runtime testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from rl_opts.numba.rl_framework import TargetEnv "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = TargetEnv(Nt = 1000,\n",
    "                      L = 123,\n",
    "                      r = 50,\n",
    "                      lc = np.array([[0.1],[1]]),\n",
    "                      lc_distribution = 'pareto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.19 µs ± 7.96 ns per loop (mean ± std. dev. of 7 runs, 100,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit env.check_encounter()\n",
    "# Runtime of env.check_encounter():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rl_opts.rl_framework import TargetEnv as TargetEnv_classic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oenv = TargetEnv_classic(Nt = 100,\n",
    "                         L = 123,\n",
    "                         r = 0.2,\n",
    "                         lc = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "283 µs ± 211 ns per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit oenv.check_encounter()\n",
    "# Runtime of oenv.check_encounter():"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Walk from policy\n",
    "\n",
    "These replicate what we were doing in `rl_opts.learn_and_bench.walk_from_policy` and help get efficiencies for fixed policies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               \n",
    "@jit(nopython = NOPYTHON)\n",
    "def single_agent_walk(N_runs : int, # Total number of runs / episodes to evaluate\n",
    "                      time_ep : int, # Length of each run / episode\n",
    "                      policy : np.array, # Policy of the walker\n",
    "                      env : object# Environment where the walker moves\n",
    "                     )-> np.array :  # Array containing the number of targets from in each run\n",
    "\n",
    "    \"\"\"\n",
    "    Walk of a single in env of type TargetEnv given a policy. Performance is evaluated as the number of targets found in a fixed time time_ep.\n",
    "    \"\"\"\n",
    "    \n",
    "    save_rewards = np.zeros(N_runs)\n",
    "    \n",
    "    for ep in range(N_runs):\n",
    "        #initialize environment and agent's counter and g matrix\n",
    "        env.init_env()\n",
    "        agent_state = 0\n",
    "\n",
    "        for t in range(time_ep):\n",
    "            \n",
    "            if t == 0 or env.kicked[0]:\n",
    "                # change direction\n",
    "                env.update_pos(1)\n",
    "                #check boundary conditions\n",
    "                env.check_bc()\n",
    "                #reset counter\n",
    "                agent_state = 0\n",
    "                #set kicked value to false again\n",
    "                env.kicked[0] = 0\n",
    "                \n",
    "            else: \n",
    "                # decide\n",
    "                action = 0 if policy[0, agent_state] > np.random.rand() else 1\n",
    "                #update positions\n",
    "                env.update_pos(action)\n",
    "                #check if target was found + kick if it is\n",
    "                reward = env.check_encounter()\n",
    "                #check boundary conditions\n",
    "                env.check_bc()\n",
    "                # update agent_state\n",
    "                agent_state += 1\n",
    "                \n",
    "                save_rewards[ep] += reward\n",
    "                \n",
    "    return save_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@jit(nopython = NOPYTHON, parallel = True)\n",
    "def multi_agents_walk(N_runs : int, # Total number of runs / episodes to evaluate\n",
    "                      time_ep : int, # Length of each run / episode\n",
    "                      N_agents : int, # Number of agents to consider\n",
    "                      Nt = 100, # Number of targets in the environment\n",
    "                      L = 100, # Size of the environment\n",
    "                      r = 0.5, # Radius of the targets\n",
    "                      lc = 1.0, # Parameters of lc distribution or lc itself \n",
    "                      agent_step = 1, # Length of agent's step\n",
    "                      destructive_targets = False, # True if targets are destructive. The default is False. \n",
    "                      lc_distribution = 'constant', # lc distribution\n",
    "                      policy = [[1,1], [0,0]] # Policy of the agents\n",
    "              )-> np.array : # Array containing number of targets found for each agent at each run.\n",
    "    \"\"\"\n",
    "    Runs in parallel single_agent_walk. Due to numba props, we need to give all parameters as inputs (see source).\n",
    "    \"\"\"\n",
    "    \n",
    "    save_rewards = np.zeros((N_agents, N_runs))\n",
    "    \n",
    "    for n_agent in prange(N_agents):\n",
    "        \n",
    "        env = TargetEnv(Nt,L,r,lc,agent_step,1,destructive_targets,lc_distribution)\n",
    "        \n",
    "        rews = single_agent_walk(N_runs, time_ep, policy, env) \n",
    "    \n",
    "        save_rewards[n_agent] = rews\n",
    "        \n",
    "    return save_rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projective Simulation agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#| hide\n",
    "## Base Forager\n",
    "Here we do the numba implementation of `rl_opts.rl_framework.Forager`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "#| hide\n",
    "@jitclass([(\"num_percepts_list\", int64[:]),           \n",
    "           (\"initial_prob_distr\", float64[:,:]),           \n",
    "           (\"fixed_policy\", float64[:,:]) ,\n",
    "           (\"h_matrix\", float64[:,:]) ,\n",
    "           (\"g_matrix\", float64[:,:]) ,\n",
    "           (\"h_0\", float64[:,:]),\n",
    "           ])\n",
    "class _Forager_original():\n",
    "    num_actions : int\n",
    "    gamma_damping : float\n",
    "    eta_glow_damping : float\n",
    "    policy_type : str\n",
    "    beta_softmax : float\n",
    "    num_percepts : int\n",
    "    agent_state : int\n",
    "    num_percepts_list : np.array\n",
    "    initial_prob_distr : np.array\n",
    "    fixed_policy : np.array    \n",
    "    h_matrix : np.array\n",
    "    g_matrix : np.array\n",
    "    h_0 : np.array\n",
    "    \n",
    "    def __init__(self,\n",
    "                 num_actions, # Number of actions\n",
    "                 state_space, \n",
    "                 # List where each entry is the state space of each perceptual feature. \n",
    "                 # In general we only consider one perceptual feature (counter)\n",
    "                 gamma_damping=0.0, # Gamma of PS\n",
    "                 eta_glow_damping=0.0, # Glow of PS\n",
    "                 policy_type='standard', # Sampling of policy\n",
    "                 beta_softmax=3, # Parameters if policy is softmax\n",
    "                 initial_prob_distr = np.array([[],[]]), # Initial h-matrix\n",
    "                 fixed_policy=np.array([[],[]]) # If considering a fixed policy\n",
    "                ):\n",
    "        \"\"\"\n",
    "        Adapted to numba from rl_framework.Forager. This is an intermediate step to Forager with no efficient H and G updates.\n",
    "        To improve clarity we have changed num_percepts_list variable to state_space\n",
    "        \"\"\"\n",
    "        \n",
    "        self.agent_state = 0\n",
    "        \n",
    "        self.num_actions = num_actions       \n",
    "\n",
    "        \n",
    "        self.num_percepts_list = np.array([len(i) for i in state_space], dtype = np.int64) # change w.r.t PSAGENT\n",
    "        self.gamma_damping = gamma_damping\n",
    "        self.eta_glow_damping = eta_glow_damping\n",
    "        self.policy_type = policy_type\n",
    "        self.beta_softmax = beta_softmax\n",
    "        self.initial_prob_distr = initial_prob_distr\n",
    "        self.fixed_policy = fixed_policy\n",
    "        \n",
    "        self.num_percepts = int(np.prod(self.num_percepts_list)) # total number of possible percepts\n",
    "        \n",
    "        self.init_matrices()\n",
    "        \n",
    "    def init_matrices(self):\n",
    "\n",
    "        self.g_matrix = np.zeros((self.num_actions, self.num_percepts)) #glow matrix, for processing delayed rewards\n",
    "\n",
    "        # initialize h matrix with different values\n",
    "        if len(self.initial_prob_distr[0]) > 0:          \n",
    "            self.h_0 = self.initial_prob_distr\n",
    "            self.h_matrix = self.h_0.copy()\n",
    "        else: \n",
    "            self.h_matrix = np.ones((self.num_actions, self.num_percepts), dtype=np.float64) #Note: the first index specifies the action, the second index specifies the percept.\n",
    "            \n",
    "    def percept_preprocess(self, observation):\n",
    "        \"\"\"\n",
    "        Takes a multi-feature percept and reduces it to a single integer index.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        observation : ARRAY of integers >=0, of the same length as self.num_percepts_list\n",
    "            List that describes the observation. Each entry is the value that each feature takes in the observation.\n",
    "            observation[i] < num_percepts_list[i] (strictly)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        percept : int\n",
    "            Percept index that corresponds to the input observation.\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        percept = 0\n",
    "        for which_feature in range(len(observation)):\n",
    "            percept += int(observation[which_feature] * np.prod(self.num_percepts_list[:which_feature]))\n",
    "        return percept\n",
    "    \n",
    "    def deliberate(self, observation):\n",
    "        \"\"\"\n",
    "        Given an observation , this method chooses the next action and records that choice in the g_matrix.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        observation : list\n",
    "            List that describes the observation, as specified in percept_preprocess.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        action : int\n",
    "            Index of the chosen action.\n",
    "\n",
    "        \"\"\"\n",
    "        percept = self.percept_preprocess(observation) \n",
    "        action = rand_choice_nb(arr = np.arange(self.num_actions), prob = self.probability_distr(percept))\n",
    "        self.g_matrix = (1 - self.eta_glow_damping) * self.g_matrix\n",
    "        self.g_matrix[action, percept] += 1 #record latest decision in g_matrix\n",
    "        return action\n",
    "    \n",
    "    def probability_distr(self, percept):\n",
    "        \"\"\"\n",
    "        Given a percept index, this method returns a probability distribution over actions.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        percept : int\n",
    "            Index of the given percept.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        probability_distr : np.array, length = num_actions\n",
    "            Probability for each action (normalized to unit sum), computed according to policy_type.\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        if self.policy_type == 'standard':\n",
    "            h_vector = self.h_matrix[:, percept]\n",
    "            probability_distr = h_vector / np.sum(h_vector)\n",
    "        elif self.policy_type == 'softmax':\n",
    "            h_vector = self.beta_softmax * self.h_matrix[:, percept]\n",
    "            h_vector_mod = h_vector - np.max(h_vector)\n",
    "            probability_distr = np.exp(h_vector_mod) / np.sum(np.exp(h_vector_mod))\n",
    "        return probability_distr\n",
    "    \n",
    "    def learn(self, reward):\n",
    "        \"\"\"\n",
    "        Given a reward, this method updates the h matrix.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        reward : float\n",
    "            Value of the obtained reward.\n",
    "        \"\"\"\n",
    "        if len(self.initial_prob_distr[0]) > 0:\n",
    "            self.h_matrix =  self.h_matrix - self.gamma_damping * (self.h_matrix - self.h_0) + reward * self.g_matrix\n",
    "        else:\n",
    "            self.h_matrix =  self.h_matrix - self.gamma_damping * (self.h_matrix - 1.) + reward * self.g_matrix\n",
    "            \n",
    "    def reset_g(self):\n",
    "        \"\"\"\n",
    "        Resets the g_matrix.\n",
    "        \"\"\"\n",
    "        self.g_matrix = np.zeros((self.num_actions, self.num_percepts), dtype=np.float64)\n",
    "        \n",
    "    def deliberate_fixed_policy(self, observation):\n",
    "        \"\"\"\n",
    "        Given an observation , this method chooses the next action according to the fixed policy specified as attribute of the class.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        observation : list\n",
    "            List that describes the observation, as specified in percept_preprocess.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        action : int\n",
    "            Index of the chosen action.\n",
    "\n",
    "        \"\"\"\n",
    "        percept = self.percept_preprocess(observation) \n",
    "        if len(self.fixed_policy[0]) > 0:\n",
    "            action = rand_choice_nb(arr = np.arange(self.num_actions), prob = self.fixed_policy[percept])\n",
    "        else:\n",
    "            print('No fixed policy was given to the agent. The action will be selected randomly.')\n",
    "            action = np.random.choice(self.num_actions)\n",
    "    \n",
    "        self.g_matrix = (1 - self.eta_glow_damping) * self.g_matrix\n",
    "        self.g_matrix[action, percept] += 1 #record latest decision in g_matrix\n",
    "    \n",
    "        return action\n",
    "    \n",
    "    def act(self, action):\n",
    "        \"\"\"\n",
    "        Agent performs the given action.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        action : int (0, 1)\n",
    "            1 if it changes direction, 0 otherwise\n",
    "        \"\"\"\n",
    "        \n",
    "        # If the agent changes direction   \n",
    "        if action == 1:\n",
    "            self.agent_state = 0\n",
    "        else:\n",
    "            self.agent_state += 1  \n",
    "            \n",
    "    \n",
    "    def get_state(self):  \n",
    "        ''' simplified to case of single forager. Returns list because is what deliberate needs'''\n",
    "        return np.array([self.agent_state])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No fixed policy was given to the agent. The action will be selected randomly.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| hide\n",
    "#| exec : false\n",
    "agent = _Forager_original(num_actions = 2, state_space = np.array([np.arange(100)]))\n",
    "agent.percept_preprocess([0]*agent.num_percepts_list)\n",
    "agent.probability_distr(0)\n",
    "observation = [0]*agent.num_percepts_list[0]\n",
    "agent.deliberate(np.array(observation))\n",
    "agent.learn(1)\n",
    "agent.reset_g()\n",
    "agent.deliberate_fixed_policy(np.array(observation))\n",
    "agent.act(0)\n",
    "agent.get_state()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#| hide\n",
    "\n",
    "## Forager with efficient H update\n",
    "We use the formula $H_{t+i} = (1-\\gamma)^i H_t + \\gamma H_0 \\sum_{j=1}^i(1-\\gamma)^{j-1}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "#| hide\n",
    "@jitclass([(\"num_percepts_list\", int64[:]),           \n",
    "           (\"initial_prob_distr\", float64[:,:]),           \n",
    "           (\"fixed_policy\", float64[:,:]) ,\n",
    "           (\"h_matrix\", float64[:,:]) ,\n",
    "           (\"g_matrix\", float64[:,:]) ,\n",
    "           (\"h_0\", float64[:,:]),\n",
    "           (\"prefactor_1\", float64[:]),\n",
    "           (\"prefactor_2\", float64[:])\n",
    "          ])\n",
    "class _Forager_efficient_H():\n",
    "    num_actions : int\n",
    "    gamma_damping : float\n",
    "    eta_glow_damping : float\n",
    "    policy_type : str\n",
    "    beta_softmax : float\n",
    "    num_percepts : int\n",
    "    agent_state : int\n",
    "    max_no_update : int\n",
    "    counter_upd : int\n",
    "    num_percepts_list : np.array\n",
    "    initial_prob_distr : np.array\n",
    "    fixed_policy : np.array    \n",
    "    h_matrix : np.array\n",
    "    g_matrix : np.array\n",
    "    h_0 : np.array\n",
    "    prefactor_1: np.array\n",
    "    prefactor_2: np.array\n",
    "    \n",
    "    def __init__(self, num_actions, \n",
    "                 state_space, \n",
    "                 gamma_damping=0.0, \n",
    "                 eta_glow_damping=0.0, \n",
    "                 policy_type='standard', \n",
    "                 beta_softmax=3, \n",
    "                 initial_prob_distr = np.array([[],[]]), \n",
    "                 fixed_policy=np.array([[],[]]),\n",
    "                 max_no_update = int(1e4)\n",
    "                ):\n",
    "        \"\"\"\n",
    "        Improved agent from _Agent_original with efficient H update implemented\n",
    "        \"\"\"\n",
    "        \n",
    "        self.agent_state = 0\n",
    "        \n",
    "        self.num_actions = num_actions\n",
    "        self.num_percepts_list = np.array([len(i) for i in state_space], dtype = np.int64) # change w.r.t PSAGENT\n",
    "        self.gamma_damping = gamma_damping\n",
    "        self.eta_glow_damping = eta_glow_damping\n",
    "        self.policy_type = policy_type\n",
    "        self.beta_softmax = beta_softmax\n",
    "        self.initial_prob_distr = initial_prob_distr\n",
    "        self.fixed_policy = fixed_policy\n",
    "        \n",
    "        self.num_percepts = int(np.prod(self.num_percepts_list)) # total number of possible percepts\n",
    "        \n",
    "        self.init_matrices()\n",
    "        \n",
    "        self.max_no_update = max_no_update      \n",
    "        self.counter_upd = 0\n",
    "        self.prefactor_1 = (1-self.gamma_damping)**(np.arange(self.max_no_update+1)) \n",
    "        # This is the slow / easy to understand way of computing prefactor_2\n",
    "        # self.prefactor_2 = np.zeros(self.max_no_H_update+1)       \n",
    "        # for i in range(self.max_no_H_update):\n",
    "        #     self.prefactor_2[i+1] = self.gamma_damping*np.sum((1-self.gamma_damping)**np.arange(i+1))\n",
    "        # and this it the efficient way\n",
    "        sum_term = (1-self.gamma_damping)**np.arange(self.max_no_update+1)\n",
    "        self.prefactor_2 = self.gamma_damping*(np.cumsum(sum_term)-sum_term)\n",
    "        \n",
    "    def init_matrices(self):\n",
    "\n",
    "        self.g_matrix = np.zeros((self.num_actions, self.num_percepts)) #glow matrix, for processing delayed rewards\n",
    "\n",
    "        # initialize h matrix with different values\n",
    "        if len(self.initial_prob_distr[0]) > 0:          \n",
    "            self.h_0 = self.initial_prob_distr\n",
    "            self.h_matrix = self.h_0.copy()\n",
    "        else: \n",
    "            self.h_matrix = np.ones((self.num_actions, self.num_percepts), dtype=np.float64) #Note: the first index specifies the action, the second index specifies the percept.\n",
    "            \n",
    "    def _learn_post_reward(self, reward):\n",
    "        if self.counter_upd == 0:\n",
    "            print('Counter for h_matrix is zero, check that your are properly updating it!')\n",
    "        if len(self.initial_prob_distr[0]) > 0:\n",
    "            self.h_matrix = self.prefactor_1[self.counter_upd ] * self.h_matrix + self.prefactor_2[self.counter_upd] * self.h_0 + reward * self.g_matrix\n",
    "        else:\n",
    "            self.h_matrix = self.prefactor_1[self.counter_upd ] * self.h_matrix + self.prefactor_2[self.counter_upd] + reward * self.g_matrix\n",
    "        self.counter_upd = 0\n",
    "        \n",
    "    def _hmat_upd_single_percept(self, t, percept):\n",
    "        if len(self.initial_prob_distr[0]) > 0:\n",
    "            return self.prefactor_1[t] * self.h_matrix[:, percept] + self.prefactor_2[t] * self.h_0[:, percept]\n",
    "        else:\n",
    "            return self.prefactor_1[t] * self.h_matrix[:, percept] + self.prefactor_2[t] \n",
    "            \n",
    "            \n",
    "    def percept_preprocess(self, observation):\n",
    "        \"\"\"\n",
    "        Takes a multi-feature percept and reduces it to a single integer index.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        observation : ARRAY of integers >=0, of the same length as self.num_percepts_list\n",
    "            List that describes the observation. Each entry is the value that each feature takes in the observation.\n",
    "            observation[i] < num_percepts_list[i] (strictly)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        percept : int\n",
    "            Percept index that corresponds to the input observation.\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        percept = 0\n",
    "        for which_feature in range(len(observation)):\n",
    "            percept += int(observation[which_feature] * np.prod(self.num_percepts_list[:which_feature]))\n",
    "        return percept\n",
    "    \n",
    "    def deliberate(self, observation):\n",
    "        \"\"\"\n",
    "        Given an observation , this method chooses the next action and records that choice in the g_matrix.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        observation : list\n",
    "            List that describes the observation, as specified in percept_preprocess.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        action : int\n",
    "            Index of the chosen action.\n",
    "\n",
    "        \"\"\"\n",
    "        percept = self.percept_preprocess(observation) \n",
    "        \n",
    "        # Probabilities must be of update h_matrix. We feed the prob distr the update h_matrix\n",
    "        # for the percept, but don't update the h_matrix\n",
    "        current_h_mat = self._hmat_upd_single_percept(self.counter_upd, percept)\n",
    "        probs = self.probability_distr(percept, h_matrix = current_h_mat)\n",
    "        \n",
    "        action = rand_choice_nb(arr = np.arange(self.num_actions), prob = probs)\n",
    "        self.g_matrix = (1 - self.eta_glow_damping) * self.g_matrix\n",
    "        self.g_matrix[action, percept] += 1 #record latest decision in g_matrix\n",
    "        return action\n",
    "    \n",
    "    def probability_distr(self, percept, h_matrix = None):\n",
    "        \"\"\"\n",
    "        UPDATE (added the optional input)\n",
    "         \n",
    "        Given a percept index, this method returns a probability distribution over actions.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        percept : int\n",
    "            Index of the given percept.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        probability_distr : np.array, length = num_actions\n",
    "            Probability for each action (normalized to unit sum), computed according to policy_type.\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        if self.policy_type == 'standard':\n",
    "            h_vector = self.h_matrix[:, percept] if h_matrix is None else h_matrix\n",
    "            probability_distr = h_vector / np.sum(h_vector)\n",
    "        elif self.policy_type == 'softmax':\n",
    "            h_vector = self.beta_softmax * self.h_matrix[:, percept] if h_matrix is None else h_matrix\n",
    "            h_vector_mod = h_vector - np.max(h_vector)\n",
    "            probability_distr = np.exp(h_vector_mod) / np.sum(np.exp(h_vector_mod))\n",
    "        return probability_distr\n",
    "    \n",
    "    def learn(self, reward):\n",
    "        \"\"\"\n",
    "        Given a reward, this method updates the h matrix.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        reward : float\n",
    "            Value of the obtained reward.\n",
    "        \"\"\"\n",
    "        if len(self.initial_prob_distr[0]) > 0:\n",
    "            self.h_matrix =  self.h_matrix - self.gamma_damping * (self.h_matrix - self.h_0) + reward * self.g_matrix\n",
    "        else:\n",
    "            self.h_matrix =  self.h_matrix - self.gamma_damping * (self.h_matrix - 1.) + reward * self.g_matrix\n",
    "            \n",
    "    def reset_g(self):\n",
    "        \"\"\"\n",
    "        Resets the g_matrix.\n",
    "        \"\"\"\n",
    "        self.g_matrix = np.zeros((self.num_actions, self.num_percepts), dtype=np.float64)\n",
    "        \n",
    "    def deliberate_fixed_policy(self, observation):\n",
    "        \"\"\"\n",
    "        Given an observation , this method chooses the next action according to the fixed policy specified as attribute of the class.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        observation : list\n",
    "            List that describes the observation, as specified in percept_preprocess.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        action : int\n",
    "            Index of the chosen action.\n",
    "\n",
    "        \"\"\"\n",
    "        percept = self.percept_preprocess(observation) \n",
    "        if len(self.fixed_policy[0]) > 0:\n",
    "            action = rand_choice_nb(arr = np.arange(self.num_actions), prob = self.fixed_policy[percept])\n",
    "        else:\n",
    "            print('No fixed policy was given to the agent. The action will be selected randomly.')\n",
    "            action = np.random.choice(self.num_actions)\n",
    "    \n",
    "        self.g_matrix = (1 - self.eta_glow_damping) * self.g_matrix\n",
    "        self.g_matrix[action, percept] += 1 #record latest decision in g_matrix\n",
    "    \n",
    "        return action\n",
    "    \n",
    "    def act(self, action):\n",
    "        \"\"\"\n",
    "        Agent performs the given action.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        action : int (0, 1)\n",
    "            1 if it changes direction, 0 otherwise\n",
    "        \"\"\"\n",
    "        \n",
    "        # If the agent changes direction   \n",
    "        if action == 1:\n",
    "            self.agent_state = 0\n",
    "        else:\n",
    "            self.agent_state += 1  \n",
    "            \n",
    "    \n",
    "    def get_state(self):  \n",
    "        ''' simplified to case of single forager. Returns list because is what deliberate needs'''\n",
    "        return np.array([self.agent_state])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#| hide\n",
    "**Testing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "def test_train_loop_Heff(efficient, agent, episodes):\n",
    "    \n",
    "    for i in range(episodes):\n",
    "        \n",
    "        if efficient:\n",
    "            agent.counter_upd += 1\n",
    "        \n",
    "        state = np.array([i])\n",
    "        \n",
    "        if i % 2 == 0:\n",
    "            action = 0\n",
    "        else: 1\n",
    "        \n",
    "        # here is where glow matrix updates:\n",
    "        agent.g_matrix = (1 - agent.eta_glow_damping) * agent.g_matrix\n",
    "        agent.g_matrix[action, i] += 1 #record latest decision in g_matrix\n",
    "        \n",
    "        if i == 2 or i == 6:\n",
    "            reward = 1\n",
    "        else: reward = 0\n",
    "        \n",
    "        if efficient:\n",
    "            if reward == 1:\n",
    "                agent._learn_post_reward(reward)\n",
    "                agent.counter_upd = 0\n",
    "        else:\n",
    "            agent.learn(reward)\n",
    "\n",
    "    if efficient:\n",
    "        agent._learn_post_reward(reward)\n",
    "            \n",
    "    return agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#| hide\n",
    "**Value testing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from rl_opts.rl_framework import Forager as Forager_classic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "eps = 10\n",
    "gamma_damping = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.90236435, 2.90236435, 2.90236435, 1.970299  , 1.970299  ,\n",
       "        1.970299  , 1.970299  , 1.        , 1.        , 1.        ],\n",
       "       [1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| hide\n",
    "agent_noopt = Forager_classic(num_actions = 2,\n",
    "                              state_space = np.array([np.arange(eps)]),\n",
    "                              gamma_damping = gamma_damping)\n",
    "\n",
    "trained_noopt = test_train_loop_Heff(efficient = False, agent = agent_noopt, episodes = eps)\n",
    "trained_noopt.h_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.90236435, 2.90236435, 2.90236435, 1.970299  , 1.970299  ,\n",
       "        1.970299  , 1.970299  , 1.        , 1.        , 1.        ],\n",
       "       [1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| hide\n",
    "agent_opt = _Forager_efficient_H(num_actions = 2,\n",
    "                                 state_space = np.array([np.arange(eps)]),\n",
    "                                 gamma_damping = gamma_damping)\n",
    "\n",
    "trained = test_train_loop_Heff(efficient=True, agent = agent_opt, episodes = eps)\n",
    "trained.h_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'comparison old and efficient: -7.438494264988549e-15 ||||| IF value != 0, something is wrong!!!'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| hide\n",
    "f'comparison old and efficient: {(trained.h_matrix-trained_noopt.h_matrix).sum()} ||||| IF value != 0, something is wrong!!!'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#| hide\n",
    "Manual testing: we define an h-matrix and let it damp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "eps = 10\n",
    "len_hmat = 5\n",
    "gamma_damping = 0.001\n",
    "rand_h = [[2.0,2,4,5,1],\n",
    "          [3,3,1,1,1]]\n",
    "rand_h = np.array(rand_h)\n",
    "\n",
    "\n",
    "#np.random.rand(2, len_hmat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "agent_noopt = Forager_classic(num_actions = 2,\n",
    "                              state_space = np.array([np.arange(len_hmat)]),\n",
    "                              gamma_damping = gamma_damping)\n",
    "agent_noopt.h_matrix = rand_h.copy()\n",
    "\n",
    "for e in range(eps):\n",
    "    agent_noopt.learn(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.99004488, 1.99004488, 3.97013464, 4.96017952, 1.        ],\n",
       "       [2.98008976, 2.98008976, 1.        , 1.        , 1.        ]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| hide\n",
    "agent_noopt.h_matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "agent_opt = _Forager_efficient_H(num_actions = 2,\n",
    "                                 state_space = np.array([np.arange(len_hmat)]),\n",
    "                                 gamma_damping = gamma_damping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.99004488, 1.99004488, 3.97013464, 4.96017952, 1.        ],\n",
       "       [2.98008976, 2.98008976, 1.        , 1.        , 1.        ]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| hide\n",
    "agent_opt.h_matrix = rand_h.copy()\n",
    "\n",
    "agent_opt.counter_upd = 10\n",
    "agent_opt._learn_post_reward(0)\n",
    "agent_opt.h_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forager \n",
    "This agent is an efficient version of rl_opts.rl_framework.Forager with:\n",
    "\n",
    "- `numba` implementation\n",
    "-  H and G matrix efficient updates (contribution by Michele Caraglio)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "@jitclass([(\"size_state_space\", int64[:]),           \n",
    "           (\"initial_prob_distr\", float64[:,:]),           \n",
    "           (\"fixed_policy\", float64[:,:]) ,\n",
    "           (\"h_matrix\", float64[:,:]) ,\n",
    "           (\"g_matrix\", float64[:,:]) ,\n",
    "           (\"h_0\", float64[:,:]),\n",
    "           (\"prefactor_1\", float64[:]),\n",
    "           (\"prefactor_2\", float64[:]),\n",
    "           (\"last_upd_G\", float64[:,:])\n",
    "          ])\n",
    "class Forager():\n",
    "    num_actions : int\n",
    "    gamma_damping : float\n",
    "    eta_glow_damping : float\n",
    "    policy_type : str\n",
    "    beta_softmax : float\n",
    "    num_percepts : int\n",
    "    agent_state : int\n",
    "    size_state_space : np.array\n",
    "    initial_prob_distr : np.array\n",
    "    fixed_policy : np.array    \n",
    "    h_matrix : np.array\n",
    "    g_matrix : np.array\n",
    "    h_0 : np.array\n",
    "    # Efficient H update\n",
    "    prefactor_1: np.array\n",
    "    prefactor_2: np.array\n",
    "    max_no_H_update : int\n",
    "    N_upd_H : int\n",
    "    # Efficient G update\n",
    "    last_upd_G: np.array\n",
    "    N_upd_G: int\n",
    "    \n",
    "    def __init__(self, num_actions, # Number of actions\n",
    "                 size_state_space, \n",
    "                 # List where each entry is the state space of each perceptual feature. \n",
    "                 # In general we only consider one perceptual feature (counter)\n",
    "                 gamma_damping=0.0, # Gamma of PS\n",
    "                 eta_glow_damping=0.0, # Glow of PS\n",
    "                 policy_type='standard', # Sampling of policy\n",
    "                 beta_softmax=3, # Parameters if policy is softmax\n",
    "                 initial_prob_distr = np.array([[],[]]), # Initial h-matrix\n",
    "                 fixed_policy=np.array([[],[]]), # If considering a fixed policy\n",
    "                 max_no_H_update = int(1e4) # maximum number of steps before an update of H and G matrices.\n",
    "                ):\n",
    "\n",
    "        \n",
    "        \"\"\"\n",
    "        Updated version of the `rl_framework.Forager` class, with an efficient update both for the H-matrix and the G-matrix.\n",
    "\n",
    "        **Inputs**\n",
    "        \n",
    "        `num_actions` : \n",
    "            Number of actions\n",
    "            \n",
    "        `size_state_space` : \n",
    "             List where each entry is the state space of each perceptual feature. In general we only consider one perceptual feature (counter)\n",
    "        \n",
    "        `gamma_damping` :\n",
    "            Gamma of PS\n",
    "        \n",
    "        `eta_glow_damping` :\n",
    "            Glow of PS\n",
    "        \n",
    "        `policy_type` :\n",
    "            Sampling of policy\n",
    "        \n",
    "        `beta_softmax` :\n",
    "            Parameters if policy is softmax\n",
    "        \n",
    "        `initial_prob_distr` :\n",
    "            Initial h-matrix\n",
    "        \n",
    "        `fixed_policy` :\n",
    "            If considering a fixed policy\n",
    "        \n",
    "        `max_no_H_update` :\n",
    "            Maximum number of steps before an update of H and G matrices.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.agent_state = 0\n",
    "        \n",
    "        self.num_actions = num_actions\n",
    "        self.size_state_space = size_state_space\n",
    "        self.num_percepts = int(np.prod(self.size_state_space)) # total number of possible percepts\n",
    "        \n",
    "        self.gamma_damping = gamma_damping\n",
    "        self.eta_glow_damping = eta_glow_damping\n",
    "        self.policy_type = policy_type\n",
    "        self.beta_softmax = beta_softmax\n",
    "        self.initial_prob_distr = initial_prob_distr\n",
    "        self.fixed_policy = fixed_policy       \n",
    "        \n",
    "        self.init_matrices()\n",
    "        \n",
    "        # # For H update\n",
    "        self.max_no_H_update = max_no_H_update      \n",
    "        self.N_upd_H = 0\n",
    "        self.prefactor_1 = (1-self.gamma_damping)**(np.arange(self.max_no_H_update+1)) \n",
    "\n",
    "        # This is the slow / easy to understand way of computing prefactor_2\n",
    "        # self.prefactor_2 = np.zeros(self.max_no_H_update+1)       \n",
    "        # for i in range(self.max_no_H_update):\n",
    "        #     self.prefactor_2[i+1] = self.gamma_damping*np.sum((1-self.gamma_damping)**np.arange(i+1))\n",
    "        # and this it the efficient way\n",
    "        sum_term = (1-self.gamma_damping)**np.arange(self.max_no_H_update+1)\n",
    "        self.prefactor_2 = self.gamma_damping*(np.cumsum(sum_term)-sum_term)\n",
    "        \n",
    "        \n",
    "            \n",
    "        # For G update\n",
    "        self.last_upd_G = np.zeros((self.num_actions, self.num_percepts))\n",
    "        self.N_upd_G = 0\n",
    "                              \n",
    "        \n",
    "    def init_matrices(self):\n",
    "\n",
    "        self.g_matrix = np.zeros((self.num_actions, self.num_percepts)) #glow matrix, for processing delayed rewards\n",
    "\n",
    "        # initialize h matrix with different values\n",
    "        if len(self.initial_prob_distr[0]) > 0:          \n",
    "            self.h_0 = self.initial_prob_distr\n",
    "            self.h_matrix = self.h_0.copy()\n",
    "        else: \n",
    "            self.h_matrix = np.ones((self.num_actions, self.num_percepts), dtype=np.float64) #Note: the first index specifies the action, the second index specifies the percept.\n",
    "            \n",
    "    def _learn_post_reward(self, reward):\n",
    "        '''Given a reward, updates the whole H-matrix taking into account that we did not have updates\n",
    "        for the last N_upd_H steps.'''\n",
    "        # Update the full G matrix\n",
    "        self._G_upd_full()\n",
    "        \n",
    "        if self.N_upd_H == 0:\n",
    "            print('Counter for h_matrix is zero, check that your are properly updating it!')\n",
    "        if len(self.initial_prob_distr[0]) > 0:\n",
    "            self.h_matrix = self.prefactor_1[self.N_upd_H ] * self.h_matrix + self.prefactor_2[self.N_upd_H] * self.h_0 + reward * self.g_matrix\n",
    "        else:\n",
    "            self.h_matrix = self.prefactor_1[self.N_upd_H ] * self.h_matrix + self.prefactor_2[self.N_upd_H] + reward * self.g_matrix\n",
    "        self.N_upd_H = 0\n",
    "        \n",
    "    def _H_upd_single_percept(self, t, percept):\n",
    "        '''Given a percept and the time t passed since the last H-matrix update,\n",
    "        returns the corresponding --updated-- column of the H-matrix for all actions.\n",
    "        This updated is local and does no affect the H-matrix.'''\n",
    "        if len(self.initial_prob_distr[0]) > 0:\n",
    "            return self.prefactor_1[t] * self.h_matrix[:, percept] + self.prefactor_2[t] * self.h_0[:, percept]\n",
    "        else:\n",
    "            return self.prefactor_1[t] * self.h_matrix[:, percept] + self.prefactor_2[t] \n",
    "        \n",
    "    def _G_upd_single_percept(self, percept, action):\n",
    "        '''Given a percept-action tuple, updates that element of the G-matrix. Updates the last_upd_G\n",
    "        to keep track of when was the matrix updated.'''\n",
    "        # For the current (a,s) tuple, we damp and sum one\n",
    "        self.g_matrix[action, percept] = (1 - self.eta_glow_damping)**(self.N_upd_G - self.last_upd_G[action, percept])*self.g_matrix[action, percept] + 1\n",
    "        # Then update the last_upd matrix\n",
    "        self.last_upd_G[action, percept] = self.N_upd_G\n",
    "        \n",
    "    def _G_upd_full(self):\n",
    "        '''Given the current number of steps without an update, updates the whole G-matrix.\n",
    "        Then, resets all counters.'''\n",
    "        self.g_matrix = (1 - self.eta_glow_damping)**(self.N_upd_G - self.last_upd_G) * self.g_matrix\n",
    "        self.N_upd_G = 0\n",
    "        self.last_upd_G = np.zeros((self.num_actions, self.num_percepts))\n",
    "            \n",
    "            \n",
    "    def percept_preprocess(self, observation):\n",
    "        \"\"\"\n",
    "        Takes a multi-feature percept and reduces it to a single integer index.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        observation : ARRAY of integers >=0, of the same length as self.num_percepts_list\n",
    "            List that describes the observation. Each entry is the value that each feature takes in the observation.\n",
    "            observation[i] < num_percepts_list[i] (strictly)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        percept : int\n",
    "            Percept index that corresponds to the input observation.\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        percept = 0\n",
    "        for idx_obs, obs_feature in enumerate(observation):\n",
    "            percept += int(obs_feature * np.prod(self.size_state_space[:idx_obs]))  \n",
    "        return percept\n",
    "    \n",
    "    def deliberate(self, observation):\n",
    "        \"\"\"\n",
    "        Given an observation , this method chooses the next action and records that choice in the g_matrix.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        observation : list\n",
    "            List that describes the observation, as specified in percept_preprocess.\n",
    "        action : optional, bool\n",
    "            Mostly for debugging, we can input the action and no deliberation takes place, but g_matrix is updated\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        action : int\n",
    "            Index of the chosen action.\n",
    "\n",
    "        \"\"\"\n",
    "        percept = self.percept_preprocess(observation) \n",
    "        \n",
    "        \n",
    "        # Probabilities must be of update h_matrix. We feed the prob distr the update h_matrix\n",
    "        # for the percept, but don't update the h_matrix\n",
    "        current_h_mat = self._H_upd_single_percept(self.N_upd_H, percept)\n",
    "        probs = self.probability_distr(percept, h_matrix = current_h_mat)        \n",
    "        action = rand_choice_nb(arr = np.arange(self.num_actions), prob = probs)\n",
    "        \n",
    "        # Update the G matrix for current (s,a) tuple\n",
    "        self._G_upd_single_percept(percept, action)\n",
    "        \n",
    "        return action\n",
    "    \n",
    "    def probability_distr(self, percept, h_matrix = None):\n",
    "        \"\"\"\n",
    "        UPDATE (added the optional input)\n",
    "         \n",
    "        Given a percept index, this method returns a probability distribution over actions.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        percept : int\n",
    "            Index of the given percept.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        probability_distr : np.array, length = num_actions\n",
    "            Probability for each action (normalized to unit sum), computed according to policy_type.\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        if self.policy_type == 'standard':\n",
    "            h_vector = self.h_matrix[:, percept] if h_matrix is None else h_matrix\n",
    "            probability_distr = h_vector / np.sum(h_vector)\n",
    "        elif self.policy_type == 'softmax':\n",
    "            h_vector = self.beta_softmax * self.h_matrix[:, percept] if h_matrix is None else h_matrix\n",
    "            h_vector_mod = h_vector - np.max(h_vector)\n",
    "            probability_distr = np.exp(h_vector_mod) / np.sum(np.exp(h_vector_mod))\n",
    "        return probability_distr\n",
    "    \n",
    "    def learn(self, reward):\n",
    "        \"\"\"\n",
    "        Given a reward, this method updates the h matrix.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        reward : float\n",
    "            Value of the obtained reward.\n",
    "        \"\"\"\n",
    "        if len(self.initial_prob_distr[0]) > 0:\n",
    "            self.h_matrix =  self.h_matrix - self.gamma_damping * (self.h_matrix - self.h_0) + reward * self.g_matrix\n",
    "        else:\n",
    "            self.h_matrix =  self.h_matrix - self.gamma_damping * (self.h_matrix - 1.) + reward * self.g_matrix\n",
    "            \n",
    "    def reset_g(self):\n",
    "        \"\"\"\n",
    "        Resets the g_matrix.\n",
    "        \"\"\"\n",
    "        self.g_matrix = np.zeros((self.num_actions, self.num_percepts), dtype=np.float64)\n",
    "        \n",
    "    def deliberate_fixed_policy(self, observation):\n",
    "        \"\"\"\n",
    "        Given an observation , this method chooses the next action according to the fixed policy specified as attribute of the class.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        observation : list\n",
    "            List that describes the observation, as specified in percept_preprocess.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        action : int\n",
    "            Index of the chosen action.\n",
    "\n",
    "        \"\"\"\n",
    "        percept = self.percept_preprocess(observation) \n",
    "        if len(self.fixed_policy[0]) > 0:\n",
    "            action = rand_choice_nb(arr = np.arange(self.num_actions), prob = self.fixed_policy[percept])\n",
    "        else:\n",
    "            print('No fixed policy was given to the agent. The action will be selected randomly.')\n",
    "            action = np.random.choice(self.num_actions)\n",
    "    \n",
    "        self.g_matrix = (1 - self.eta_glow_damping) * self.g_matrix\n",
    "        self.g_matrix[action, percept] += 1 #record latest decision in g_matrix\n",
    "    \n",
    "        return action\n",
    "    \n",
    "    def act(self, action):\n",
    "        \"\"\"\n",
    "        Agent performs the given action.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        action : int (0, 1)\n",
    "            1 if it changes direction, 0 otherwise\n",
    "        \"\"\"\n",
    "        \n",
    "        # If the agent changes direction   \n",
    "        if action == 1:\n",
    "            self.agent_state = 0\n",
    "        else:\n",
    "            self.agent_state += 1  \n",
    "            \n",
    "    \n",
    "    def get_state(self):  \n",
    "        ''' simplified to case of single forager. Returns list because is what deliberate needs'''\n",
    "        return np.array([self.agent_state])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#| hide\n",
    "**Testing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# @jit\n",
    "def test_train_loop(efficient, agent, episodes):\n",
    "    \n",
    "    for i in range(episodes):\n",
    "        \n",
    "        if efficient:\n",
    "            agent.N_upd_H += 1\n",
    "            agent.N_upd_G += 1\n",
    "        \n",
    "        state = np.array([i])\n",
    "        \n",
    "        # Do determinist action to keep track\n",
    "        if i % 2 == 0:\n",
    "            action = 0\n",
    "        else: action = 1       \n",
    "        \n",
    "        # Because deterministic action, do update of G matrix by hand copying what is in agent.deliberate\n",
    "        agent.deliberate(state)\n",
    "        \n",
    "        if i == 2 or i == 6:\n",
    "            reward = 1\n",
    "        else: reward = 0\n",
    "        \n",
    "        if efficient:\n",
    "            if reward == 1:\n",
    "                agent._learn_post_reward(reward)\n",
    "        else:\n",
    "            agent.learn(reward)\n",
    "    \n",
    "    if efficient:\n",
    "        agent._learn_post_reward(reward)\n",
    "\n",
    "    \n",
    "            \n",
    "    return agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Value testing**\n",
    "\n",
    "Because actions are random, the row of the value will change. But in each column, you should have consistent values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rl_opts.rl_framework import Forager as Forager_classic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0.91, 0.  , 0.93, 0.94, 0.95, 0.96, 0.97, 0.98, 0.  , 1.  ],\n",
       "        [0.  , 0.92, 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.99, 0.  ]]),\n",
       " array([[2.91192, 1.     , 2.95074, 1.96739, 1.97716, 1.98703, 1.997  ,\n",
       "         1.     , 1.     , 1.     ],\n",
       "        [1.     , 2.93123, 1.     , 1.     , 1.     , 1.     , 1.     ,\n",
       "         1.     , 1.     , 1.     ]]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eps = 10\n",
    "agent_noopt = Forager_classic(num_actions = 2,\n",
    "                              state_space = np.array([np.arange(eps)]), eta_glow_damping = 0.01, gamma_damping = 0.001)\n",
    "trained_noopt = test_train_loop(efficient = False, agent = agent_noopt, episodes = eps)\n",
    "trained_noopt.g_matrix.round(2), trained_noopt.h_matrix.round(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0.  , 0.92, 0.  , 0.  , 0.  , 0.96, 0.  , 0.98, 0.99, 0.  ],\n",
       "        [0.91, 0.  , 0.93, 0.94, 0.95, 0.  , 0.97, 0.  , 0.  , 1.  ]]),\n",
       " array([[1.     , 2.93123, 1.     , 1.     , 1.     , 1.98703, 1.     ,\n",
       "         1.     , 1.     , 1.     ],\n",
       "        [2.91192, 1.     , 2.95074, 1.96739, 1.97716, 1.     , 1.997  ,\n",
       "         1.     , 1.     , 1.     ]]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_opt = Forager(num_actions = 2,\n",
    "                    size_state_space = np.array([eps]), eta_glow_damping = 0.01, gamma_damping = 0.001)\n",
    "trained = test_train_loop(efficient=True, agent = agent_opt, episodes = eps)\n",
    "\n",
    "trained.g_matrix.round(2), trained.h_matrix.round(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Runtime testing only H vs. full efficient**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps = int(1e4); eta = 0.1\n",
    "agent_noopt = _Forager_original(num_actions = 2,\n",
    "                                state_space = np.array([np.arange(eps)]), \n",
    "                                eta_glow_damping = eta)\n",
    "agent_opt = Forager(num_actions = 2,\n",
    "                    size_state_space = np.array([eps]), \n",
    "                    eta_glow_damping = eta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22.6 ms ± 122 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit test_train_loop(efficient=True, agent = agent_opt, episodes = eps)\n",
    "# Runtime new Forager (efficient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "401 ms ± 32.5 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit test_train_loop(efficient=False, agent = agent_noopt, episodes = eps)\n",
    "# Runtime old Forager "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Launch multi agent learning\n",
    "We now make use of the parallel option of `numba` to create launchers where many agents are trained at the same time:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "#| export                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               \n",
    "@jit(nopython = NOPYTHON)\n",
    "def _train_loop_original(episodes, # Number of episodes to train\n",
    "               time_ep, # Length of episode\n",
    "               agent, # Agent class\n",
    "               env # Environment class\n",
    "              ): # Rewards in each episode and time step and h_matrix of the trained agent \n",
    "    '''\n",
    "    Training loop for _Forager_original.\n",
    "    '''\n",
    "    \n",
    "    save_rewards = np.zeros((episodes, time_ep))\n",
    "    \n",
    "    for ep in range(episodes):\n",
    "        # print(f'starting episode {ep} for agent {n_agent}')\n",
    "        #initialize environment and agent's counter and g matrix\n",
    "        env.init_env()\n",
    "        agent.agent_state = 0\n",
    "        agent.reset_g()\n",
    "\n",
    "        for t in range(time_ep):\n",
    "\n",
    "            #step to set counter to its min value n=1\n",
    "            if t == 0 or env.kicked[0]:\n",
    "                #do one step with random direction (no learning in this step)\n",
    "                env.update_pos(1)\n",
    "                #check boundary conditions\n",
    "                env.check_bc()\n",
    "                #reset counter\n",
    "                agent.agent_state = 0\n",
    "                #set kicked value to false again\n",
    "                env.kicked[0] = 0\n",
    "                reward = 0\n",
    "\n",
    "            else:\n",
    "                #get perception\n",
    "                state = agent.get_state()\n",
    "                #decide\n",
    "                action = agent.deliberate(state)\n",
    "                #act (update counter)\n",
    "                agent.act(action)\n",
    "\n",
    "                #update positions\n",
    "                env.update_pos(action)\n",
    "                #check if target was found + kick if it is\n",
    "                reward = env.check_encounter()\n",
    "\n",
    "                #check boundary conditions\n",
    "                env.check_bc()\n",
    "                #learn\n",
    "\n",
    "                agent.learn(reward)                \n",
    "                    \n",
    "            save_rewards[ep, t] = reward\n",
    "      \n",
    "    \n",
    "    return save_rewards, agent.h_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "#| export                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               \n",
    "@jit(nopython = NOPYTHON)\n",
    "def _train_loop_h_efficient(episodes, time_ep, agent, env, h_mat_allT = False):  \n",
    "    '''\n",
    "    Training loop for _Forager_h_efficient.\n",
    "    '''\n",
    "\n",
    "    if h_mat_allT: policy_t = np.zeros((episodes, agent.h_matrix.shape[-1]))\n",
    "    save_rewards = np.zeros((episodes, time_ep))\n",
    "    \n",
    "    for ep in range(episodes):\n",
    "        # print(f'starting episode {ep} for agent {n_agent}')\n",
    "        #initialize environment and agent's counter and g matrix\n",
    "        env.init_env()\n",
    "        agent.agent_state = 0\n",
    "        agent.reset_g()\n",
    "\n",
    "        for t in range(time_ep):\n",
    "            agent.counter_upd += 1\n",
    "            \n",
    "            #step to set counter to its min value n=1\n",
    "            if t == 0 or env.kicked[0]:\n",
    "                #do one step with random direction (no learning in this step)\n",
    "                env.update_pos(1)\n",
    "                #check boundary conditions\n",
    "                env.check_bc()\n",
    "                #reset counter\n",
    "                agent.agent_state = 0\n",
    "                #set kicked value to false again\n",
    "                env.kicked[0] = 0\n",
    "                reward = 0\n",
    "\n",
    "            else:\n",
    "                #get perception\n",
    "                state = agent.get_state()\n",
    "                #decide\n",
    "                action = agent.deliberate(state)\n",
    "                #act (update counter)\n",
    "                agent.act(action)\n",
    "\n",
    "                #update positions\n",
    "                env.update_pos(action)\n",
    "                #check if target was found + kick if it is\n",
    "                reward = env.check_encounter()\n",
    "\n",
    "                #check boundary conditions\n",
    "                env.check_bc()\n",
    "                #learn\n",
    "                if reward == 1 or agent.counter_upd > agent.max_no_update:\n",
    "                    agent._learn_post_reward(reward)\n",
    "                    \n",
    "            # Saving\n",
    "            save_rewards[ep, t] = reward\n",
    "            if h_mat_allT: policy_t[ep] = agent.h_matrix[0,:] / agent.h_matrix.sum(0)\n",
    "      \n",
    "    return (save_rewards, policy_t) if h_mat_allT else (save_rewards, agent.h_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               \n",
    "@jit(nopython = NOPYTHON)\n",
    "def train_loop(episodes : int, # Number of episodes to train\n",
    "               time_ep : int, # Length of episode\n",
    "               agent : object, # Agent class\n",
    "               env : object, # Environment class\n",
    "               h_mat_allT : bool = False # If True, returns the h_matrix at all times\n",
    "              )-> tuple: # Rewards and h-matrix of the trained agent      \n",
    "    '''    \n",
    "    Given an agent and environment, performs a loop train for the `TargetEnv` type of environment, by adequatly\n",
    "    updating the H and G counters, considering boundaries, etc... \n",
    "    '''\n",
    "\n",
    "    if h_mat_allT: policy_t = np.zeros((episodes, agent.h_matrix.shape[-1]))\n",
    "    save_rewards = np.zeros((episodes, time_ep))\n",
    "    \n",
    "    for ep in range(episodes):\n",
    "        # print(f'starting episode {ep} for agent {n_agent}')\n",
    "        #initialize environment and agent's counter and g matrix\n",
    "        env.init_env()\n",
    "        agent.agent_state = 0\n",
    "        agent.reset_g()\n",
    "\n",
    "        for t in range(time_ep):\n",
    "            agent.N_upd_H += 1\n",
    "            agent.N_upd_G += 1\n",
    "            \n",
    "            #step to set counter to its min value n=1\n",
    "            if t == 0 or env.kicked[0]:\n",
    "                #do one step with random direction (no learning in this step)\n",
    "                env.update_pos(1)\n",
    "                #check boundary conditions\n",
    "                env.check_bc()\n",
    "                #reset counter\n",
    "                agent.agent_state = 0\n",
    "                #set kicked value to false again\n",
    "                env.kicked[0] = 0\n",
    "                reward = 0\n",
    "\n",
    "            else:\n",
    "                #get perception\n",
    "                state = agent.get_state()\n",
    "                #decide\n",
    "                action = agent.deliberate(state)\n",
    "                #act (update counter)\n",
    "                agent.act(action)\n",
    "\n",
    "                #update positions\n",
    "                env.update_pos(action)\n",
    "                #check if target was found + kick if it is\n",
    "                reward = env.check_encounter()\n",
    "\n",
    "                #check boundary conditions\n",
    "                env.check_bc()\n",
    "                #learn\n",
    "                if reward == 1:\n",
    "                    agent._learn_post_reward(reward)\n",
    "\n",
    "            if agent.N_upd_H == agent.max_no_H_update-1:\n",
    "                agent._learn_post_reward(reward)\n",
    "\n",
    "            # Saving\n",
    "            save_rewards[ep, t] = reward\n",
    "\n",
    "        # updating H and G at the last episode if there was no reward on it.\n",
    "        if ep == episodes-1 and reward != 1:\n",
    "            agent._learn_post_reward(reward)            \n",
    "            \n",
    "        if h_mat_allT: policy_t[ep] = agent.h_matrix[0,:] / agent.h_matrix.sum(0)\n",
    "\n",
    "    \n",
    "      \n",
    "    return (save_rewards, policy_t) if h_mat_allT else (save_rewards, agent.h_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Runtime testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = TargetEnv(Nt = 100,\n",
    "               L = 100, \n",
    "               r = 0.5, \n",
    "               lc = np.array([[1.0],[1]]), \n",
    "               agent_step = 1, \n",
    "               destructive = False, \n",
    "               lc_distribution = 'constant')\n",
    "\n",
    "\n",
    "agent = Forager(num_actions = 2, # From here are props of the agent (see Forager for details)\n",
    "               size_state_space = np.array([100]))\n",
    "\n",
    "agent_nopt = _Forager_original(num_actions = 2, # From here are props of the agent (see Forager for details)\n",
    "                     state_space = np.array([np.arange(100)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24.7 ms ± 18.8 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit _train_loop_original(episodes = 100, time_ep = 100, agent = agent_nopt, env = env)\n",
    "# Runtime without h-matrix efficient update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21 ms ± 57.6 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit train_loop(episodes = 100, time_ep = 100, agent = agent, env = env)\n",
    "# Runtime with h-matrix efficient update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run multiple agents in parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@jit(nopython = NOPYTHON, parallel = True)\n",
    "def run_agents(episodes, # Number of episodes\n",
    "               time_ep, # Length of episode\n",
    "               N_agents, # Number of agents               \n",
    "               h_mat_allT = False, # If to save the h_matrix at all times\n",
    "               efficient_agent = True, # If to consider efficient H and G agent (other options: 'only H' or False)\n",
    "               Nt = 100, # From here are props of the environment (see TargetEnv for details) \n",
    "               L = 100, \n",
    "               r = 0.5, \n",
    "               lc = np.array([[1.0],[1]]), \n",
    "               agent_step = 1, \n",
    "               destructive_targets = False, \n",
    "               lc_distribution = 'constant', \n",
    "               num_actions = 2, # From here are props of the agent (see Forager for details)\n",
    "               size_state_space = np.array([100]), \n",
    "               gamma_damping = 0.00001,\n",
    "               eta_glow_damping = 0.1,\n",
    "               initial_prob_distr = np.array([[],[]]),\n",
    "               policy_type = 'standard', \n",
    "               beta_softmax = 3,  \n",
    "               fixed_policy = np.array([[],[]]),\n",
    "               max_no_H_update = int(1e3) \n",
    "              ):\n",
    "    \n",
    "    save_rewards = np.zeros((N_agents, episodes))\n",
    "    if h_mat_allT:\n",
    "        save_h_matrix = np.zeros((N_agents, episodes, size_state_space[0]))  \n",
    "    else:        \n",
    "        save_h_matrix = np.zeros((N_agents, 2, size_state_space[0])) \n",
    "    \n",
    "    for n_agent in prange(N_agents):\n",
    "        \n",
    "        env = TargetEnv(Nt,L,r,lc,agent_step,1,destructive_targets,lc_distribution)\n",
    "\n",
    "\n",
    "        if efficient_agent == True: # Both G and H efficient update\n",
    "            agent = Forager(num_actions,size_state_space,gamma_damping,\n",
    "                            eta_glow_damping,policy_type,beta_softmax,\n",
    "                            initial_prob_distr,fixed_policy,max_no_H_update)\n",
    "            rews, mat = train_loop(episodes, time_ep, agent, env, h_mat_allT) \n",
    "        \n",
    "        elif efficient_agent == 'only H': # Only H efficient update\n",
    "            state_space = np.arange(size_state_space[0]).reshape(1, size_state_space[0])\n",
    "            agent = _Forager_efficient_H(num_actions,state_space,gamma_damping,\n",
    "                                        eta_glow_damping,policy_type,beta_softmax,\n",
    "                                        initial_prob_distr,fixed_policy,max_no_H_update)\n",
    "            rews, mat = _train_loop_h_efficient(episodes, time_ep, agent, env, h_mat_allT)  \n",
    "            \n",
    "        elif efficient_agent == False: # Old version without efficient updates            \n",
    "            state_space = np.arange(size_state_space[0]).reshape(1, size_state_space[0])\n",
    "            agent = _Forager_original(num_actions,state_space,gamma_damping,\n",
    "                            eta_glow_damping,policy_type,beta_softmax,\n",
    "                            initial_prob_distr,fixed_policy)\n",
    "            rews, mat = _train_loop_original(episodes, time_ep, agent, env)    \n",
    "    \n",
    "        for t in range(episodes):\n",
    "            save_rewards[n_agent, t] = np.mean(rews[t])\n",
    "        save_h_matrix[n_agent] = mat\n",
    "        \n",
    "    return save_rewards, save_h_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Runtime testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gorka/miniconda3/envs/rl_opts_main/lib/python3.10/site-packages/numba/np/ufunc/parallel.py:371: NumbaWarning: The TBB threading layer requires TBB version 2021 update 6 or later i.e., TBB_INTERFACE_VERSION >= 12060. Found TBB_INTERFACE_VERSION = 12050. The TBB threading layer is disabled.\n",
      "  warnings.warn(problem)\n"
     ]
    }
   ],
   "source": [
    "#| hide\n",
    "# For compiling and checking\n",
    "time_ep = 12000\n",
    "run_agents(episodes = 10, time_ep = time_ep, N_agents = 5, efficient_agent=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.26 s ± 35.7 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit run_agents(episodes = 100, time_ep = time_ep, N_agents = 10, size_state_space = np.array([100]), efficient_agent=False)\n",
    "# Runtime original in numba:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.66 s ± 11.4 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit k = run_agents(episodes = 100, time_ep = time_ep, N_agents = 10, size_state_space = np.array([100]), efficient_agent='only H')\n",
    "# Runtime original with only H efficient:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.66 s ± 20.6 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit k = run_agents(episodes = 100, time_ep = time_ep, N_agents = 10, size_state_space = np.array([100]), efficient_agent=True)\n",
    "# Runtime original with full H and G efficient:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Virtual environment \n",
    "This is a wrapper that merges the forager and agent whose only input is the displacement of a real particle at each step. The output is the action to be taken."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "#@jitclass\n",
    "class virtual_ABM:\n",
    "\n",
    "    def __init__(self, \n",
    "                 num_episodes : int = None, # If int, number of episodes. If None, don't consider episodes\n",
    "                 time_ep : int = None, # If int, length of an episodes. If episodes = None, it is not considered.\n",
    "                 L : float = 5, # Size of the environment\n",
    "                 r : float = 1, # Radius of the target       \n",
    "                 max_counter : int = 500, # Maximum number for the agent's phase counter (i.e. what it gets as station)\n",
    "                 gamma_damping : float = 0.001, # Gamma of PS\n",
    "                 eta_glow_damping : float = 0.001, # Glow of PS                 \n",
    "                 max_no_H_update : int = int(1e4) # maximum number of steps before an update of H and G matrices.\n",
    "                 ):\n",
    "        \n",
    "        # Arguments for TargetEnv\n",
    "        Nt = 1 # Number of targets\n",
    "        destructive = True\n",
    "        lc = np.array([[1.0],[1]]) # Won't enter into effect if destructive = True\n",
    "        lc_distribution = 'constant' # Won't enter into effect if destructive = True\n",
    "        agent_step = 1\n",
    "\n",
    "        self.env = TargetEnv(Nt, L, r, lc, agent_step, 1, destructive, lc_distribution)\n",
    "        \n",
    "        self.particle_position = self.env.positions[0]\n",
    "\n",
    "        # Arguments for Forager\n",
    "        num_actions = 2\n",
    "        self.max_counter = max_counter\n",
    "        size_state_space = np.array([2, self.max_counter]) # first is phase (either active or passive). Second is the counter till last change\n",
    "        policy_type='standard' # Sampling of policy\n",
    "        beta_softmax=3 # Parameters if policy is softmax\n",
    "        initial_prob_distr = np.array([[],[]]) # Initial h-matrix\n",
    "        fixed_policy=np.array([[],[]]) # If considering a fixed policy\n",
    "        \n",
    "        self.agent = Forager(num_actions,size_state_space,gamma_damping,\n",
    "                             eta_glow_damping,policy_type,beta_softmax,\n",
    "                             initial_prob_distr,fixed_policy,max_no_H_update)       \n",
    "        \n",
    "        self.num_episodes = num_episodes\n",
    "        self.time_ep = time_ep\n",
    "        \n",
    "\n",
    "    def _particle_position(self):\n",
    "        return self.env.positions[0]\n",
    "    def _target_position(self):\n",
    "        return self.env.target_positions[0] \n",
    "    \n",
    "    def init_training(self):\n",
    "        ''' Initializes the environment and epochs '''\n",
    "        self.init_virtual_env()\n",
    "        \n",
    "        if self.num_episodes: \n",
    "            self.epoch = -1 # we start at -1 because init_epoch will sum one and set it at zero.\n",
    "            self.init_epoch()\n",
    "\n",
    "    def init_virtual_env(self):\n",
    "        # Current phase is: 0 for passive, 1 for active\n",
    "        self.current_phase = 0 # Starting always in the passive phase\n",
    "\n",
    "        # Initialize the environment (puts agent and target in random position)\n",
    "        self.env.init_env()\n",
    "\n",
    "        # Initialize / Reset agent's props\n",
    "        self.agent.agent_state = 0\n",
    "        self.agent.reset_g()\n",
    "        self.agent.N_upd_H = 0\n",
    "        self.agent.N_upd_G = 0\n",
    "\n",
    "    def init_epoch(self):\n",
    "        ''' Initializes a new epoch '''\n",
    "        self.init_virtual_env()\n",
    "        self.epoch += 1\n",
    "        self.t_ep = 0\n",
    "\n",
    "    def step(self, disp, return_reward = False):\n",
    "        ''' Makes a step in the virtual environment and subsequently learns'''\n",
    "\n",
    "        # Given the new displacement, update the environment and get the reward\n",
    "        self.env.update_pos_disp(disp)\n",
    "        reward = self.env.check_encounter()        \n",
    "        self.env.check_bc()\n",
    "\n",
    "\n",
    "        # Constraint that rewards can only be received in the passive phase:\n",
    "        if self.current_phase != 0:\n",
    "            reward = 0\n",
    "            \n",
    "        # Learn\n",
    "        # Now that we collected the reward, we have s,a,R and can learn        \n",
    "        # First we update the H update counter\n",
    "        self.agent.N_upd_H += 1        \n",
    "        # If the rewards are not zero or we reach the maximum no upd counters, we update\n",
    "        if (reward != 0) or (self.agent.N_upd_H == self.agent.max_no_H_update-1):\n",
    "            self.agent._learn_post_reward(reward)\n",
    "        \n",
    "        \n",
    "        # Acting\n",
    "        # Create a state based on current phase and agent state\n",
    "        state = np.array([self.current_phase, self.agent.agent_state])\n",
    "        # Get an action based on this state\n",
    "        action = self.agent.deliberate(state)\n",
    "        \n",
    "        if action == 0: # Continuing in the same phase\n",
    "            self.agent.agent_state += 1 \n",
    "        elif action == 1:\n",
    "            self.agent.agent_state = 0\n",
    "            self.current_phase = 1 - self.current_phase\n",
    "\n",
    "        if self.agent.agent_state == self.max_counter - 1:\n",
    "            self.agent.agent_state = 0\n",
    "\n",
    "        if self.num_episodes:\n",
    "            self.t_ep += 1\n",
    "            if self.t_ep == self.time_ep:\n",
    "                self.init_epoch()\n",
    "            \n",
    "\n",
    "        if return_reward:\n",
    "            return action, reward\n",
    "        else:\n",
    "            return action\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing on ABM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "#| hide\n",
    "def get_ABM_motion(x, y, theta, phi, vdt, sigma, sigma_theta, L):\n",
    "       \n",
    "    x += phi * vdt * np.cos(theta) + sigma * np.random.randn() \n",
    "    x = x % L\n",
    "\n",
    "    y += phi * vdt * np.sin(theta) + sigma * np.random.randn() \n",
    "    y = y % L\n",
    "\n",
    "    theta += sigma_theta * np.random.randn() \n",
    "    return x, y, theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from rl_opts.numba.rl_framework import TargetEnv, Forager\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "D = 2 # mum² s⁻¹\n",
    "L = 50 # mu² \n",
    "tau = 1.25 * 1e3 # s\n",
    "Pe = 100\n",
    "v = Pe * L / tau\n",
    "dt = tau*1e-4\n",
    "vdt = v*dt\n",
    "ell = 10\n",
    "D_theta = v/ell\n",
    "\n",
    "sigma = np.sqrt(2*D*dt);\n",
    "sigma_theta = np.sqrt(2*D_theta*dt)\n",
    "\n",
    "r = 2\n",
    "# venv = virtual_ABM(L = L, r = r, num_episodes = 50, time_ep = 5)\n",
    "venv = virtual_ABM(L = L, r = r, num_episodes = None)\n",
    "# manually creating longer passive phases:\n",
    "venv.agent.h_matrix[:500,0] *= 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "theta = 2*np.pi*np.random.rand()\n",
    "phi = 0\n",
    "T = 50\n",
    "\n",
    "\n",
    "venv.init_virtual_env()\n",
    "\n",
    "pos = np.zeros((T, 2))\n",
    "phase = np.zeros(T)\n",
    "rewards = np.zeros(T)\n",
    "pos[0] = venv.env.positions[0]\n",
    "pos_f = pos.copy()\n",
    "\n",
    "\n",
    "targets_pos = [venv.env.target_positions[0]]\n",
    "for t in range(1, T):    \n",
    "    # if t == int(T/2): phi = 1\n",
    "\n",
    "    x0, y0 = venv.env.positions[0].copy()\n",
    "    x1, y1, theta = get_ABM_motion(x0, y0, theta, phi, vdt, sigma, sigma_theta, L)\n",
    "    \n",
    "    \n",
    "    action, reward = venv.step((x1-x0, y1-y0), return_reward=True)\n",
    "    \n",
    "    if action == 1:\n",
    "        phi = 1 - phi\n",
    "        \n",
    "    \n",
    "    pos[t] = venv.env.positions[0].copy()\n",
    "    \n",
    "    phase[t] = venv.current_phase\n",
    "    if reward == 1: \n",
    "        print('sdf')\n",
    "        targets_pos.append(venv.env.target_positions[0])\n",
    "    rewards[t] = reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#| hide\n",
    "import matplotlib.patches as patches\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.plot(pos[:,0], pos[:,1])\n",
    "ax.scatter(pos[phase == 0,0], pos[phase == 0,1], c = 'r')\n",
    "ax.scatter(pos[phase == 1,0], pos[phase == 1,1], c = 'g')\n",
    "\n",
    "for tp in targets_pos:\n",
    "    circle = patches.Circle(tp, radius=r, linewidth=1, edgecolor='k', facecolor='none')\n",
    "    ax.add_patch(circle)\n",
    "\n",
    "ax.set_xlim(0, L)\n",
    "ax.set_ylim(0, L)\n",
    "ax.set_aspect('equal', 'box')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#| hide\n",
    "# nbdev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev import nbdev_export ; nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl_opts_main",
   "language": "python",
   "name": "rl_opts_main"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
