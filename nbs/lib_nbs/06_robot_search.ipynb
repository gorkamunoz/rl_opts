{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2be70a58-5e4d-4b9b-9e0f-8f60aff70737",
   "metadata": {},
   "source": [
    "# Robot search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c50860bf-b0b5-4003-bf10-e528104a98b3",
   "metadata": {},
   "source": [
    "In this notebook we introduce the RL framework for target searching with robots. It is based on the agents and environments presented in `rl_opts.rl_framework`. Some features of the code are:\n",
    "\n",
    "- we use `numba` to improve speed.\n",
    "- we implement more efficient ways of updating the H and G matrix (contribution by Dr. Michele Caraglio).\n",
    "- we consider as base case `num_agents = 1`. \n",
    "- we modify the old version of our agents to make them more realistic and more similar to the dynamics of real robots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee201d24-d37c-44a4-8168-1cff008a29dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp robot_search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "539d9282-bc0a-495c-a424-dac1e0eb8c4e",
   "metadata": {},
   "source": [
    "#| hide\n",
    "# Libs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "746b502b-ca4b-40d3-a091-3a87fa2f6d4b",
   "metadata": {},
   "source": [
    "Input libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a2d9fc-79c7-4324-9031-4cf5de678f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import numpy as np\n",
    "from numba.experimental import jitclass\n",
    "from numba import float64, int64, bool_, prange, njit\n",
    "import math\n",
    "import random\n",
    "\n",
    "from rl_opts.utils import isBetween_c_Vec_numba, rand_choice_nb\n",
    "from rl_opts.rl_framework.numba.agents import Forager\n",
    "\n",
    "NOPYTHON = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c81e1f-aca9-4307-8ea9-f3be43f7cc6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "#| exec: false\n",
    "# for debugging\n",
    "NOPYTHON = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e8b5b0c-9a99-4388-a20c-638ace2f3689",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc2b189-f187-40dc-a91d-56a91438f536",
   "metadata": {},
   "source": [
    "# Helpers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff9b311-2498-4574-a5df-26840f8ecd33",
   "metadata": {},
   "source": [
    "## Crossing targets while walking along an arc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d04b15-4950-49ca-8d48-4f9e67377983",
   "metadata": {},
   "source": [
    "Functions to evaluate whether the robot crosses a target while reorienting itself (action turn). During reorientation, the robot walks along a circumference arc.\n",
    "- rotation_arc: creates the points of the arc (how many points are created can be chosen).\n",
    "- arc_in_target_numba: evaluates whether the arc points are within a target area."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b8103ff-a5f4-4daf-a511-a36d5dc2a5b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "@njit\n",
    "def rotation_arc(theta : float, # Angle in rad (from origin, counterclockwise) describing the current orientation of the robot.\n",
    "                 dtheta : float, # Angle difference in rad. How much the robot rotates when it decides to turn.\n",
    "                 left : bool, # True if robot decided to turn left. False implies that the agent turns right.\n",
    "                 num_points_arc:int=50, # Number of points that form the arc that is going to be evaluated.\n",
    "                 R:float=1, # Rotation radius, distance from robot's center to the side. The robot reorients itself by rotating along the wheels of one side.\n",
    "                 x:float=0, # Horizontal component of the robot's current position.\n",
    "                 y:float=0 # Vertical component of the robot's current position.\n",
    "                ):\n",
    "    \"\"\"Creates the points of the circumference arc covered by the robot while it reorients itself (action \"turn\").\"\"\"\n",
    "    \n",
    "    if left:\n",
    "        value = 1\n",
    "        sum_angle = 0\n",
    "    else:\n",
    "        value = -1\n",
    "        sum_angle = np.pi\n",
    "    \n",
    "    phi = np.pi / 2 - theta\n",
    "    \n",
    "    # Get origin of rotation\n",
    "    origin_x = x - value * R * np.cos(phi)\n",
    "    origin_y = y + value * R * np.sin(phi)\n",
    "    \n",
    "    # Generate points for the arc in rad: from start point to final point = start point + length of arc\n",
    "    start_angle = sum_angle - phi\n",
    "    angles = np.linspace(start_angle, start_angle + dtheta, num_points_arc)\n",
    "    x_arc = origin_x + R * np.cos(angles) #x positions of all the arc points\n",
    "    y_arc = origin_y + R * np.sin(angles) #y positions of all the arc points\n",
    "    \n",
    "    return x_arc, y_arc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf1810e-02f5-457d-9ea1-4f29a9d16d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "@njit\n",
    "def arc_in_target_numba(target_coord : np.array, # Array (number targets, 2) with the coordinates of all targets.\n",
    "                        target_radius : float, # Radius of each target.\n",
    "                        x_arc: np.array, # Horizontal components of arc points.\n",
    "                        y_arc: np.array, # Vertical components of arc points.\n",
    "                       )->np.array: # 1D array of size (number targets) with 1 on the indices of the found targets.\n",
    "    \"\"\"\n",
    "    Checks which targets have been found by the agent when it reorients itself.\n",
    "    \"\"\"\n",
    "\n",
    "    # Reshape the arc arrays to make them broadcastable\n",
    "    x_arc_broadcasted = x_arc[:, np.newaxis]  # Shape (num_points_arc, 1)\n",
    "    y_arc_broadcasted = y_arc[:, np.newaxis]  # Shape (num_points_arc, 1)\n",
    "\n",
    "    # Compute the squared distances from all arc points to all target centers\n",
    "    distances_squared = (x_arc_broadcasted - target_coord[:, 0])**2 + (y_arc_broadcasted - target_coord[:, 1])**2\n",
    "\n",
    "    # Check if these distances are within the target radius squared\n",
    "    mask_in_target = distances_squared <= target_radius**2\n",
    "\n",
    "    # Summing over the arc points axis (axis=0), find which targets are found\n",
    "    mask_found_targets = np.sign(np.sum(mask_in_target, axis=0))\n",
    "    \n",
    "    return mask_found_targets.astype('bool')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03202ead-4f3a-42b6-9fb1-f4dd2c649079",
   "metadata": {},
   "outputs": [],
   "source": [
    "targ = np.random.rand(100, 2)* 100\n",
    "target_r = 1\n",
    "a_x, a_y = rotation_arc(np.pi/6, np.pi/3, True, num_points_arc=50, R=1, x=0, y=0)\n",
    "compile_first = arc_in_target_numba(targ, target_r, a_x, a_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e336ff-8325-455e-8946-92aef3e20c51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.4 μs ± 21.3 ns per loop (mean ± std. dev. of 7 runs, 100,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit arc_in_target_numba(targ, target_r, a_x, a_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43cc2a39-7403-4a48-91b7-8dbaa3f5dede",
   "metadata": {},
   "source": [
    "# Environments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fac9792-10e9-43fc-9732-d9494ee5440d",
   "metadata": {},
   "source": [
    "## Single Robot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4814ea68-75e1-4adc-8743-9f6f4038608d",
   "metadata": {},
   "source": [
    "Environment where a simulated robot (RL agent) searches for targets. The robot cannot see the targets. It gets a signal from the environment in the form of a positive reward whenever its position is within a target area.\n",
    "\n",
    "- States: the agent perceives the counter of steps that it has been walking straight, i.e. it decided to continue in the same direction.\n",
    "\n",
    "- Actions: \"continue\" , \"turn left\" or \"turn right\".\n",
    "    - For \"continue\": walks straight in the same direction (no reorientation) for a distance $v$ sampled from a normal distribution. $N(\\alpha, \\sigma_\\alpha)$\n",
    "    - For \"turning\" actions: normal distribution centered on some angle to the left or to the right. $N(\\beta, \\sigma_\\beta)$\n",
    "    - When it turns, it just reorients (NO EXTRA DISPLACEMENT). The reorientation occurs with respect to a center that is on the side wheels (the ones that don't move), so (in contrast to our previous ideal agent) it also displaces its position when it reorients.\n",
    "\n",
    "- Instead of kick, here we implement a time delay $\\tau$. The agent needs to wait $\\tau$ time steps from the moment it gets a target until it can get the same target again. \n",
    "\n",
    "- When the agent crosses a target, it resets the counter of steps walking straight. Its position doesn't change, it continues from where it was when it found the target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "902917eb-6068-4978-80dd-98bfd978a113",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "@jitclass([(\"target_positions\", float64[:,:]) ,\n",
    "           (\"current_rewards\", float64[:]) ,\n",
    "           (\"tau_allagents\", float64[:]) ,\n",
    "           (\"depleted\", float64[:,:]) ,\n",
    "           (\"current_directions\", float64[:]) ,\n",
    "           (\"positions\", float64[:,:]),\n",
    "           (\"previous_pos\", float64[:,:]),\n",
    "           (\"mask\", bool_[:])])\n",
    "           \n",
    "class RobotSearch():\n",
    "    Nt : int\n",
    "    L : float\n",
    "    r : float\n",
    "    tau : float\n",
    "    agent_radius:float\n",
    "    avg_vel:float\n",
    "    std_vel:float\n",
    "    avg_turn_angle:float\n",
    "    std_turn_angle:float\n",
    "    num_agents : int\n",
    "    target_positions : np.ndarray\n",
    "    current_rewards : np.array\n",
    "    tau_allagents : np.array\n",
    "    depleted : np.array\n",
    "    current_directions : np.array\n",
    "    positions : np.array\n",
    "    previous_pos : np.array\n",
    "    mask : np.array\n",
    "    \n",
    "    \n",
    "    def __init__(self,\n",
    "                 Nt:int, # Number of targets.\n",
    "                 L:float, # Size of the (squared) world.\n",
    "                 r:float, # Radius with center the target position. It defines the area in which agent detects the target.\n",
    "                 tau:float, # Time it takes for the targets to replenish.\n",
    "                 agent_radius:float, # Size of the agent (radius, from the center to the side).\n",
    "                 avg_vel:float, # Mean displacement of the agent per time step (v). Normal distr.: N(v,std_v).\n",
    "                 std_vel:float, # Std for the normal distr. of the agent's velocity (std_v).\n",
    "                 avg_turn_angle:float, # Mean turning angle (in rad) when agent decides to turn (Theta). Normal distr.: N(Theta,std_theta).\n",
    "                 std_turn_angle:float, # Std for the normal distr. of the agent's turning velocity (std_theta).\n",
    "                 num_agents:int=1 # Number of agents that forage at the same time.\n",
    "                 ):\n",
    "        \n",
    "        \"\"\"        \n",
    "        Class defining the foraging environment with replenishable targets modeled with a time delay instead of a kick. \n",
    "        It includes the methods to simulate the robot dynamics.\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "        self.Nt = Nt\n",
    "        self.L = L\n",
    "        self.r = r\n",
    "        self.agent_radius = agent_radius\n",
    "        self.avg_vel = avg_vel\n",
    "        self.std_vel = std_vel\n",
    "        self.avg_turn_angle = avg_turn_angle\n",
    "        self.std_turn_angle = std_turn_angle\n",
    "        self.num_agents = num_agents\n",
    "        \n",
    "        # same replenishing time for all agents\n",
    "        self.tau_allagents = tau * np.ones(self.num_agents)\n",
    "        \n",
    "\n",
    "        self.init_env()\n",
    "        \n",
    "    def init_env(self):\n",
    "        \"\"\"\n",
    "        Environment initialization.\n",
    "        \"\"\"\n",
    "        self.target_positions = np.random.rand(self.Nt, 2)*self.L\n",
    "        \n",
    "        #store who is/was rewarded\n",
    "        self.current_rewards = np.zeros(self.num_agents)\n",
    "        \n",
    "        #number of steps since last target encounter, for each agent.\n",
    "        self.depleted = np.zeros((self.Nt, self.num_agents))\n",
    "        \n",
    "        #set positions and directions of the agents\n",
    "        self.current_directions = np.random.rand(self.num_agents)*2*np.pi\n",
    "        self.positions = np.random.rand(self.num_agents, 2)*self.L\n",
    "        self.previous_pos = self.positions.copy()       \n",
    "\n",
    "        \n",
    "\n",
    "    def update_pos(self, action, agent_index = 0):        \n",
    "        \"\"\"\n",
    "        Updates information of the agent depending on its decision.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        action : int\n",
    "            0 (continue), 1 (left), 2 (right).\n",
    "        agent_index : int, optional\n",
    "            Index of the given agent. The default is 0.\n",
    "            \n",
    "        Returns\n",
    "        ----------\n",
    "        (x_arc, y_arc): (np.array, np.array)\n",
    "            Positions of the turning arc. In case the agent does not turn, it returns (None, None).\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        # Save previous position to check if crossing happened\n",
    "        self.previous_pos[agent_index] = self.positions[agent_index].copy()\n",
    "        \n",
    "        if action == 0: #continue\n",
    "            # Update the velocity of the agent\n",
    "            v = np.random.normal( max(0,self.avg_vel), max(0,self.std_vel))\n",
    "            # Get the positional changes\n",
    "            dx = v * np.cos(self.current_directions[agent_index])\n",
    "            dy = v * np.sin(self.current_directions[agent_index])\n",
    "            \n",
    "            x_arc = None\n",
    "            y_arc = None\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            if action == 1: #left turn\n",
    "                # Get angle difference\n",
    "                dtheta = np.random.normal(self.avg_turn_angle, self.std_turn_angle)\n",
    "                left = True\n",
    "\n",
    "            elif action == 2: #right turn\n",
    "                # Get angle difference\n",
    "                dtheta = - np.random.normal(self.avg_turn_angle, self.std_turn_angle)\n",
    "                left = False\n",
    "\n",
    "            # Get new orientation of the agent\n",
    "            new_theta = self.current_directions[agent_index] + dtheta\n",
    "            # Get the positional changes\n",
    "            dx =  np.sign(dtheta) * self.agent_radius * (np.sin(new_theta) - np.sin(self.current_directions[agent_index]))\n",
    "            dy = - np.sign(dtheta) * self.agent_radius * (np.cos(new_theta) - np.cos(self.current_directions[agent_index]))\n",
    "            \n",
    "            # Get arc coordinates (default: 50 points)\n",
    "            x_arc, y_arc = rotation_arc(self.current_directions[agent_index], dtheta, left, R=self.agent_radius, x=self.positions[agent_index][0], y=self.positions[agent_index][1])\n",
    "            \n",
    "            # Update agent's orientation\n",
    "            self.current_directions[agent_index] = new_theta\n",
    "        \n",
    "       \n",
    "        # Update the position\n",
    "        self.positions[agent_index][0] += dx\n",
    "        self.positions[agent_index][1] += dy\n",
    "        \n",
    "        return x_arc, y_arc\n",
    "        \n",
    "       \n",
    "    def check_encounter(self, action, x_arc, y_arc, agent_index = 0):\n",
    "        \"\"\"\n",
    "        Checks whether the agent found a target, and updates the information accordingly.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        action : int\n",
    "            Action index. Continue (0), turn left (1) or turn right (2).\n",
    "        x_arc : np.array or None\n",
    "            Horizontal components of the turning arc coordinates. None if agent walked straight.\n",
    "        y_arc : np.array or None\n",
    "            Vertical components of the turning arc coordinates. None if agent walked straight.\n",
    "        agent_index : int, optional\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        True if the agent found a target.\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        # Check if there was a target encounter\n",
    "        if action == 0:\n",
    "            encounters = isBetween_c_Vec_numba(self.previous_pos[agent_index], self.positions[agent_index], self.target_positions, self.r)\n",
    "        else:\n",
    "            encounters = arc_in_target_numba(self.target_positions, self.r, x_arc, y_arc)\n",
    "            \n",
    "        #In case there were encounters:\n",
    "        if np.sum(encounters) > 0: \n",
    "            # If there is more than 1 encounter, pick the closest to the agent.\n",
    "            if sum(encounters) == 1:\n",
    "                first_encounter = np.argwhere(encounters == True).flatten()\n",
    "                \n",
    "            else:\n",
    "                # compute the distance from the previous position to each target            \n",
    "                distance_previous_pos = np.sqrt((self.previous_pos[agent_index][0]- self.target_positions[:, 0])**2 + (self.previous_pos[agent_index][1] - self.target_positions[:, 1])**2)            \n",
    "                # checking which encountered point is closer to previous position\n",
    "                min_distance_masked = np.argmin(distance_previous_pos[encounters])\n",
    "                first_encounter = np.argwhere(encounters == True)[min_distance_masked].flatten()\n",
    "           \n",
    "            # If the agent is NOT waiting for the recently found target to replenish:\n",
    "            if not self.depleted[first_encounter, agent_index]:\n",
    "                # We add the information that this agent got a reward\n",
    "                self.current_rewards[agent_index] = 1\n",
    "                # The replenishing time for this target and this agent starts.\n",
    "                self.depleted[first_encounter, agent_index] += 1\n",
    "                \n",
    "                # The env issues a positive reward.\n",
    "                return 1\n",
    "            else:\n",
    "                self.current_rewards[agent_index] = 0\n",
    "                return 0\n",
    "                    \n",
    "        else:\n",
    "            self.current_rewards[agent_index] = 0\n",
    "            return 0   \n",
    "    \n",
    "    def update_replenishing_times(self, agent_index = 0):\n",
    "        \"\"\"\n",
    "        Updates the replenishing times of the targets for a given agent.\n",
    "        \"\"\"\n",
    "        self.depleted[np.argwhere(self.depleted[:, agent_index] != 0).flatten(), agent_index] += 1\n",
    "        self.depleted[:, agent_index] = self.depleted[:, agent_index] % (self.tau_allagents[agent_index] + 1)\n",
    "        \n",
    "    def check_bc(self, agent_index = 0):\n",
    "        \"\"\"\n",
    "        Updates position coordinates of a given agent to fulfill fixed boundary conditions.\n",
    "\n",
    "        \"\"\"\n",
    "        self.positions[agent_index][0]  = max( 0, min( self.positions[agent_index][0] , self.L ) )\n",
    "        self.positions[agent_index][1]  = max( 0, min( self.positions[agent_index][1] , self.L ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f9b4c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "\n",
    "### Tests \n",
    "\n",
    "env = RobotSearch(Nt=100, \n",
    "                 L=100, \n",
    "                 r=1, \n",
    "                 tau=3, \n",
    "                 agent_radius=2, # Size of the agent (radius, from the center to the side).\n",
    "                 avg_vel=1, # Mean displacement of the agent per time step (v). Normal distr.: N(v,std_v).\n",
    "                 std_vel=0.3, # Std for the normal distr. of the agent's velocity (std_v).\n",
    "                 avg_turn_angle=np.pi/6, # Mean turning angle (in rad) when agent decides to turn (Theta). Normal distr.: N(Theta,std_theta).\n",
    "                 std_turn_angle=np.pi/18) # Std for the normal distr. of the agent's turning velocity (std_theta).\n",
    "\n",
    "env.check_encounter(0,np.random.rand(20),np.random.rand(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32fea60d-a54f-4ef2-b637-24d969e2e8bf",
   "metadata": {},
   "source": [
    "# Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a9f3129-cdd6-4a8a-b339-6e646f7ad8f4",
   "metadata": {},
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84de1bb1-523b-4041-863c-8f3f7e596382",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               \n",
    "@njit\n",
    "def train_loop_robot(episodes : int, # Number of episodes to train\n",
    "                   time_ep : int, # Length of episode\n",
    "                   agent : object, # Agent class\n",
    "                   env : object, # Environment class\n",
    "                   h_mat_allT : bool = False # If True, returns the h_matrix at all times\n",
    "                  )-> tuple: # Rewards and h-matrix of the trained agent      \n",
    "    '''    \n",
    "    Given an agent and environment, performs a loop train for the `RobotSearch` environment.\n",
    "    '''\n",
    "\n",
    "    if h_mat_allT: policy_t = np.zeros((episodes, agent.h_matrix.shape[-1]))\n",
    "    \n",
    "    save_rewards = np.zeros((episodes, time_ep))\n",
    "    \n",
    "    for ep in range(episodes):\n",
    "        #initialize environment and agent's counter and g matrix\n",
    "        env.init_env()\n",
    "        agent.agent_state = 0\n",
    "        agent.reset_g()\n",
    "\n",
    "        for t in range(time_ep):\n",
    "            agent.N_upd_H += 1\n",
    "            agent.N_upd_G += 1\n",
    "            \n",
    "            #step to set counter to its min value n=1\n",
    "            if t == 0 or env.current_rewards[0] == 1:\n",
    "                #do one step with random direction (no learning in this step)\n",
    "                action = rand_choice_nb(arr = np.arange(1, agent.num_actions), prob = np.array([1/len(np.arange(1,agent.num_actions))]*len(np.arange(1,agent.num_actions))))\n",
    "                _,_ = env.update_pos(action)\n",
    "                #check boundary conditions\n",
    "                env.check_bc()\n",
    "                #reset counter\n",
    "                agent.agent_state = 0\n",
    "                #update current rewards\n",
    "                env.current_rewards[0] = 0\n",
    "                reward = 0\n",
    "                #update replenishing times\n",
    "                env.update_replenishing_times()\n",
    "\n",
    "            else:\n",
    "                #get perception\n",
    "                state = agent.get_state()\n",
    "                #decide\n",
    "                action = agent.deliberate(state)\n",
    "                #act (update counter)\n",
    "                agent.act(action)\n",
    "\n",
    "                #update positions and get coordinates for turning arc (in case agent turned, otherwise these are None)\n",
    "                x_arc, y_arc = env.update_pos(action)\n",
    "                \n",
    "                #update replenishing times\n",
    "                env.update_replenishing_times()\n",
    "                \n",
    "                #check if target was found \n",
    "                reward = env.check_encounter(action, x_arc, y_arc)\n",
    "                \n",
    "                #check boundary conditions\n",
    "                env.check_bc()\n",
    "                #learn\n",
    "                if reward == 1:\n",
    "                    agent._learn_post_reward(reward)\n",
    "\n",
    "            if agent.N_upd_H == agent.max_no_H_update-1:\n",
    "                agent._learn_post_reward(reward)\n",
    "\n",
    "            # Saving\n",
    "            save_rewards[ep, t] = reward\n",
    "            \n",
    "        #Update of the H matrix at the end of the episode in case it was not updated in the last time steps\n",
    "        if agent.N_upd_H != 0:\n",
    "            agent._learn_post_reward(0)\n",
    "        if h_mat_allT: policy_t[ep] = agent.h_matrix[0,:] / agent.h_matrix.sum(0)\n",
    "        \n",
    "      \n",
    "    return (save_rewards, policy_t) if h_mat_allT else (save_rewards, agent.h_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "170c19f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rl_opts.rl_framework.numba.agents import Forager\n",
    "from rl_opts.robot_search import RobotSearch, train_loop_robot  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f36705d-36e6-45a5-9a95-f6c7510ab0d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_ep = 20000\n",
    "\n",
    "env = RobotSearch(Nt=100, \n",
    "                 L=100, \n",
    "                 r=1, \n",
    "                 tau=3, \n",
    "                 agent_radius=2, # Size of the agent (radius, from the center to the side).\n",
    "                 avg_vel=1, # Mean displacement of the agent per time step (v). Normal distr.: N(v,std_v).\n",
    "                 std_vel=0.3, # Std for the normal distr. of the agent's velocity (std_v).\n",
    "                 avg_turn_angle=np.pi/6, # Mean turning angle (in rad) when agent decides to turn (Theta). Normal distr.: N(Theta,std_theta).\n",
    "                 std_turn_angle=np.pi/18) # Std for the normal distr. of the agent's turning velocity (std_theta).\n",
    "\n",
    "agent = Forager(num_actions = 3, # From here are props of the agent (see Forager for details)\n",
    "                gamma_damping = 0.00001,\n",
    "                size_state_space = np.array([time_ep], dtype=np.int64),\n",
    "                initial_prob_distr = (np.array([0.988, 0.012/2, 0.012/2])*np.ones((3, time_ep)).transpose()).transpose())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0efc4cb-4846-4666-8042-7cbfc6f216d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "rew, h = train_loop_robot(episodes = 50, time_ep = time_ep, agent = agent, env = env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b496ca54-8192-4d37-9259-67ffeb06cda0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current memory usage: 9.59 MB\n",
      "Peak memory usage: 10.40 MB\n"
     ]
    }
   ],
   "source": [
    "import tracemalloc\n",
    "\n",
    "# Start tracing memory allocations\n",
    "tracemalloc.start()\n",
    "\n",
    "# Run your function\n",
    "rew, h = train_loop_robot(episodes = 50, time_ep = time_ep, agent = agent, env = env)\n",
    "\n",
    "# Get the current memory usage and peak memory usage\n",
    "current, peak = tracemalloc.get_traced_memory()\n",
    "\n",
    "# Convert bytes to more readable units (e.g., MB)\n",
    "print(f\"Current memory usage: {current / 10**6:.2f} MB\")\n",
    "print(f\"Peak memory usage: {peak / 10**6:.2f} MB\")\n",
    "\n",
    "# Stop tracing\n",
    "tracemalloc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "052e4c84-c84e-4465-84d9-14fe8f1387c9",
   "metadata": {},
   "source": [
    "## Parallel training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca276db6-3318-4577-87eb-fce17438c8c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@njit(parallel = True)\n",
    "def run_robot_training_parallel(episodes, # Number of episodes\n",
    "                                time_ep, # Length of episode\n",
    "                                N_agents, # Number of agents               \n",
    "                                h_mat_allT = False, # If to save the h_matrix at all times\n",
    "                                Nt = 100, # From here are props of the environment (see TargetEnv for details) \n",
    "                                L = 100, \n",
    "                                r = 0.5, \n",
    "                                tau = 3,\n",
    "                                agent_radius = 2,\n",
    "                                avg_vel=1,\n",
    "                                std_vel=0.3,\n",
    "                                avg_turn_angle=np.pi/6,\n",
    "                                std_turn_angle=np.pi/18,\n",
    "                                num_actions = 3, # From here are props of the agent (see Forager for details)\n",
    "                                size_state_space = np.array([100], dtype=np.int64), \n",
    "                                gamma_damping = 0.00001,\n",
    "                                eta_glow_damping = 0.1,\n",
    "                                initial_prob_distr = np.array([[],[],[]]),\n",
    "                                policy_type = 'standard', \n",
    "                                beta_softmax = 3,  \n",
    "                                fixed_policy = np.array([[],[]]),\n",
    "                                max_no_H_update = int(1e3)\n",
    "                               ):\n",
    "    \n",
    "    save_rewards = np.zeros((N_agents, episodes))\n",
    "    \n",
    "    if h_mat_allT:\n",
    "        save_h_matrix = np.zeros((N_agents, episodes, size_state_space[0]))  \n",
    "    else:        \n",
    "        save_h_matrix = np.zeros((N_agents, num_actions, size_state_space[0])) \n",
    "    \n",
    "    for n_agent in prange(N_agents):\n",
    "        \n",
    "        env = RobotSearch(Nt, L, r, tau, agent_radius, avg_vel, std_vel, avg_turn_angle, std_turn_angle,1)\n",
    "        \n",
    "        agent = Forager(num_actions,size_state_space,gamma_damping,\n",
    "                        eta_glow_damping,policy_type,beta_softmax,\n",
    "                        initial_prob_distr,fixed_policy,max_no_H_update,'s')\n",
    "                          \n",
    "        rews, mat = train_loop_robot(episodes, time_ep, agent, env, h_mat_allT)\n",
    "\n",
    "             \n",
    "        #mean reward per episode\n",
    "        for t in range(episodes):\n",
    "            save_rewards[n_agent, t] = np.mean(rews[t])\n",
    "        \n",
    "        save_h_matrix[n_agent] = mat\n",
    "        \n",
    "    return save_rewards, save_h_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b77d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rl_opts.robot_search import RobotSearch, train_loop_robot\n",
    "from rl_opts.rl_framework.numba.agents import Forager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac8de1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPISODES = 9\n",
    "TIME_EP = 10\n",
    "N_AGENTS = 4\n",
    "\n",
    "\n",
    "rewards, h_matrix = run_robot_training_parallel(episodes = int(EPISODES), \n",
    "                                                time_ep = int(TIME_EP), \n",
    "                                                N_agents = N_AGENTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e24c049a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 9)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rewards.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d12677d-9b61-46da-bc2a-275d44327d85",
   "metadata": {},
   "source": [
    "# Walk from policy\n",
    "\n",
    "These replicate what we were doing in `rl_opts.learn_and_bench.walk_from_policy` and help get efficiencies for fixed policies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "242babee-e1ff-42d9-80e6-c918bc34c350",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export   \n",
    "@njit\n",
    "def single_robot_trajectory(time_ep : int, # Length of each run / episode\n",
    "                            policy : np.array, # Policy of the walker (rows: states. columns:prob of doing each action)\n",
    "                            env : object # Environment where the walker moves\n",
    "                           )-> tuple: # number of visited targets, agent and target positions\n",
    "\n",
    "    '''\n",
    "    Walk of a single agent in env of type RobotSearch given a policy. Performance is evaluated as the number of targets found in a fixed time time_ep.\n",
    "    '''\n",
    "    \n",
    "    number_visited_targets = 0\n",
    "    agent_positions = np.zeros((time_ep, 2))\n",
    "    rew_per_timestep = np.zeros(time_ep)\n",
    "    \n",
    "    #initialize environment and agent's counter and g matrix\n",
    "    env.init_env()\n",
    "    agent_state = 0\n",
    "\n",
    "    for t in range(time_ep):\n",
    "        \n",
    "        agent_positions[t,:] = env.positions[0] #position of agent with index 0\n",
    "        \n",
    "        if t == 0 or env.current_rewards[0] == 1:\n",
    "            # change direction\n",
    "            action = rand_choice_nb(arr = np.arange(1, len(policy[0])), prob = np.array([1/len(np.arange(1,len(policy[0])))]*len(np.arange(1,len(policy[0])))))\n",
    "            _,_ = env.update_pos(action)\n",
    "            #check boundary conditions\n",
    "            env.check_bc()\n",
    "            #reset counter\n",
    "            agent_state = 0\n",
    "            #update current rewards\n",
    "            env.current_rewards[0] = 0\n",
    "            reward = 0\n",
    "            #update replenishing times\n",
    "            env.update_replenishing_times()\n",
    "            \n",
    "\n",
    "        else: \n",
    "            # decide       \n",
    "            action = rand_choice_nb(arr = np.arange(len(policy[0])), prob = policy[agent_state])\n",
    "            #update positions\n",
    "            x_arc, y_arc = env.update_pos(action)\n",
    "            #update replenishing times\n",
    "            env.update_replenishing_times()\n",
    "            \n",
    "            #check if target was found \n",
    "            reward = env.check_encounter(action, x_arc, y_arc)\n",
    "            \n",
    "            #check boundary conditions\n",
    "            env.check_bc()\n",
    "            \n",
    "            # update agent_state\n",
    "            if action == 0: #continue\n",
    "                agent_state += 1\n",
    "            else: #turn\n",
    "                agent_state = 0\n",
    "\n",
    "            number_visited_targets += reward\n",
    "            rew_per_timestep[t] += reward\n",
    "                \n",
    "    return (number_visited_targets, agent_positions, env.target_positions, rew_per_timestep) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8403723-95e9-457a-89c2-e8f386025b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@njit(parallel = True)\n",
    "def parallel_robots_trajectories(time_ep : int, # Length of each run / episode\n",
    "                                 N_agents : int, # Number of agents to consider\n",
    "                                 Nt = 100, # Number of targets in the environment\n",
    "                                 L = 100, # Size of the environment\n",
    "                                 r = 0.5, # Radius of the targets\n",
    "                                 tau = 3, # Time delay until target is available again\n",
    "                                 agent_radius = 2,\n",
    "                                 avg_vel=1,\n",
    "                                 std_vel=0.3,\n",
    "                                 avg_turn_angle=np.pi/6,\n",
    "                                 std_turn_angle=np.pi/18,\n",
    "                                 policies = np.array([[1.0,0.0,0.0,0.0], [0.0,1.0,0.0,0.0]]) # Policy of the agents\n",
    "                                )-> tuple: # number of visited targets, agent and target positions\n",
    "    \"\"\"\n",
    "    Runs in parallel single_robot_trajectory. Due to numba props, we need to give all parameters as inputs (see source).\n",
    "    \"\"\"\n",
    "    \n",
    "    agent_positions = np.zeros((N_agents, time_ep, 2))\n",
    "    number_visited_targets = np.zeros(N_agents)\n",
    "    target_positions = np.zeros((N_agents, Nt, 2))\n",
    "    reward_per_timestep = np.zeros((N_agents, time_ep))\n",
    "    \n",
    "    for n_agent in prange(N_agents):\n",
    "        \n",
    "        env = RobotSearch(Nt, L, r, tau, agent_radius, avg_vel, std_vel, avg_turn_angle, std_turn_angle,1)\n",
    "        \n",
    "        num_tar, pos, tar_pos, rew_t = single_robot_trajectory(time_ep, policies[n_agent], env) \n",
    "    \n",
    "        agent_positions[n_agent] = pos\n",
    "        number_visited_targets[n_agent] = num_tar\n",
    "        target_positions[n_agent] = tar_pos\n",
    "        reward_per_timestep[n_agent] = rew_t\n",
    "        \n",
    "    return (number_visited_targets, agent_positions, target_positions, reward_per_timestep)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a841da4-030a-415e-8bcf-42d3c8deacdb",
   "metadata": {},
   "source": [
    "#| hide\n",
    "# nbdev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f310a72-f356-4689-ad09-96c9747e912d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev import nbdev_export ; nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
