{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "432b43b7-63c8-4360-83c9-098856e14cdf",
   "metadata": {},
   "source": [
    "# Reinforcement learning environments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e936db82-2862-41d4-83ea-6688b945127d",
   "metadata": {},
   "source": [
    "This notebook gathers the functions creating different kinds of environments for foraging and target search in various scenarios, adapted for their use in the reinforcement learning paradigm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8661f884-5d59-469a-90a5-c65f6f72a68d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp rl_framework.numba.environments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebdde34f-4614-4fad-acc9-b49ef769db41",
   "metadata": {},
   "source": [
    "#| hide\n",
    "# Libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc944947-aef5-4f8f-bc02-54b4abda36dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import numpy as np\n",
    "import numba\n",
    "from numba.experimental import jitclass\n",
    "from numba import float64, bool_, prange, njit\n",
    "import math\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f1e505d-55fb-4644-8f6e-639c3c7dca96",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00e5b6cb-6385-4c84-be00-cbace1d00d0c",
   "metadata": {},
   "source": [
    "# Helpers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd57c4a6-ebf8-4cb0-9395-a9a707f67415",
   "metadata": {},
   "source": [
    "## isBetween"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36567cb6-7e16-4860-bd57-c43be26fa8ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "@njit\n",
    "def isBetween_c_Vec_numba(a, b, c, r):\n",
    "        \"\"\"\n",
    "        Checks whether point c is crossing the line formed with point a and b.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        a : tensor, shape = (1,2)\n",
    "            Previous position.\n",
    "        b : tensor, shape = (1,2)\n",
    "            Current position.\n",
    "        c : tensor, shape = (Nt,2)\n",
    "            Positions of all targets.\n",
    "        r : int/float\n",
    "            Target radius.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        mask : array of boolean values\n",
    "            True at the indices of found targets.\n",
    "\n",
    "        \"\"\"\n",
    "        if (a == b).all():\n",
    "            return np.array([False]*c.shape[0])\n",
    "\n",
    "        mask = np.array([True]*c.shape[0])\n",
    "        \n",
    "        dotproduct = (c[:, 0] - a[0]) * (b[0] - a[0]) + (c[:, 1] - a[1])*(b[1] - a[1])\n",
    "        squaredlengthba = (b[0] - a[0])*(b[0] - a[0]) + (b[1] - a[1])*(b[1] - a[1])\n",
    "        \n",
    "        #exclude the targets whose vertical projection of the vector c-a w.r.t. the vector b-a is larger than the target radius.\n",
    "        idx = np.argwhere(np.abs(numba.np.arraymath.cross2d(b-a, c-a))/np.linalg.norm(b-a) > r) \n",
    "        for i1 in idx:\n",
    "            mask[i1] = False        \n",
    "        \n",
    "        #exclude the targets whose scalar product is negative (they are on the other side of the step direction)\n",
    "        for i2 in np.argwhere(dotproduct < 0):\n",
    "            mask[i2] = False\n",
    "\n",
    "        #exclude the targets that are beyond the step.\n",
    "        for i3 in np.argwhere(dotproduct > squaredlengthba):\n",
    "            mask[i3] = False\n",
    "            \n",
    "        return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "563b378d-3038-4805-81f5-310571527bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "compiling = isBetween_c_Vec_numba(np.array([0.1,1]), np.array([1,3]), np.random.rand(100,2), 0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85311de9-5c87-4128-b2b6-aa6b139fb829",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.65 μs ± 25.2 ns per loop (mean ± std. dev. of 7 runs, 100,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit isBetween_c_Vec_numba(np.array([0.1,1]), np.array([1,3]), np.random.rand(100,2), 0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd700d0-2844-454c-a0ce-ab2a147c5bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rl_opts.utils import isBetween_c_Vec as oldbetween"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f346fb-44b9-4d7f-8273-e2f9b71faca5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40.4 μs ± 177 ns per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit oldbetween(np.array([0.1,1]), np.array([1,3]), np.random.rand(100,2), 0.00001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb1ecda-8072-4072-9d0f-3eb8689a3f28",
   "metadata": {},
   "source": [
    "## Pareto sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1023cf70-3639-4629-9ab1-76eb94bd20ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "@njit\n",
    "def pareto_sample(alpha, xm, size=1):\n",
    "    samples = np.zeros(size)\n",
    "    for ii in range(size):\n",
    "        u = random.random()  # Uniform random variable between 0 and 1\n",
    "        x = xm / (u ** (1 / alpha))\n",
    "        samples[ii] = x\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c099a4b2-2b60-4adb-bc60-beca05387032",
   "metadata": {},
   "source": [
    "## Random sampling from array with probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a5fee3-fac9-42fe-88a1-b12b0f4493be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "@njit\n",
    "def rand_choice_nb(arr, prob):\n",
    "    \"\"\"\n",
    "    :param arr: A 1D numpy array of values to sample from.\n",
    "    :param prob: A 1D numpy array of probabilities for the given samples.\n",
    "    :return: A random sample from the given array with a given probability.\n",
    "    \"\"\"\n",
    "    return arr[np.searchsorted(np.cumsum(prob), np.random.random(), side=\"right\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0370360-425e-4698-9383-c1ff4461ddb5",
   "metadata": {},
   "source": [
    "# TargetEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d3f8518-7596-4126-ae87-9d48e4dbfbab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "@jitclass([(\"target_positions\", float64[:,:]) ,\n",
    "           (\"current_rewards\", float64[:]) ,\n",
    "           (\"kicked\", float64[:]) ,\n",
    "           (\"current_directions\", float64[:]) ,\n",
    "           (\"positions\", float64[:,:]),\n",
    "           (\"previous_pos\", float64[:,:]),\n",
    "           (\"lc\", float64[:,:]),\n",
    "           (\"mask\", bool_[:]),\n",
    "           (\"first_encounter\", float64[:,:])])\n",
    "class TargetEnv():\n",
    "    Nt : int\n",
    "    L : float\n",
    "    r : float\n",
    "    lc : np.array\n",
    "    agent_step : float\n",
    "    num_agents : int\n",
    "    destructive_targets : bool\n",
    "    target_positions : np.ndarray\n",
    "    current_rewards : np.array\n",
    "    kicked : np.array\n",
    "    current_directions : np.array\n",
    "    positions : np.array\n",
    "    previous_pos : np.array\n",
    "    mask : np.array\n",
    "    first_encounter : np.array\n",
    "    lc_distribution : str\n",
    "    \n",
    "    \n",
    "    def __init__(self,\n",
    "                 Nt = 10, # Number of targets.\n",
    "                 L = 1.3, #  Size of the (squared) world.\n",
    "                 r = 1.5, # Radius with center the target position. It defines the area in which agent detects the target.\n",
    "                 lc = np.array([[1.0],[1]]), # Cutoff length. Displacement away from target \n",
    "                 agent_step = 1, # Displacement of one step. The default is 1.\n",
    "                 num_agents = 1, # Number of agents that forage at the same time. The default is 1. > 1 not fully implemented\n",
    "                 destructive = False, # True if targets are destructive. The default is False.\n",
    "                 lc_distribution = 'constant' # Distribution from where to sample l_c. Can be 'power_law', 'pareto' or something else. See comments self.check_encounter for explanations\n",
    "                ):\n",
    "        \n",
    "        \"\"\"\n",
    "        Class defining the a Foraging environment with multiple targets and two actions: continue in \n",
    "        the same direction and turn by a random angle.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.Nt = Nt\n",
    "        self.L = L\n",
    "        self.r = r\n",
    "        self.lc = lc\n",
    "        self.agent_step = agent_step \n",
    "        self.num_agents = num_agents\n",
    "        self.destructive_targets = destructive\n",
    "        self.lc_distribution = lc_distribution\n",
    "        \n",
    "\n",
    "        self.init_env()\n",
    "        \n",
    "    def init_env(self):\n",
    "        \"\"\"\n",
    "        Environment initialization.\n",
    "        \"\"\"\n",
    "        self.target_positions = np.random.rand(self.Nt, 2)*self.L\n",
    "        \n",
    "        #store who is/was rewarded\n",
    "        self.current_rewards = np.zeros(self.num_agents)\n",
    "        \n",
    "        #signal whether agent has been kicked\n",
    "        self.kicked = np.zeros(self.num_agents)\n",
    "        \n",
    "        #set positions and directions of the agents\n",
    "        self.current_directions = np.random.rand(self.num_agents)*2*np.pi\n",
    "        self.positions = np.random.rand(self.num_agents, 2)*self.L\n",
    "        self.previous_pos = self.positions.copy()       \n",
    "\n",
    "        \n",
    "\n",
    "    def update_pos(self, \n",
    "                   change_direction, # Whether the agent decided to turn or not.\n",
    "                   agent_index = 0 # Index of the given agent. The default is 0. This is only keeped for future devs\n",
    "                  ):        \n",
    "        \"\"\"\n",
    "        Updates information of the agent depending on its decision.            \n",
    "        \"\"\"\n",
    "        \n",
    "        # Save previous position to check if crossing happened\n",
    "        self.previous_pos[agent_index] = self.positions[agent_index].copy()\n",
    "        \n",
    "        if change_direction:\n",
    "            self.current_directions[agent_index] = random.uniform(0,1)*2*math.pi\n",
    "        \n",
    "        #Update position\n",
    "        self.positions[agent_index][0] = self.positions[agent_index][0] + self.agent_step*np.cos(self.current_directions[agent_index])\n",
    "        self.positions[agent_index][1] = self.positions[agent_index][1] + self.agent_step*np.sin(self.current_directions[agent_index])\n",
    "        \n",
    "    def update_pos_disp(self, \n",
    "                    displacement, # tuple or array stating (disp_x, disp_y)                        \n",
    "                   ):\n",
    "        \"\"\"\n",
    "        Updates the position of the agent based on the input displacement\n",
    "        \"\"\"\n",
    "        agent_index = 0 # index of the agent. For collective experiments we should put this as input\n",
    "        \n",
    "        # Save previous position to check if crossing happened\n",
    "        self.previous_pos[agent_index] = self.positions[agent_index].copy()\n",
    "        \n",
    "        #Update position\n",
    "        self.positions[agent_index][0] = self.positions[agent_index][0] + displacement[0]\n",
    "        self.positions[agent_index][1] = self.positions[agent_index][1] + displacement[1]\n",
    "        \n",
    "    def check_encounter(self,\n",
    "                       agent_index=0 # Index of the given agent. The default is 0. This is only keeped for future devs\n",
    "                       ): # True if the agent found a target, else False\n",
    "        \"\"\"\n",
    "        Checks whether the agent found a target, and updates the information accordingly.\n",
    "        \"\"\"       \n",
    "        \n",
    "        encounters = isBetween_c_Vec_numba(self.previous_pos[agent_index], self.positions[agent_index], self.target_positions, self.r)\n",
    "        \n",
    "        if sum(encounters) > 0: \n",
    "            \n",
    "            #if there is more than 1 encounter, pick the closest to the agent.\n",
    "            if sum(encounters) == 1:\n",
    "                first_encounter = np.argwhere(encounters == True).flatten()\n",
    "            else:\n",
    "                # compute the distance from the previous position to each target            \n",
    "                distance_previous_pos = np.sqrt((self.previous_pos[agent_index][0]- self.target_positions[:, 0])**2 + (self.previous_pos[agent_index][1] - self.target_positions[:, 1])**2)            \n",
    "                \n",
    "                # checking which encountered point is closer to previous position\n",
    "                min_distance_masked = np.argmin(distance_previous_pos[encounters])\n",
    "                first_encounter = np.argwhere(encounters == True)[min_distance_masked].flatten()\n",
    "            if self.destructive_targets:\n",
    "                self.target_positions[first_encounter] = np.random.rand(2)*self.L\n",
    "            else:\n",
    "                #----KICK----\n",
    "                # If there was encounter, we reset direction and change position of particle to (pos target + lc)\n",
    "                kick_direction = np.random.uniform(low = 0, high = 2*np.pi)\n",
    "                for idx_first in first_encounter: # This is super weird!\n",
    "                    if self.lc_distribution == 'power_law':\n",
    "                        # when we have the power law, the first value of lc is considered to be the exponent.\n",
    "                        # The following samples from a power law x^{-1-alpha} where alpha = self.lc.flatten()[0]                        \n",
    "                        current_lc = (1-random.uniform(0,1))**(-1/self.lc.flatten()[0])\n",
    "\n",
    "                    elif self.lc_distribution == 'pareto':\n",
    "                        # Sampling from Pareto. Here alpha = self.lc.flatten()[0] and x_minim = self.lc.flatten()[0]\n",
    "                        current_lc = pareto_sample(self.lc[0,0], self.lc[1,0])[0]\n",
    "                    else:\n",
    "                        # if lc has a single element, take that one as lc, if not sample\n",
    "                        current_lc = self.lc.flatten()[0] if len(self.lc.flatten()) == 2 else rand_choice_nb(arr = self.lc[0], prob = self.lc[1])\n",
    "                    self.positions[agent_index][0] = self.target_positions[idx_first, 0] + current_lc*np.cos(kick_direction)\n",
    "                    self.positions[agent_index][1] = self.target_positions[idx_first, 1] + current_lc*np.sin(kick_direction)\n",
    "                self.kicked[agent_index] = 1\n",
    "                #------------\n",
    "                \n",
    "            #...and we add the information that this agent got to the target\n",
    "            self.current_rewards[agent_index] = 1              \n",
    "            return 1\n",
    "        \n",
    "        else: \n",
    "            self.kicked[agent_index] = 0\n",
    "            self.current_rewards[agent_index] = 0\n",
    "            return 0   \n",
    "        \n",
    "    def check_bc(self):\n",
    "        \"\"\"\n",
    "        Updates position coordinates of agent agent_index to fulfill periodic boundary conditions.\n",
    "\n",
    "        \"\"\"\n",
    "        agent_index=0\n",
    "        self.positions[agent_index] = (self.positions[agent_index])%self.L\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e875fda0-3264-47ea-b5b2-5c36c6964b9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| hide\n",
    "env = TargetEnv(Nt = 1000,\n",
    "                 L = 123,\n",
    "                 r = 50,\n",
    "                 lc = np.array([[0.1],[1]]),\n",
    "                 lc_distribution = 'pareto')\n",
    "env.check_encounter()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad681096-c21e-426a-9e10-2fbeb42807b9",
   "metadata": {},
   "source": [
    "#|hide \n",
    "#### Runtime testing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb45255-2aa0-457d-a2da-855ff55d283c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide \n",
    "env = TargetEnv(Nt = 1000,\n",
    "                 L = 123,\n",
    "                 r = 50,\n",
    "                 lc = np.array([[0.1],[1]]),\n",
    "                 lc_distribution = 'pareto')\n",
    "compiling = env.check_encounter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99353d94-43b0-4273-9d2e-3b930e10c9a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.18 μs ± 13.7 ns per loop (mean ± std. dev. of 7 runs, 100,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "#|hide \n",
    "%timeit env.check_encounter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8401468f-a199-4eb8-b6b9-c9466e8145d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49.7 μs ± 849 ns per loop (mean ± std. dev. of 7 runs, 10,000 loops each)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gorka/github/rl_opts/rl_opts/utils.py:36: RuntimeWarning: invalid value encountered in divide\n",
      "  mask[np.argwhere(np.abs(np.cross(b-a, c-a))/np.linalg.norm(b-a) > r)] = False\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#|hide \n",
    "#|eval: false\n",
    "\n",
    "# from rl_opts.rl_framework.numpy import TargetEnv as oldEnv\n",
    "\n",
    "oenv = oldEnv(Nt = 100,\n",
    "                 L = 123,\n",
    "                 r = 0.2,\n",
    "                 lc = 1)\n",
    "\n",
    "%timeit oenv.check_encounter()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32539df0-f46b-4ac8-b944-8faae5679b37",
   "metadata": {},
   "source": [
    "# ResetEnv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b5a55ce",
   "metadata": {},
   "source": [
    "### Search loop with fixed policy an arbitrary environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a38073",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "\n",
    "@njit\n",
    "def reset_search_loop(T, # Number of steps \n",
    "                      reset_policy, # Reset policy\n",
    "                      env # Environment\n",
    "                      ):\n",
    "    '''\n",
    "    Loop that runs the reset environment with a given reset policy.\n",
    "    '''\n",
    "    \n",
    "    rewards = 0\n",
    "    tau = 0 # time since last reset\n",
    "    \n",
    "    for t in range(T):\n",
    "        \n",
    "        action = 0 if np.random.rand() > reset_policy[tau] else 1\n",
    "        rew = env.update_pos(action = action)\n",
    "        \n",
    "        if rew == 1 or action == 1:\n",
    "            tau = 0\n",
    "        else:\n",
    "            tau += 1\n",
    "        \n",
    "        rewards += rew\n",
    "    \n",
    "    return rewards\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7abf9375-7e97-43fe-9f91-0bff65d7767d",
   "metadata": {},
   "source": [
    "## 1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "657f70be-ac30-4f9c-8c12-b0938eb60270",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "@jitclass\n",
    "class ResetEnv_1D():\n",
    "    L : float\n",
    "    D : float    \n",
    "    position : float    \n",
    "    \n",
    "    def __init__(self,\n",
    "                 L = 1.3,\n",
    "                 D = 1.0,                    \n",
    "                ):        \n",
    "   \n",
    "        self.L = L\n",
    "        self.D = D\n",
    "        self.position = 0\n",
    "        \n",
    "    def init_env(self):\n",
    "        self.position = 0\n",
    "    \n",
    "    def update_pos(self, \n",
    "                   action # 0: continue walk, 1: reset to origin\n",
    "                  ): # Reward = 1 if crossed L, else = 0\n",
    "        \n",
    "        if action == 0:\n",
    "            self.position += np.random.randn()*np.sqrt(2*self.D)        \n",
    "        else: self.position = 0\n",
    "                \n",
    "        if self.position >= self.L: \n",
    "            self.init_env()\n",
    "            return 1\n",
    "        else: return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbefb75f",
   "metadata": {},
   "source": [
    "### Parallel search loops for Reset 1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8900112c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@njit(parallel = True)\n",
    "def parallel_Reset1D_sharp(T, resets, L, D):\n",
    "    '''\n",
    "    Runs the Reset 1D loop in parallel for different sharp resetting times.\n",
    "    '''\n",
    "    rews_reset = np.zeros_like(resets)\n",
    "    \n",
    "    for idxr in prange(len(resets)):\n",
    "        \n",
    "        env = ResetEnv_1D(L, D)        \n",
    "        reset_policy = np.zeros(resets[idxr])\n",
    "        reset_policy[resets[idxr]-1] = 1        \n",
    "        \n",
    "        rews_reset[idxr] = reset_search_loop(T = T, reset_policy = reset_policy, env = env)\n",
    "    return rews_reset\n",
    "\n",
    "@njit(parallel = True)\n",
    "def parallel_Reset1D_exp(T, rates, L, D):\n",
    "    '''\n",
    "    Runs the Reset 1D loop in parallel for different exponential resetting rates.\n",
    "    '''\n",
    "    \n",
    "    rews_rate = np.zeros_like(rates)\n",
    "    for idxr in prange(len(rates)):\n",
    "        \n",
    "        env = ResetEnv_1D(L, D)        \n",
    "        reset_policy = np.ones(T)*rates[idxr]\n",
    "        \n",
    "        rews_rate[idxr] = reset_search_loop(T = T, reset_policy = reset_policy, env = env)\n",
    "    return rews_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "600c93b4-a288-47aa-aaf9-b4fb52fef424",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37.4 μs ± 2.34 μs per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "#| hide\n",
    "%timeit parallel_Reset1D_sharp(100, np.linspace(70, 150, 20).astype(np.int64), 5, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "539cd73f-9893-4f05-863b-090ec19b0f4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48.2 μs ± 29 μs per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "#| hide\n",
    "%timeit parallel_Reset1D_exp(100, np.linspace(70, 150, 40).astype(np.int64), 5, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba8309e-8b30-4205-9fb5-7ec1fa077da1",
   "metadata": {},
   "source": [
    "## 2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a9ee9ec-6a02-452f-8017-778c9b81cf5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "@jitclass([(\"position\", float64[:]),\n",
    "           (\"target_position\", float64[:,:]),\n",
    "           (\"previous_pos\", float64[:])\n",
    "          ])\n",
    "class ResetEnv_2D():\n",
    "    D : float    \n",
    "    dist_target :float\n",
    "    r: float\n",
    "    position : np.array    \n",
    "    target_position : np.array\n",
    "    previous_pos : np.array\n",
    "    \n",
    "    def __init__(self,\n",
    "                 dist_target = 0.2, # Distance from init position and target\n",
    "                 radius_target = 0.5, # Radius of the target\n",
    "                 D = 1.0, # Diffusion coefficient of the walker              \n",
    "                ):    \n",
    "        \n",
    "\n",
    "        self.D = D\n",
    "        self.dist_target = dist_target\n",
    "        self.r = radius_target        \n",
    "        \n",
    "        self.target_position = np.array([self.dist_target*np.cos(np.pi/4), self.dist_target*np.sin(np.pi/4)])[np.newaxis, :]       \n",
    "        \n",
    "        self.init_env()\n",
    "        \n",
    "    def init_env(self):\n",
    "        self.position = np.array([0.0,0.0])\n",
    "        self.previous_pos = self.position.copy()     \n",
    "        \n",
    "        \n",
    "    \n",
    "    def update_pos(self, \n",
    "                   action # 0: continue walk, 1: reset to origin\n",
    "                  ): # Reward = 1 if encountered target, else = 0\n",
    "        \n",
    "        if action == 1:\n",
    "            self.init_env()\n",
    "            return 0\n",
    "        \n",
    "        elif action == 0:\n",
    "            \n",
    "            self.previous_pos = self.position.copy()            \n",
    "            self.position += np.random.randn(2)*np.sqrt(2*self.D)\n",
    "            \n",
    "            # Checking encounter\n",
    "            inside_target = np.linalg.norm(self.position-self.target_position) <= self.r\n",
    "            crossed_target = isBetween_c_Vec_numba(self.previous_pos, self.position, self.target_position, self.r)\n",
    "                        \n",
    "            if inside_target or crossed_target:\n",
    "                self.init_env()\n",
    "                return 1\n",
    "            else: \n",
    "                return 0\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de1eda84",
   "metadata": {},
   "source": [
    "## Parallel search loops for Reset 2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64fa95ae-5fc4-42e0-9157-2091de54fa44",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "@njit(parallel = True)\n",
    "def parallel_Reset2D_sharp(T, resets, dist_target, radius_target, D):\n",
    "    rews_reset = np.zeros_like(resets)\n",
    "    \n",
    "    for idxr in prange(len(resets)): \n",
    "        \n",
    "        env = ResetEnv_2D(dist_target, radius_target, D)        \n",
    "        reset_policy = np.zeros(resets[idxr])\n",
    "        reset_policy[resets[idxr]-1] = 1        \n",
    "        \n",
    "        rews_reset[idxr] = reset_search_loop(T = T, reset_policy = reset_policy, env = env)\n",
    "    return rews_reset\n",
    "\n",
    "\n",
    "\n",
    "@njit(parallel = True)\n",
    "def parallel_Reset2D_exp(T, rates, dist_target, radius_target, D):\n",
    "    \n",
    "    rews_rate = np.zeros_like(rates)\n",
    "    for idxr in prange(len(rates)):\n",
    "        \n",
    "        env = ResetEnv_2D(dist_target, radius_target, D)         \n",
    "        reset_policy = np.ones(T)*rates[idxr]\n",
    "        \n",
    "        rews_rate[idxr] = reset_search_loop(T = T, reset_policy = reset_policy, env = env)\n",
    "    return rews_rate\n",
    "\n",
    "@njit(parallel = True)\n",
    "def parallel_Reset2D_policies(T, reset_policies, dist_target, radius_target, D):\n",
    "    \n",
    "    rews_rate = np.zeros(reset_policies.shape[0])    \n",
    "    \n",
    "    for idx_policy in prange(reset_policies.shape[0]):    \n",
    "        \n",
    "        env = ResetEnv_2D(dist_target, radius_target, D)  \n",
    "                \n",
    "        rews_rate[idx_policy] = reset_search_loop(T = T, reset_policy = reset_policies[idx_policy], env = env)\n",
    "        \n",
    "    return rews_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d7e68c-86d2-4ae5-abea-8b2b4df83122",
   "metadata": {},
   "source": [
    "# TurnResetEnv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b732f4-6bff-459a-b5ff-ef8c89e9e353",
   "metadata": {},
   "source": [
    "> Only 2D is considered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "747fb886-db05-40b9-b569-b6b8a361ae60",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "@jitclass([(\"position\", float64[:]),\n",
    "           (\"target_position\", float64[:,:]),\n",
    "           (\"previous_pos\", float64[:])\n",
    "          ])\n",
    "class TurnResetEnv_2D():\n",
    "    agent_step : float    \n",
    "    dist_target :float\n",
    "    r: float\n",
    "    position : np.array    \n",
    "    target_position : np.array\n",
    "    previous_pos : np.array\n",
    "    current_direction : float\n",
    "    \n",
    "    def __init__(self,\n",
    "                 dist_target = 0.2, # Distance from init position and target\n",
    "                 radius_target = 0.5, # Radius of the target\n",
    "                 agent_step = 1.0, # Diffusion coefficient of the walker              \n",
    "                ):   \n",
    "        \n",
    "        '''\n",
    "        Class defining a Foraging environment with a single target and three possible actions:\n",
    "\n",
    "        - Continue in the same direction\n",
    "        - Turn by a random angle\n",
    "        - Reset to the origin\n",
    "\n",
    "        The agent makes steps of constant length given by agent_step. \n",
    "        '''\n",
    "        \n",
    "\n",
    "        self.agent_step = agent_step\n",
    "        self.dist_target = dist_target\n",
    "        self.r = radius_target        \n",
    "        \n",
    "        self.target_position = np.array([self.dist_target*np.cos(np.pi/4), self.dist_target*np.sin(np.pi/4)])[np.newaxis, :]       \n",
    "        \n",
    "        self.init_env()\n",
    "        \n",
    "    def init_env(self):\n",
    "        self.position = np.array([0.0,0.0])\n",
    "        self.current_direction = np.random.rand()*2*np.pi\n",
    "        self.previous_pos = self.position.copy()     \n",
    "        \n",
    "        \n",
    "    \n",
    "    def update_pos(self, \n",
    "                   change_direction, # If True, the agent changes direction by a random angle\n",
    "                   reset # If True, the agent is reset to the origin\n",
    "                   ):        \n",
    "        \"\"\"\n",
    "        Updates position of the agent depending on its decision\n",
    "        \"\"\"\n",
    "        \n",
    "        if reset:\n",
    "            self.init_env()\n",
    "            return 0\n",
    "\n",
    "        else:\n",
    "            if change_direction:\n",
    "                self.current_direction = np.random.rand()*2*np.pi\n",
    "\n",
    "            self.position[0] += self.agent_step*np.cos(self.current_direction)\n",
    "            self.position[1] += self.agent_step*np.sin(self.current_direction)\n",
    "\n",
    "            # Checking encounter\n",
    "            inside_target = np.linalg.norm(self.position-self.target_position) <= self.r\n",
    "            crossed_target = isBetween_c_Vec_numba(self.previous_pos, self.position, self.target_position, self.r)\n",
    "                        \n",
    "            if inside_target or crossed_target:\n",
    "                self.init_env()\n",
    "                return 1\n",
    "            else:                \n",
    "                self.previous_pos = self.position.copy()\n",
    "                return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "651c1c49",
   "metadata": {},
   "source": [
    "## Search loop with fixed policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1306cd1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def search_loop_turn_reset_sharp(T, reset, turn, env):\n",
    "    \"\"\"\n",
    "    Runs a search loop of T steps. There is a single counter that works as follows:\n",
    "\n",
    "    - Starts at 0\n",
    "    - For each turn or continue action gets +1\n",
    "    - If reset or reach the target is set to 0\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    rewards = 0\n",
    "    counter = 0 \n",
    "\n",
    "    env.init_env()\n",
    "    \n",
    "    for t in range(T):        \n",
    "        counter += 1\n",
    "        # Reset\n",
    "        if counter == reset:\n",
    "            rew = env.update_pos(False, # change direction\n",
    "                                 True   # reset\n",
    "                                )           \n",
    "            counter = 0\n",
    "        \n",
    "        # Turn\n",
    "        elif counter == turn:\n",
    "            rew = env.update_pos(True, # change direction\n",
    "                                 False # reset\n",
    "                                )\n",
    "            \n",
    "            \n",
    "        else:\n",
    "            rew = env.update_pos(False, # change direction\n",
    "                                 False # reset\n",
    "                                )\n",
    "        if rew == 1:\n",
    "            counter = 0\n",
    "            \n",
    "        rewards += rew\n",
    "        \n",
    "    return rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d4b2eb3",
   "metadata": {},
   "source": [
    "#| hide\n",
    "# nbdev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3655ef79",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev import nbdev_export ; nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
