{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e5fb3bd-5f21-4ab7-a168-76830ac2b9e7",
   "metadata": {},
   "source": [
    "# RL-OptS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a2a706-236f-4b46-b77c-2b9a7fa527c4",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "  <img width=\"350\" src=\"https://github.com/gorkamunoz/rl_opts/blob/master/nbs/figs/index_fig.png\">\n",
    "</p>\n",
    "<h4 align=\"center\">Reinforcement Learning of Optimal Search strategies</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e7ad7c-f68f-4881-8a57-3bf626dab4d9",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "  <a href=\"https://zenodo.org/badge/latestdoi/424986383\"><img src=\"https://zenodo.org/badge/424986383.svg\" alt=\"DOI\"></a>\n",
    "  <a href=\"https://badge.fury.io/py/rl_opts\"><img src=\"https://badge.fury.io/py/rl_opts.svg\" alt=\"PyPI version\"></a>\n",
    "  <a href=\"https://badge.fury.io/py/b\"><img src=\"https://img.shields.io/badge/python-3.9-red\" alt=\"Python version\"></a>\n",
    "</p>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "This library builds the necessary tools needed to study, replicate and develop the results of the paper:\n",
    "[\"Optimal foraging strategies can be learned and outperform Lévy walks\"](https://arxiv.org/abs/2303.06050) by *G. Muñoz-Gil, A. López-Incera, L. J. Fiderer* and *H. J. Briegel*. \n",
    "\n",
    "### Installation\n",
    "\n",
    "You can access all these tools installing the python package `rl_opts` via Pypi:\n",
    "```python\n",
    "pip install rl-opts\n",
    "```\n",
    "You can also opt for cloning the [source repository](https://github.com/gorkamunoz/rl_opts) and executing the following on the parent folder you just cloned the repo:\n",
    "```python\n",
    "pip install -e rl_opts\n",
    "```\n",
    "This will install both the library and the necessary packages. \n",
    "\n",
    "### Tutorials\n",
    "\n",
    "We have prepared a series of tutorials to guide you through the most important functionalities of the package. You can find them in the [Tutorials folder](https://github.com/gorkamunoz/rl_opts/tree/master/nbs/tutorials) of the Github repository or in the Tutorials tab of our [webpage](https://gorkamunoz.github.io/rl_opts/), with notebooks that will help you navigate the package as well as reproducing the results of our paper via minimal examples. In particular, we have three tutorials:\n",
    "\n",
    "- <a href=\"tutorials/tutorial_learning.ipynb\" style=\"text-decoration:none\">Reinforcement learning </a> : shows how to train a RL agent based on Projective Simulation agents to search targets in randomly distributed environments as the ones considered in our paper.\n",
    "- <a href=\"tutorials/tutorial_imitation.ipynb\" style=\"text-decoration:none\">Imitation learning </a> : shows how to train a RL agent to imitate the policy of an expert equipped with a pre-trained policy. The latter is based on the benchmark strategies common in the literature.\n",
    "- <a href=\"tutorials/tutorial_benchmarks.ipynb\" style=\"text-decoration:none\">Benchmarks </a> : shows how launch various benchmark strategies with which to compare the trained RL agents.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Package structure\n",
    "\n",
    "The package contains a set of modules for:\n",
    "\n",
    "- <a href=\"lib_nbs/01_rl_framework.ipynb\" style=\"text-decoration:none\">Reinforcement learning framework (`rl_opts.rl_framework`)</a> : building foraging environments as well as the RL agents moving on them.\n",
    "- <a href=\"lib_nbs/02_learning_and_benchmark.ipynb\" style=\"text-decoration:none\">Learning and benchmarking (`rl_opts.learn_and_bench`)</a> : training RL agents as well as benchmarking them w.r.t. to known foraging strategies.\n",
    "- <a href=\"lib_nbs/04_imitation_learning.ipynb\" style=\"text-decoration:none\">Imitation learning (`rl_opts.imitation`)</a>: training RL agents in imitation schemes via foraging experts.\n",
    "- <a href=\"lib_nbs/03_analytics.ipynb\" style=\"text-decoration:none\">Analytical functions (`rl_opts.analytics)`</a>: builiding analytical functions for step length distributions as well as tranforming these to foraging policies.\n",
    "- <a href=\"lib_nbs/00_utils.ipynb\" style=\"text-decoration:none\">Utils (`rl_opts.utils)`</a>: helpers used throughout the package.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Cite\n",
    "\n",
    "We kindly ask you to cite our paper if any of the previous material was useful for your work, here is the bibtex info:\n",
    "\n",
    "```latex\n",
    "@article{munoz2023optimal,\n",
    "  doi = {10.48550/ARXIV.2303.06050},  \n",
    "  url = {https://arxiv.org/abs/2303.06050},  \n",
    "  author = {Muñoz-Gil, Gorka and López-Incera, Andrea and Fiderer, Lukas J. and Briegel, Hans J.},  \n",
    "  title = {Optimal foraging strategies can be learned and outperform Lévy walks},  \n",
    "  publisher = {arXiv},  \n",
    "  archivePrefix = {arXiv},\n",
    "  eprint = {2303.06050},\n",
    "  primaryClass = {cond-mat.stat-mech},  \n",
    "  year = {2023},\n",
    "}\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "optimal_search",
   "language": "python",
   "name": "optimal_search"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
