[
  {
    "objectID": "tutorials/tutorial_imitation.html",
    "href": "tutorials/tutorial_imitation.html",
    "title": "Imitation learning",
    "section": "",
    "text": "In this brief tutorial we show how to reproduce the Figures 6 and 7c of the Appendix of our paper. The goal is to show that projective simulation (PS) can imitate the policy of an agent moving following a certain step length distribution. We focus on two distributions: Lévy and Bi-exponential.\nFirst, let’s load the needed libraries and functions. See that rl_opts needs to be already installed (see instructions in the README file)."
  },
  {
    "objectID": "tutorials/tutorial_imitation.html#lévy-distributions-fig.-6a",
    "href": "tutorials/tutorial_imitation.html#lévy-distributions-fig.-6a",
    "title": "Imitation learning",
    "section": "Lévy distributions (Fig. 6a)",
    "text": "Lévy distributions (Fig. 6a)\nWe consider distribution of the type \\(P(L)=L^{-1-\\beta}\\), with various \\(\\beta\\). To understand how the following code works, you can check the example shown in the documentation of PS_imitation. Here we just do the same but looping over various \\(\\beta\\).\n\nNUM_STATES = 100 # size of the state space\nEPOCHS = 1000 # number of epochs\nNUM_STEPS = 10000 # number of learning steps per episode\n\n\nbetas = [0.5, 1, 1.5, 2]\nhmatrix_pw = np.zeros((len(betas), NUM_STATES))\n\nfor idxb, beta in enumerate(tqdm(betas)):\n    \n    # For every beta, we sample steps from the corresponding powerlaw (Levy dist.)    \n    steps = pdf_discrete_sample(beta = beta,\n                                pdf_func = pdf_powerlaw,                                \n                                L = np.arange(1, NUM_STATES),  \n                                num_samples = (EPOCHS, NUM_STEPS))\n    \n    # We define the imitator and train it\n    imitator = PS_imitation(num_states = NUM_STATES,\n                            eta = int(1e-7),\n                            gamma = 0)\n    for e in (range(EPOCHS)):\n        imitator.reset()\n        for s in steps[e]:    \n            imitator.update(length = s)\n            \n    # We only save the turn probability\n    hmatrix_pw[idxb] = imitator.h_matrix[1]/imitator.h_matrix.sum(0)\n\n  0%|          | 0/4 [01:50<?, ?it/s]\n\n\nKeyboardInterrupt: \n\n\nNow hmatrix_pw contains the turn probability of an imitator agent for every \\(\\beta\\). We can now plot this and compare it with the theoretical prediction, calculated by means of the get_policy_from_dist function.\n\nfig, ax_pw = plt.subplots(figsize = (3,3))\ncolor = plt.cm.plasma(np.linspace(0,1,len(betas)+1))\n\nfor idx, (h, beta) in enumerate(zip(hmatrix_pw, betas)):\n    \n    #---- Analytical solution ----#\n    theory = get_policy_from_dist(n_max = NUM_STATES, \n                                  func = pdf_powerlaw,\n                                  beta = beta)\n    ax_pw.plot(np.arange(2, NUM_STATES+1), 1-np.array(theory[1:]), c = color[idx])\n    \n    #---- Numerical solution ----#\n    ax_pw.plot(np.arange(2, NUM_STATES+2), h, 'o', \n               c = color[idx],  label = fr'$\\beta$ = {beta}', alpha = 0.8, markeredgecolor='None', lw = 0.05)\n\n#---- Plot features ----#\nplt.setp(ax_pw, xlim = (1.8, 30), ylim = (0.0, 1.01),\n         xlabel =r'$n$', ylabel = r'$\\pi_s(\\Rsh|n)$', \n         yticks = np.round(np.arange(0.2, 1.01, 0.2),1),\n         yticklabels = np.round(np.arange(0.2, 1.01, 0.2),1).astype(str),\n         xscale = 'log')\nax_pw.plot(10, 10, label = 'Theory', c = 'k')   \nax_pw.legend()\n\n<matplotlib.legend.Legend>\n\n\n\n\n\n\nfig, ax_pw = plt.subplots(figsize = (3,3))\ncolor = plt.cm.plasma(np.linspace(0,1,len(betas)+1))\n\nfor idx, (h, beta) in enumerate(zip(hmatrix_pw, betas)):\n    \n    #---- Analytical solution ----#\n    theory = get_policy_from_dist(n_max = NUM_STATES, \n                                  func = pdf_powerlaw,\n                                  beta = beta)\n    ax_pw.plot(np.arange(2, NUM_STATES+1), 1-theory[1:], c = color[idx])\n    \n    #---- Numerical solution ----#\n    ax_pw.plot(np.arange(2, NUM_STATES+2), h, 'o', \n               c = color[idx],  label = fr'$\\beta$ = {beta}', alpha = 0.8, markeredgecolor='None', lw = 0.05)\n\n#---- Plot features ----#\nplt.setp(ax_pw, xlim = (1.8, 30), ylim = (0.0, 1.01),\n         xlabel =r'$n$', ylabel = r'$\\pi_s(\\Rsh|n)$', \n         yticks = np.round(np.arange(0.2, 1.01, 0.2),1),\n         yticklabels = np.round(np.arange(0.2, 1.01, 0.2),1).astype(str),\n         xscale = 'log')\nax_pw.plot(10, 10, label = 'Theory', c = 'k')   \nax_pw.legend()\n\n<matplotlib.legend.Legend>"
  },
  {
    "objectID": "tutorials/tutorial_imitation.html#bi-exponentials-fig.-6b",
    "href": "tutorials/tutorial_imitation.html#bi-exponentials-fig.-6b",
    "title": "Imitation learning",
    "section": "Bi-exponentials (Fig. 6b)",
    "text": "Bi-exponentials (Fig. 6b)\nWe consider here distributions of the form \\[\n\\Pr(L) = \\sum_{i=1,2} \\omega_i (1-e^{-1/\\lambda_i}) e^{-(L-1)/\\lambda_i} \\, ,\n\\] with \\(\\omega = [0.94, 0.06]\\), \\(\\lambda_2 = 5000\\) and varying \\(\\lambda_1\\).\n\nNUM_STATES = 100 # size of the state space\nEPOCHS = 100 # number of epochs\nNUM_STEPS = 1000 # number of learning steps per episode\n\n# Bi-exponential parameters (lambda_1 will vary)\nomegas = np.array([0.94, 0.06])\nlambdas = np.array([0, 5000]).astype(float)\nlambdas_1 = [0.6, 0.6*2, 0.6*8, 0.6*16]\n\n# Array saving the results\nhmatrix_bi = np.zeros((len(lambdas_1), NUM_STATES))\n\nfor idx_l, lambda_1 in enumerate(tqdm(lambdas_1)):\n    \n    lambdas[0] = lambda_1\n    steps = pdf_discrete_sample(pdf_func = pdf_multimode,\n                                lambdas = lambdas,\n                                probs = omegas,\n                                L = np.arange(1, NUM_STATES),  \n                                num_samples = (EPOCHS, NUM_STEPS))\n\n    imitator = PS_imitation(num_states = NUM_STATES,\n                            eta = int(1e-7),\n                            gamma = 0)\n\n\n    for e in (range(EPOCHS)):\n        imitator.reset()\n        for s in steps[e]:    \n            imitator.update(length = s)\n    # We only save the turn probability\n    hmatrix_bi[idx_l] = imitator.h_matrix[1]/imitator.h_matrix.sum(0)\n\n\n  0%|          | 0/4 [00:00<?, ?it/s]\n 25%|██▌       | 1/4 [00:01<00:03,  1.14s/it]\n 50%|█████     | 2/4 [00:02<00:02,  1.16s/it]\n 75%|███████▌  | 3/4 [00:03<00:01,  1.17s/it]\n100%|██████████| 4/4 [00:04<00:00,  1.17s/it]\n\n\n\nfig, ax_bi = plt.subplots(figsize = (4,4))\ncolor = plt.cm.inferno(np.linspace(0,1,len(lambdas_1)+1))\n\n############# Powerlaw #################\nfor idx, (lambda_1, h) in enumerate(zip(lambdas_1, hmatrix_bi)):  \n    \n    #---- Analytical solution ----#\n    lambdas[0] = lambda_1     \n    theory = get_policy_from_dist(n_max = NUM_STATES, \n                        func = pdf_multimode,\n                        lambdas = lambdas,\n                        probs = omegas,)\n    ax_bi.plot(np.arange(1, NUM_STATES), 1-np.array(theory[1:]), c = color[idx])\n    \n    #---- Numerical solution ----#\n    ax_bi.plot(np.arange(2, NUM_STATES+2), h, 'o',               \n               c = color[idx], \n               label = fr'$d_1$ = {np.round(lambda_1,1)}',\n               alpha = 0.8, markeredgecolor='None')\n\n\n#---- Plot features ----#\nplt.setp(ax_bi, xlim = (1.8, 30), ylim = (0.0, 1.01),\n         xlabel =r'$n$', ylabel = r'$\\pi_s(\\Rsh|n)$', \n         yticks = np.round(np.arange(0.2, 1.01, 0.2),1),\n         yticklabels = np.round(np.arange(0.2, 1.01, 0.2),1).astype(str),\n         xscale = 'log')\nax_bi.plot(10, 10, label = 'Theory', c = 'k')   \nax_bi.legend()\n\n<matplotlib.legend.Legend>\n\n\n\n\n\n\nfig, ax_bi = plt.subplots(figsize = (4,4))\ncolor = plt.cm.inferno(np.linspace(0,1,len(lambdas_1)+1))\n\n############# Powerlaw #################\nfor idx, (lambda_1, h) in enumerate(zip(lambdas_1, hmatrix_bi)):  \n    \n    #---- Analytical solution ----#\n    lambdas[0] = lambda_1     \n    theory = get_policy_from_dist(n_max = NUM_STATES, \n                        func = pdf_multimode,\n                        lambdas = lambdas,\n                        probs = omegas,)\n    ax_bi.plot(np.arange(2, NUM_STATES+1), 1-theory[1:], c = color[idx])\n    \n    #---- Numerical solution ----#\n    ax_bi.plot(np.arange(2, NUM_STATES+2), h, 'o',               \n               c = color[idx], \n               label = fr'$d_1$ = {np.round(lambda_1,1)}',\n               alpha = 0.8, markeredgecolor='None')\n\n\n#---- Plot features ----#\nplt.setp(ax_bi, xlim = (1.8, 30), ylim = (0.0, 1.01),\n         xlabel =r'$n$', ylabel = r'$\\pi_s(\\Rsh|n)$', \n         yticks = np.round(np.arange(0.2, 1.01, 0.2),1),\n         yticklabels = np.round(np.arange(0.2, 1.01, 0.2),1).astype(str),\n         xscale = 'log')\nax_bi.plot(10, 10, label = 'Theory', c = 'k')   \nax_bi.legend()\n\n<matplotlib.legend.Legend>"
  },
  {
    "objectID": "tutorials/tutorial_imitation.html#effect-of-a-cutoff-l_max-fig.-7c",
    "href": "tutorials/tutorial_imitation.html#effect-of-a-cutoff-l_max-fig.-7c",
    "title": "Imitation learning",
    "section": "Effect of a cutoff \\(L_{max}\\) (Fig. 7c)",
    "text": "Effect of a cutoff \\(L_{max}\\) (Fig. 7c)\nIn our paper we explain that introducing a cutoff in the distribution used by the expert in the imitation scheme affects the resulting policy of the agent. Here we show this effect, using one of the previous bi-exponential policies as example:\n\n# Simulation parameters\nNUM_STATES = 1000 # size of the state space\nEPOCHS = 100 # number of epochs\nNUM_STEPS = 1000 # number of learning steps per episode\n\n# Distribution paremeters\nomegas = np.array([0.94, 0.06])\nlambdas = np.array([0.6, 5000])\n# Get theoretical policy (without cutoff)\ntheory_nocutoff =  get_policy_from_dist(n_max = NUM_STATES, \n                              func = pdf_multimode,\n                              lambdas = lambdas,\n                              probs = omegas,)\n\n\n# Setting a max step length\nL_cutoffs = [30, 150, 500, 2000, 10000]\n# To make the loop more efficient, we sample now all steps,\n# which we will then cut if the are bigger than L_cutoff\nsteps_og = pdf_discrete_sample(pdf_func = pdf_multimode,\n                                lambdas = lambdas,\n                                probs = omegas,\n                                L_max = NUM_STATES,  \n                                num_samples = EPOCHS*NUM_STEPS)\n\n\nhmatrix_co = np.zeros((len(L_cutoffs), NUM_STATES))\nfor idx_c, L_cutoff in enumerate(tqdm(L_cutoffs)):\n\n    # Copy steps we sampled above and apply cutoff. \n    # We re-generate the cutted steps\n    steps = steps_og.copy()\n    while np.max(steps) > L_cutoff:\n        steps[steps > L_cutoff] = pdf_discrete_sample(pdf_func = pdf_multimode,\n                                                      lambdas = lambdas,\n                                                      probs = omegas,\n                                                      L_max = NUM_STATES,\n                                                      num_samples = len(steps[steps > L_cutoff]))\n    steps = steps.reshape(EPOCHS, NUM_STEPS)\n    \n    # Define PS imitator\n    imitator = PS_imitation(num_states = NUM_STATES,\n                            eta = int(1e-7),\n                            gamma = 0)\n    # Training\n    for e in (range(EPOCHS)):\n        imitator.reset()\n        for s in steps[e]:         \n            imitator.update(length = s)\n    # Saving\n    hmatrix_co[idx_c] = imitator.h_matrix[1]/imitator.h_matrix.sum(0)\n\n\n\n\n\nfig, ax_co = plt.subplots(figsize = (6, 3))\ncolor = plt.cm.cividis(np.linspace(0,1,len(L_cutoffs)+1))\n\nfor idx, (h, L_cutoff) in enumerate(zip(hmatrix_co, L_cutoffs)):  \n    #---- Numerical solutions ----#\n    ax_co.plot(np.arange(2, NUM_STATES+2), h, 'o',               \n               c = color[idx],  label = r'$L_{max}$ = '+f'{L_cutoff}', \n               alpha = 0.8, markeredgecolor='None', rasterized=True)\n    ax_co.plot(np.arange(2, NUM_STATES+2), h,                \n               c = color[idx],  alpha = 0.2)\n\n#---- Analytical solutions ----#\nax_co.plot(np.arange(2, NUM_STATES+1), 1-theory_nocutoff[1:], '-', \n           c = 'k', alpha = 0.8, label = r'$L_{max}\\rightarrow \\infty$') \n\n#---- Plot features ----#\nax_co.axhline(0.5, c = 'k', ls = '--', alpha = 0.5, zorder = -1)\nax_co.text(1.5, 0.52, r'$\\pi_0$', alpha = 0.5)\nplt.legend(loc = 'upper right', fontsize = 8)    \nplt.setp(ax_co, xlabel =r'$n$', ylabel = r'$\\pi(\\Rsh|n)$', xscale = 'log')\n\n[Text(0.5, 0, '$n$'), Text(0, 0.5, '$\\\\pi(\\\\Rsh|n)$'), None]"
  },
  {
    "objectID": "tutorials/tutorial_learning.html",
    "href": "tutorials/tutorial_learning.html",
    "title": "Reinforcement Learning",
    "section": "",
    "text": "In order to train an RL agent, we need to have (i) an environment and (ii) a learning method. In this work, we define a foraging environment where the goal of the agent is to find as many targets as possible in a given time. We consider environments with non-destructive -or replenishable- targets, which we implement by displacing the agent a distance \\(l_\\textrm{c}\\) from the center of the found target.\nAs for the agent, we use Projective Simulation (PS) to model its decision making process and learning method. However, other algorithms that work with stochastic policies can also be used.\nFirst, we import the classes that define the environment (TargetEnv), the forager dynamics (Forager), and its learning method.\nNote: the class Forager as it currently is inherits the methods of a PS agent for decision making and learning. However, other learning algorithms can be directly implemented by changing this inheritance. The learning algorithm should contain a method for decision making, called deliberate, which inputs a state; and another one for updating the policy, called learn, which inputs a reward.\nWe set up the parameters defining the length of the episodes (number of RL steps) and the number of episodes.\nWe initialize the environment.\nWe initialize the agent. As states, the agent perceives the value of an internal counter that keeps track of the number of small steps that it has performed without turning. The possible actions are continue walking in the same direction or turning. The agent performs a small step of length \\(d=1\\) in any case after making a decision. Let’s define the parameters of the PS forager agent and initialize it:\nWe run the learning process.\nNote: the code can directly accomodate environments with several agents that interact. For this reason, you will find methods in both the environment class TargetEnv and the forager class Forager that deal with agents that have visual cones and can perceive the presence of other agents in their surroundings. However, these features are not used in this work."
  },
  {
    "objectID": "tutorials/tutorial_learning.html#reproduction-of-results",
    "href": "tutorials/tutorial_learning.html#reproduction-of-results",
    "title": "Reinforcement Learning",
    "section": "Reproduction of results",
    "text": "Reproduction of results\nHere, we explain how to reproduce the results of the paper that concern the training of RL agents in the foraging environment.\n\nTraining\nYou can run the training that is detailed above by means of the method [`learning`](https://gorkamunoz.github.io/rl_opts/lib_nbs/learning_and_benchmark.html#learning), which also saves the agent’s memory periodically.\nFirst, import [`learning`](https://gorkamunoz.github.io/rl_opts/lib_nbs/learning_and_benchmark.html#learning):\n\nfrom rl_opts.learn_and_bench import learning\n\nlearning inputs a configuration dictionary (config), a path to the folder where the results are saved (results_path) and the agent’s identifier (run). Let us detail each input separately.\n\nConfiguration dictionary (config): it contains the parameters to initialize both the environment and the agent. For each set of parameters we ran, there is an identifier of the form “exp_numconfig” (e.g. exp_0) that uniquely identifies the config file. The config files for the experiments that give the results of the paper can be found in the directory ‘configurations/learning/’.\n\nThese are the parameters that you can find in the config files:\nNUM_TARGETS : number of targets\nWORLD_SIZE : side of the square that defines the world (with periodic boundary conditions)\nr : target detection radius\nlc : cutoff length\nMAX_STEP_L : maximum value of the step counter (which coincides with the number of RL steps per episode)\nNUM_BINS : number of bins in which the state space is split. This is set to have one state per value of the counter\nNUM_ACTIONS : number of actions\nGAMMA : forgetting parameter \\(\\gamma\\) in PS\nETA_GLOW : glow damping parameter \\(\\eta_g\\) in PS\nPI_INIT : policy initialization \\(\\pi_0\\) (\\(\\forall n\\)). Note that it is given as \\(\\pi_0(\\uparrow|n)\\)\nNUM_EPISODES : number of episodes\nWe study foraging in enviroments with different cutoff lengths \\(l_\\textrm{c}\\). Exp_0 corresponds to \\(l_\\textrm{c}=0.6\\). Exp_1..10 correspond to \\(l_\\textrm{c}=1..10\\), respectively. In experiments exp_0..10, the initialization policy is \\(\\pi_0(\\Rsh|n)=0.01\\) \\(\\forall n\\). Exp_11 and exp_12 correspond to experiments where the initialization policy is \\(\\pi_0(\\Rsh|n)=0.5\\) \\(\\forall n\\). Each experiment is run with 10 independent, different agents (run \\(\\in [0,9]\\)).\nAs an example, you can import the configuration from experiment exp_8 by running:\n\nfrom rl_opts.utils import get_config\n\nconfig = get_config('exp_8.cfg')\n\nAlternatively, you can also define your own config dictionary with the parameters detailed above:\n\nmy_config = {'NUM_TARGETS' : 100,\n             'WORLD_SIZE' : 100,\n             'r' : 0.5,\n             'lc' : 2,\n             'MAX_STEP_L' : 100,\n             'NUM_BINS' : 100,\n             'NUM_ACTIONS' : 2,\n             'GAMMA' : 0.00001,\n             'ETA_GLOW' : 0.1,\n             'PI_INIT' : 0.99,\n             'NUM_EPISODES' : 500}\n\n\nResults path (results_path): Path where you want to save the results. The agent’s memory (h matrix) is saved every 500 episodes on the file ‘memory_agent…’ (e.g. ‘memory_agent_0_episode_500.npy’).\n\n\nresults_path = 'results/learning/test/'\n\n\nAgent’s identifier (run): integer that identifies the agent. With this identifier, you can later retrieve the agent’s memory or its performance (see the following section on Postlearning analysis).\n\nAfter defining the inputs, you can run the learning:\n\nlearning(my_config, results_path, run=0)\n\nOnce the training is finished, you can get the policy of the agent (as \\(\\pi(\\uparrow|n)\\)) at any of the episodes in which the memory was saved by running:\n\nfrom rl_opts.utils import get_policy\n\nsaved_policy = get_policy(results_path, run=0, training_episode=500)\n\nNote: in the code, the policies are always given as \\(\\pi(\\uparrow|n)\\).\nFig. 3 and Fig. 4 show the policies of the agents at the end of a training consisting of 12000 episodes of 20000 RL steps each. The policies can be retrieved with [`get_policy`](https://gorkamunoz.github.io/rl_opts/lib_nbs/utils.html#get_policy) as detailed above, by setting training_episode = 12000 and the corresponding agent identifier.\n\n\nPostlearning analysis\nIn order to fairly compare the performance of the RL agents throughout the training with that of the benchmark models (Fig. 2), we need to run the same number of walks. In the training, the agent’s policy changes from one episode to the next one, and taking the efficiency of just one episode -i.e. one walk- is not enough since we consider \\(10^4\\) walks for the benchmark policies. Thus, we save the agent’s policy at different stages of the training and then, in a postlearning analysis, we run \\(10^4\\) walks with that frozen policy to get a more accurate evaluation of its performance.\nThis performance analysis is done with the method [`agent_efficiency`](https://gorkamunoz.github.io/rl_opts/lib_nbs/learning_and_benchmark.html#agent_efficiency), which is imported by running:\n\nfrom rl_opts.learn_and_bench import agent_efficiency\n\nTo run it, you first need to define:\n\nThe results path from where it retrieves the agent’s memory at different stages of the training. Thus, it needs to be the same path where you saved the results of the training. The results of this analysis are also saved there.\nThe configuration file you used to train the agent. To reproduce the results from Fig. 2, first get the corresponding config file as detailed in the previous section.\nThe agent’s identifier.\nThe number of walks. To reproduce the results from Fig. 2, set this parameter to 10000.\nAn episode interval. This function analyzes the performance of the agent at different stages of the training. To reproduce our results from Fig. 2, you should set this parameter to 2000, which means the performance is analyzed every 2000 episodes, until the end of the training.\n\nTo do the postlearning analysis on the example of the previous section, you run:\n\nagent_efficiency(results_path, my_config, run=0, num_walks=100, episode_interval=500)\n\nEssentially, this analysis is carried out by the method walk_from_policy, which inputs a policy (that is not changing) and runs the walks in parallel. It outputs a list with the efficiency achieved in each walk.\nYou can find the results of, for example, the last episode, in the file ‘performance_post_training_agent_0_episode_500.npy’.\nTo get an array with the average performances (over the number of walks) of several agents throughout the training, you can run:\n\nfrom rl_opts.utils import get_performance\n\nag_list = [0] #in this example, we only ran one agent, but you can input here the identifiers of all the agents you ran.\nep_list = [500] #get the performance at episode 500 of the agents in ag_list.\n\nav_performance, sem = get_performance(results_path, agent_list=ag_list, episode_list=ep_list)"
  },
  {
    "objectID": "tutorials/tutorial_benchmarks.html",
    "href": "tutorials/tutorial_benchmarks.html",
    "title": "Benchmarks",
    "section": "",
    "text": "We consider as benchmark models the discrete Lévy distribution and the bi-exponential distribution, given by equations\n\\[\\Pr(L)=\\zeta^{-1}_{(1+\\beta, 1)} L^{-1-\\beta}\\,,\\] and \\[\\Pr(L) = \\sum_{i=1,2} \\omega_i (1-e^{-1/d_i}) e^{-(L-1)/d_i} \\, ,\\] respectively, where \\(\\zeta_{(1+\\beta, 1)}=\\sum_{\\ell=0}^\\infty (\\ell+1)^{-1-\\beta}\\) is the Riemann zeta function, \\(d_i\\) are length scales and the mode weights satisfy \\(\\sum_{i=1,2} \\omega_i=1\\).\nWe transform the step length distributions into policies with Eq. (5), which is implemented in the code with the method policy_from_dist. This method inputs (i) the maximum value of step counter for which to compute the policy, (ii) the function of the model (either ‘pdf_powerlaw’ or ‘pdf_multimode’ in our case); and (iii) the parameter/s of the chosen model, which are, in our case, the exponent \\(\\beta\\) for the Lévy distribution and \\(d_1\\), \\(d_2\\), \\(\\omega_1\\) for the bi-exponential.\nThe step length distributions described above and the method to transform them into policies (policy_from_dist) can be imported as follows:\n\nfrom rl_opts.analytics import get_policy_from_dist, pdf_powerlaw, pdf_multimode\n\nYou can get the policy of, e.g., a discrete Lévy distribution with \\(\\beta=1\\) by running:\n\npolicy_levy = get_policy_from_dist(n_max = 100, \n                              func = pdf_powerlaw,\n                              beta = 1)\n\nNote: the list policy_levy displays the first n_max points of the policy, given as \\(\\pi(\\uparrow|n)\\).\nWe employ the library Tune for parameter optimization, which allows us to optimize the average search efficiency over a number a walks (mean_eff) with respect to the model parameters. The function to optimize (mean_eff) is computed with the method [average_search_efficiency](https://gorkamunoz.github.io/rl_opts/lib_nbs/learning_and_benchmark.html#average_search_efficiency), and then reported to tune.\n\nfrom rl_opts.learn_and_bench import average_search_efficiency\n\nNote: The walks are performed with [`walk_from_policy`](https://gorkamunoz.github.io/rl_opts/lib_nbs/learning_and_benchmark.html#walk_from_policy) (see also tutorial on learning), which inputs a (non-changing) policy and runs the walks in parallel. In this case, the policy is the one corresponding to the benchmark distribution that is being evaluated.\naverage_search_efficiency inputs a configuration dictionary with the parameter ranges that the optimization algorithm will consider.\nThe parameters of that dictionary are described below:\nModel (we input parameter ranges here):\n\nd_int : small scale (\\(d_1\\), first mode) of the bi-exponential distribution\n\nd_ext : large scale (\\(d_2\\), second mode) of the bi-exponential distribution\n\np : weight of the first mode (\\(\\omega_1\\)) in the bi-exponential distribution\n\nbeta : exponent of the Lévy distribution\n\nmodel : model description (fixed, either ‘powerlaw’ or ‘double_exp’)\n\nWalks (we input a single value that is fixed throughout the optimization):\n\ntime_ep : number of (small, \\(d=1\\)) steps per walk. We choose the same value for the benchmarks as for the episodes in the RL training\n\nn : number of walks (also referred to as agents in the code, but there is no relation to RL agents)\n\nEnvironment (we input a single value that is fixed throughout the optimization):\n\nlc : cutoff length\n\nNt : number of targets\n\nL : world size\n\nr : target detection radius\n\ndestructive : whether targets are destructive or not (always set to False)\n\nOther:\n\nresults_path : Path where the resulting efficiencies for each walk are saved. If you set it None, the efficiencies are not saved. The mean efficiency can still be retrieved from the final Tune dataframe.\nnum_raytune_samples : Number of samples for tune (needed for Bayesian Optimization).\n\nOnce we define the parameter ranges, we choose the optimization algorithm. Among the different possibilities that Tune offers, we chose Grid Search for the Lévy distribution and Bayesian Optimization for the bi-exponential distribution.\n\nExample\nLet us take the example with \\(l_\\textrm{c}=3\\).\nFor the Lévy distribution, the config dictionary looks like:\n\nconfig_lv = {'d_int': None,\n          'd_ext': None,\n          'p': None,\n          'beta': tune.grid_search(np.linspace(0.01,1.,20)), \n          'model': 'powerlaw',\n          'time_ep': 100,\n          'n': 100,\n          'lc': 3.0,\n          'Nt': 100,\n          'L': 100,\n          'r': 0.5,\n          'destructive': False,\n          'results_path': None,\n          'num_raytune_samples': 10\n          }\n\nWe do a grid search over 20 parameters, linearly spaced in the interval \\([0.01, 1]\\). Parameters that correspond to the other model are set to ‘None’.\nThen, we initialize the tuner, which by default does a grid search over the input parameters.\n\nfrom ray import tune\n\ntuner = tune.Tuner(average_search_efficiency,\n                   tune_config=tune.TuneConfig(num_samples=1),\n                   param_space=config_lv)\n\nAnd we run the algorithm:\n\nresult_grid_lv = tuner.fit()\n\nFor the bi-exponential distribution, the config dictionary looks like:\n\nconfig_be = {'d_int': tune.uniform(0.00001, 20.0),\n          'd_ext': 100.0,\n          'p': tune.uniform(0.0, 1.0),\n          'beta': None,\n          'model': 'double_exp',\n          'time_ep': 100,\n          'n': 100,\n          'lc': 3.0,\n          'Nt': 100,\n          'L': 100,\n          'r': 0.5,\n          'destructive': False,\n          'results_path': None,\n          'num_raytune_samples': 10\n          }\n\nIn this case, since we choose a Bayesian optimization method, we do not specify the parameters to try, but just the ranges. For the small scale, we consider a range that is of the order of the scale of \\(l_\\textrm{c}\\). We fix the value for \\(d_2\\) to further guide the search and make it more time efficient. We do the search with \\(d_2=100\\), which is the scale of the average distance between targets, and with \\(d_2=10^5\\). Again, the parameter \\(\\beta\\) that corresponds to the other model is set to ‘None’.\nWe first initialize the Bayesian optimization method, and then the tuner.\n\nfrom ray import tune\nfrom ray.tune.search.bayesopt import BayesOptSearch\nfrom ray.tune.search import ConcurrencyLimiter\n\nbayesopt = BayesOptSearch(metric=\"mean_eff\", mode=\"max\")\nbayesopt = ConcurrencyLimiter(bayesopt, max_concurrent=3)\ntuner = tune.Tuner(average_search_efficiency, \n                   tune_config=tune.TuneConfig(search_alg=bayesopt, num_samples=config['num_raytune_samples']), \n                   param_space=config_be)\n\nNote that we limit the number of concurrent processes to 3, so that the method can update itself more times within the num_raytune_samples samples.\nAnd we run it:\n\nresult_grid_be = tuner.fit()\n\n\n\nResults\nThe results can be retrieved as a panda dataframe:\n\nresults_lv_df = result_grid_lv.get_dataframe()\n\nThese results can also be saved as a panda dataframe in the folder indicated in the config dictionary. We refer the reader to the Tune documentation for further details on data saving and retrieval.\n\n\nReproduction of results\nIn order to reproduce the results of the paper, you can access the configuration dictionaries in the folder ‘configurations/benchmark_models/’. For example:\n\nconfig_path = 'configurations/benchmark_models/'\nmodel = 'powerlaw'\nlc = 3\nrun = 0\nconfig_paper = np.load(config_path+'config_'+str(model)+'_lc_'+str(float(lc))+'_run_'+str(run)+'.npy', allow_pickle=True).item()\n\nNote that you need to provide a run number. The reason for this is that, in some cases, we run the optimization several times for the same models. For example, for the bi-exponential distribution, we run it twice, first with \\(d_2 = 10^5\\) (run_0) and then with \\(d_2 = 100\\) (run_1). For the Lévy distribution, there is only run_0.\nThe mean efficiency achieved by the best model, together with the model parameters, can be retrieved from the resulting dataframe (see above). In addition, if you want the list with the efficiency of each walk, you can obtain it with get_opt (provided a results path was input in the configuration dictionary):\n\nfrom rl_opts.utils import get_opt\n\nefficiency, parameters = get_opt(config_lv['results_path'], results_lv_df)\n\nNote that this method inputs the panda dataframe with the obtained results. As additional output, it provides the parameters of the model that achieved the highest efficiency. For the Lévy distribution, the exponent \\(\\beta\\) is given. For the bi-exponential, it outputs a list of the form \\([d_1, d_2, \\omega_1]\\).\nAs default value, the saved config dictionaries have the results path set to None (in which case get_opt outputs the mean efficiency retrieved from the given dataframe), so if you want to obtain the efficiency list, change it and add a path of your choice."
  },
  {
    "objectID": "tutorials/tutorial_reset.html",
    "href": "tutorials/tutorial_reset.html",
    "title": "Learning to reset in target search problems",
    "section": "",
    "text": "In this notebook, we exemplify how to train agents in a target search problems, similar to the foraging problems we deal with in the rest of this library. In this case we consider a single target.\nThe agents we consider here have an important addition: they are able to reset to the origin. As we show in our paper (link to be update), this helps agents reach much better efficiencies compared to a non-resetting strategy.\nLet’s start this tutorial by importing some necesarry libraries:"
  },
  {
    "objectID": "tutorials/tutorial_reset.html#sharp-and-exponential",
    "href": "tutorials/tutorial_reset.html#sharp-and-exponential",
    "title": "Learning to reset in target search problems",
    "section": "Sharp and exponential",
    "text": "Sharp and exponential\nWe start by looking at the sharp and exponential resetting strategies. We start by defining the parameters for each strategy:\n\n# Reset rate for exponential\nrates = np.logspace(-2.5,-1.25,100 )\n\n# Reset time for sharp\nresets = np.linspace(5, 250, 100).astype(np.int64)\n\nNow we perform a simulation of the target search problem for each strategy and parameter. The library contains functions that launch a parallel simulation over the number of resets / rates, based on the number of cores you have:\n\nfrom rl_opts.rl_framework.numba.environments import parallel_Reset1D_exp, parallel_Reset1D_sharp\n\nFeel free to increase reps to get better results (for the paper we used reps = 1e5).\n\nreps = int(1e3)\ntime_ep = int(1e4)\nrews_exp = np.zeros((reps, len(Ls), len(rates)))\nrews_sharp = np.zeros((reps, len(Ls), len(resets)))\n\nfor idxL, L in enumerate(tqdm(Ls)):\n\n    for idxr in range(reps):\n    \n        rews_exp[idxr, idxL] = parallel_Reset1D_exp(time_ep, rates, L, D)\n        \n        rews_sharp[idxr, idxL] = parallel_Reset1D_sharp(time_ep, resets, L, D)\n\nWe can now plot the efficiencies of each resetting time for the sharp distribution:\n\n# create a Blues color map so that I can plot each line with a different blue tone\ncmap = plt.get_cmap('Blues')\ncolors = cmap(np.linspace(0.3, 1, len(Ls)))\n\nfig, ax = plt.subplots()\n\nfor i in range(len(Ls)):\n    ax.plot(resets, rews_sharp.mean(0)[i], c = colors[i])\n\n# Add a colorbar\nsm = plt.cm.ScalarMappable(cmap=cmap, norm=plt.Normalize(vmin=Ls.min(), vmax=Ls.max()))\nsm._A = []\ncbar = plt.colorbar(sm, ax=ax)\ncbar.set_label('Target distance')\n\nax.set_xlabel('Reset time')\nax.set_ylabel('Average reward')\nax.set_title('Sharp reset')\n\nText(0.5, 1.0, 'Sharp reset')\n\n\n\n\n\nand for each rate of the exponential distribution:\n\ncmap = plt.get_cmap('Reds')\ncolors = cmap(np.linspace(0.3, 1, len(Ls)))\n\nfig, ax = plt.subplots()\n\nfor i in range(len(Ls)):\n    ax.plot(np.log10(rates), rews_exp.mean(0)[i], c = colors[i])\n\n# Add a colorbar\nsm = plt.cm.ScalarMappable(cmap=cmap, norm=plt.Normalize(vmin=Ls.min(), vmax=Ls.max()))\nsm._A = []\ncbar = plt.colorbar(sm, ax=ax)\ncbar.set_label('Target distance')\n\nax.set_xlabel('Reset time')\nax.set_ylabel('Average reward')\nax.set_title('Exponential reset')\n\nText(0.5, 1.0, 'Exponential reset')"
  },
  {
    "objectID": "tutorials/tutorial_reset.html#learning",
    "href": "tutorials/tutorial_reset.html#learning",
    "title": "Learning to reset in target search problems",
    "section": "Learning",
    "text": "Learning\n\nDidactic implementation: training a single agent\nHere we will focus on implementing all steps of a training loop in a didactic way.\nWe start by defining the RL agent. We use the Forager provided by the library:\n\nfrom rl_opts.rl_framework.numba.agents import Forager\n\n\n# Number of actions of the agent. here we consider 2 actions: diffuse or reset.\nnum_actions = 2\n\n# Size of the state space: in the current context, means the maximum counter the agent can reach before compulsory turning\nsize_state_space = np.array([int(2e3)])\n\n# For the PS parameter, we choose the best parameters we chose from a grid search (see below how to do it)\ngamma_damping = 3e-6\neta_glow_damping = 0.12\n\n\n# We now define the agent (may take time in the first run due to numba compilation\nReset_agent = Forager(num_actions, \n                      size_state_space,\n                      gamma_damping, eta_glow_damping)\n\nNext we define the Reset environment:\n\nfrom rl_opts.rl_framework.numba.environments import ResetEnv_1D\n\n\nReset_env = ResetEnv_1D(L = Ls[0], # Distance of the target to the origin. We consider for this example the first distance from above.\n                  D = D # Diffusion coefficient of the agent\n                 )\n\nFinally, we perform the RL training loop:\n\n# Number of episodes\nepisodes = 300 \n# Number of steps per episode (we use same as above for the sharp and exponential)\ntime_ep = int(1e4)\n\n# To keep track of the obtained rewards\nsave_rewards = np.zeros(episodes)\n\nfor ep in tqdm(range(episodes)):\n        \n        # Initialize the environment and agent's counter and g-matrix\n        Reset_env.init_env()\n        Reset_agent.agent_state = 0\n        Reset_agent.reset_g()\n\n        for t in range(time_ep):\n\n            # The efficient agent we created needs to keep track when the last\n            # update to the h_matrix happened. \n            Reset_agent.N_upd_H += 1\n            Reset_agent.N_upd_G += 1\n\n            # Get the agent's state (i.e. its current counter)\n            # This differs a bit from typical RL scenarios in which the state comes from the environment,\n            # but changes nothing in terms of the whole RL loop.\n            state = Reset_agent.get_state()\n            \n            # If we reached the maximum state space, we perform turn action\n            if state == Reset_agent.h_matrix.shape[-1]:\n                action = 1\n            # Else we sample an action from the agent's policy\n            else: \n                action = Reset_agent.deliberate(state)\n                \n            # Now we implement this action into the state, which here means updating the agent's counter.\n            # Again, this is slightly different from typical RL where the action is implemented in the environment,\n            # but actually changes nothing to a normal RL implementation (see paper for details)\n            Reset_agent.act(action)\n\n            # We now update the position of the agent in the environment. This also checks if the agnet reached a target\n            # and return a reward = 1 if so.\n            reward = Reset_env.update_pos(action)            \n\n            # If we got a reward or reached the maximum no update value for the H_matrix (for efficient agents), we update \n            # the h_matrix\n            if reward == 1 or Reset_agent.N_upd_H == Reset_agent.max_no_H_update-1:\n                Reset_agent._learn_post_reward(reward)\n\n            # Now we make sure that the state of the agent (counter = 0) is updated if we got the reward\n            if reward != 0:\n                Reset_agent.agent_state = 0\n\n            # We keep track of the number of rewards obtained:\n            save_rewards[ep] += reward\n\nWe can now see how the agent learned by plotting the rewards (i.e. targets acquired) per episode. We will also compare to the best sharp and exponential strategies (horizontal line):\n\nplt.plot(save_rewards, label = 'Efficient agent')\n\n\nplt.axhline(rews_sharp.mean(0)[0].max(), color = 'red', linestyle = '--', label = 'Sharp reset')\nplt.axhline(rews_exp.mean(0)[0].max(), color = 'green', linestyle = '--', label = 'Exponential reset')\n\n\nplt.legend()\n\nplt.xlabel('Episodes')\nplt.ylabel('Rewards')\n\nText(0, 0.5, 'Rewards')\n\n\n\n\n\nWe can see that with only few episodes, the agent is already able to go from a very bad initial strategy to something that outperforms the exponential strategy and closely matches the efficiency of the sharp resetting!\n\nAgent’s strategy\nIn our paper we showed how the strategy learned by the agent converges to the sharp reset strategy. To do so, we have to look at the agent’s policy, defined in this case by its h-matrix.\n\nThis figures corresponds to Figure 3a from our paper.\n\n\n# The policy is calculated by normalizing the h_matrix\npolicy_reset = Reset_agent.h_matrix[1] / Reset_agent.h_matrix.sum(0)\n\n# Let's plot the first points of the policy. Beyond this, the policy converges to the initial policy\n# at 0.5, because the agent will always reset after reaching those counter values.\nplt.plot(policy_reset[:50], label = 'Trained policy')\n\nplt.axhline(0.5, color = 'k', linestyle = '--', label = 'Initial policy', zorder = -1)\n\nplt.legend()\nplt.xlabel('Counter')\nplt.ylabel('Probability of the reset action')\n\nText(0, 0.5, 'Probability of the reset action')\n\n\n\n\n\n\n\n\nGrid Search and Multi-Agent Training\nThe rl_opts library provides functionality for parallel training of multiple agents, enabling comprehensive benchmarking of their efficiencies. This feature allows us to perform grid searches and compare the performance of different agents under various conditions.\n\nfrom rl_opts.rl_framework.numba.agents import run_agents_reset_1D\n\nWe start by defining the training specifications. For the paper, in the case of Reset 1D, we used the following:\n\n\n\n\n\n\n\nParameter\nValue\n\n\n\n\nEpisodes\n1000\n\n\nSteps per episdoe\n5000\n\n\nNumber of agents\n190\n\n\nmultiplier_agents\n5\n\n\nD\n0.5\n\n\nDistances (as defined above)\nnp.arange(5, 15)\n\n\nMaximum counter value\n2000\n\n\n\\(\\gamma\\)\nnp.logspace(-9, -5.5, 10)\n\n\n\\(\\eta\\)\nnp.linspace(0.05, 0.3, 10)\n\n\n\n\nImportant: the next cell can take quite long to run, depending on your computational capabilities. We have set at a minimum such that you can still perform some analysis from the outputs. If you just want to explore how the function works, decrease the number of episodes, the number of steps per episode, as well as the number of gammas and etas explored.\n\n\n# Training specs. Commented are the original values used in the paper, but we will use a smaller numbers for this example\nepisodes = int(1e2) # paper: int(1e3)\ntime_ep = int(5e2) # paper: int(5e3)\n\n# Number of agents, defined here by the numbe of cores available multiplied by the number\n# of runs we want to perform (multiplier_agents)\nnum_cores = int(numba.get_num_threads()*0.8)\nmultiplier_agents = 5\n\n\n# Size of the state space: in the current context, means the maximum counter the agent can reach before compulsory turning\nsize_state_space = np.array([int(2e3)])\n\n# Projective simulation parameters\ngammas = np.logspace(-9, -5.5, 10)\netas = np.linspace(0.05, 0.3, 10)\n\n# Now we loop over the different parameters and distances\nrews = np.zeros((len(Ls), len(gammas), len(etas), num_cores*multiplier_agents, episodes))\nfor idxL, L in enumerate(tqdm(Ls)):\n    for idxg, gamma in enumerate(gammas):\n        for idxe, eta in enumerate(etas):\n\n            # Now we run the parallel training. This function spits out the rewards and the h_matrices. We will only keep the rewards\n            # here. The h_matrices were used during our analysis to create the plots of the policies (as the example above) as well\n            # as the analysis of the resetting times.\n            rews[idxL, idxg, idxe], h_matrices = run_agents_reset_1D(episodes = int(episodes), time_ep = int(time_ep), \n                                                                    N_agents = num_cores,\n                                                                    num_runs = multiplier_agents,\n                                                                    D = D, L = L,\n                                                                    size_state_space = size_state_space,\n                                                                    gamma_damping = gamma,\n                                                                    eta_glow_damping = eta, \n                                                                    )\n\n\n\n\nWe can now take a look at how the average accuracy was for the grid of parameters at each distance:\n\nfig, axs = plt.subplots(1, len(Ls), figsize=(3*len(Ls), 3))\n\nfor idxL, L in enumerate(Ls):\n    avg_efficiency = rews[idxL, :, :, :, -1].mean(axis=-1)\n    cax = axs[idxL].matshow(avg_efficiency, aspect='auto')\n    axs[idxL].set_title(f'L = {L}')\n\n    axs[idxL].set_xticks(np.arange(0, len(etas), 2))\n    axs[idxL].set_xticklabels([f'{etas[i]:.2f}' for i in range(0, len(etas), 2)], rotation=90)\n    axs[idxL].set_yticks(np.arange(0, len(gammas), 2))\n    axs[idxL].set_yticklabels([f'{gammas[i]:.1e}' for i in range(0, len(gammas), 2)])\n\n    axs[idxL].xaxis.set_ticks_position('bottom')\n\n    if idxL == 0:\n        axs[idxL].set_ylabel(r'$\\gamma$')\n    axs[idxL].set_xlabel(r'$\\eta$')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\nHere, yellow means a bigger efficiency. This plots have been created with very few epsiodes, hence their noise. Running them for 100x episodes would lead to much clearer plots. The best parameters for each \\(L\\) were the following:\n\n\n\n\\(L\\)\n\\(\\gamma\\)\n\\(\\eta\\)\n\n\n5\n3.16e-06\n0.122\n\n\n6\n1.29e-06\n0.122\n\n\n7\n5.27e-07\n0.151\n\n\n8\n5.27e-07\n0.151\n\n\n9\n3.59e-08\n0.206\n\n\n10\n1.00e-09\n0.220"
  },
  {
    "objectID": "tutorials/tutorial_reset.html#sharp-resetting",
    "href": "tutorials/tutorial_reset.html#sharp-resetting",
    "title": "Learning to reset in target search problems",
    "section": "Sharp resetting",
    "text": "Sharp resetting\nOur proposed baseline for this problem is again a sharp resetting strategy. For the turning time, we will also consider a sharp strategy. Indeed, in our work we found that the best turning point is at \\(\\sim L + 1\\) (see paper for details).\nIf you want to play with the sharp baseline, inputting arbitray reset and turning times, you can do with the following:\n\nfrom rl_opts.rl_framework.numba.environments import search_loop_turn_reset_sharp\n\n\nturn = env_TurnRes.dist_target + 1 \nreset = env_TurnRes.dist_target + 2\n\nlength_sharp_run = int(1e7) # Feel free to increase to get a better estimate\n\nefficiency_sharp = search_loop_turn_reset_sharp(T = length_sharp_run, # Number of steps of the search\n                                    reset = reset, # After how many steps to reset\n                                    turn = turn, # After how many steps to turn\n                                    env = env_TurnRes # The environment\n                                    )"
  },
  {
    "objectID": "tutorials/tutorial_reset.html#didactic-example-training-a-single-agent",
    "href": "tutorials/tutorial_reset.html#didactic-example-training-a-single-agent",
    "title": "Learning to reset in target search problems",
    "section": "Didactic example: training a single agent",
    "text": "Didactic example: training a single agent\nAs before, let’s start with the basics! We will first define the new environment and agent. For the later, it is actually the same agent Forager, but with now 3 actions:\n\n# Number of actions of the agent, 3: continue, turn and reset.\nnum_actions = 3\n\n# Size of the state space. Because now we have two counters (turn and reset), the state space is 2D.\nsize_state_space = np.array([100, 100])\n\n# For the PS parameter, we performed a grid search and found the best parameters as shown above\n# IMPORTANT: these parameters were indeed used for all distances in the paper!\ngamma_damping = 1.93e-6\neta_glow_damping = 0.086\n\n\n# We now define the agent (may take time in the first run due to numba compilation\nTurnReset_agent = Forager(num_actions, \n                        size_state_space,\n                        gamma_damping, eta_glow_damping)\n\n\n# Number of episodes\nepisodes = 300 \n# Number of steps per episode (we use same as above for the sharp and exponential)\ntime_ep = int(1e4)\n\nsave_rewards = np.zeros(episodes)\n\nfor ep in tqdm(range(episodes)):\n    #initialize environment and agent's counter and g matrix\n    # Improve documentation\n\n    # Initialize the environment and the agent's counters and g-matrix\n    env_TurnRes.init_env()\n\n    turn_counter = 0\n    reset_counter = 0\n\n    TurnReset_agent.reset_g()\n\n    for t in range(time_ep):\n\n        # Update counters for matrices updates\n\n        # The efficient agent we created needs to keep track when the last\n        # update to the h_matrix happened. \n        TurnReset_agent.N_upd_H += 1\n        TurnReset_agent.N_upd_G += 1    \n\n\n        # Get the agent's state (i.e. its current counter)\n        state = np.array([turn_counter, reset_counter])\n        \n        # Get the action from the agent's policy \n        action = TurnReset_agent.deliberate(state)\n            \n        # Update the counters based on the action. This would\n        # typically be done inside of the environment, but because the state\n        # is in the case inherent to the agent, we have to do it here.\n        if action == 0: # Continue\n            turn_counter += 1  \n            reset_counter += 1 \n\n        elif action == 1: # Turn\n            turn_counter = 0  \n            reset_counter += 1 \n\n        elif action == 2: # Reset            \n            # Note that resetting also resets the turn counter, as we sample a new direction\n            turn_counter = 0     \n            reset_counter = 0 \n            \n        \n        # We now send the action to the environment to update the position\n        reward = env_TurnRes.update_pos(True if action == 1 else False, \n                                        True if action == 2 else False)\n\n        # If we got a reward or reached the maximum no update value for the H_matrix (for efficient agents), we update\n        # the h_matrix\n        if reward == 1:\n            TurnReset_agent._learn_post_reward(reward)            \n\n            # After receiving a reward, we also reset the counters\n            turn_counter = 0\n            reset_counter = 0                \n\n        if TurnReset_agent.N_upd_H == TurnReset_agent.max_no_H_update-1:\n            TurnReset_agent._learn_post_reward(reward)\n                \n        # Saving the reward\n        save_rewards[ep] += reward\n\n\n\n\n\nplt.plot(save_rewards/time_ep, label = 'Efficient agent')\n\n\n# We compare with the sharp reset efficiency\nplt.axhline(efficiency_sharp/length_sharp_run, color = 'red', linestyle = '--', label = 'Sharp reset')\n\n\nplt.legend()\n\nplt.xlabel('Episodes')\nplt.ylabel('Rewards')\n\nText(0, 0.5, 'Rewards')\n\n\n\n\n\nAs we see, the agent learns, and steadily reaches the sharp baseline! While the proposed set of learning parameters have shown to be quite robust, different runs will yield different training efficiencies. Nonetheless, in average all agents reach the baseline with sufficient episodes at the current target distance.\nNow, as we did above, we can take a look at the learned strategy, namely the policy of the agent. In this case, because we have two actions, we will have something more complex:\n\nfig, axs = plt.subplots(1, 3, figsize=(12, 4))\n\n\ncurrent_dist = env_TurnRes.dist_target\n\nfor i, (action, cmap) in enumerate(zip(['continue', 'turn', 'reset'],\n                                       ['Blues', 'Reds', 'Greens'])\n                                    ):\n\n    m = (TurnReset_agent.h_matrix[i] / TurnReset_agent.h_matrix.sum(0)).reshape(size_state_space)\n    \n    \n    axs[i].matshow(m, cmap = cmap)\n\n    axs[i].xaxis.set_ticks_position(\"bottom\")\n    axs[i].invert_yaxis()\n\n    axs[i].axvline(current_dist, ls = '--', c = 'k', alpha = 0.8)\n\nplt.setp(axs, aspect = 'auto', xlim = (-0.5, 10), ylim = (-0.5, 15))\nplt.setp(axs[0], ylabel = 'Reset counter')\nplt.setp(axs[:], xlabel = 'Turn counter');\n\n\n\n\nThese plots, analogous to those of Fig. 5 in our paper, show the behaviour of the agent. As we explain in the paper, there is a very consistent behaviour (you can run the training again to see a very similar result!): the agent has high probability of continuing until the vertical line at \\(L\\), entailing a step of length \\(L+1\\) (python numbering :P). It then turns. Because of the short training, the rest of the behaviour shown in the paper is still to be learned! You can run the trainings longer to see the same clean patter we show in the paper!"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "RL-OptS",
    "section": "",
    "text": "RL-OptS\n\n\nReinforcement Learning of Optimal Search strategies\n\n\n\n  \n\nThis library builds the necessary tools needed to study, replicate and develop reinforcement learning agents for target search problems, as well as a benchmark baselines with which to compare. This library is based in two different publications:\n\n“Optimal foraging strategies can be learned” by G. Muñoz-Gil, A. López-Incera, L. J. Fiderer and H. J. Briegel. Here we developed agents able to learn how to forage efficiently in environments with multiple targets.\n“Learning to reset in target search problems” by G. Muñoz-Gil, H. J. Briegel and M. Caraglio. Here we extended the agents to be able to reset to the origin, a feature that has revolutionize target search problems in the last years.\n\n\nInstallation\nYou can access all these tools installing the python package rl_opts via Pypi:\npip install rl-opts\nYou can also opt for cloning the source repository and executing the following on the parent folder you just cloned the repo:\npip install -e rl_opts\nThis will install both the library and the necessary packages.\n\n\nTutorials\nWe have prepared a series of tutorials to guide you through the most important functionalities of the package. You can find them in the Tutorials folder of the Github repository or in the Tutorials tab of our webpage, with notebooks that will help you navigate the package as well as reproducing the results of our paper via minimal examples. In particular, we have three tutorials:\n\nReinforcement learning  : shows how to train a RL agent based on Projective Simulation agents to search targets in randomly distributed environments as the ones considered in our paper.\nLearning to reset in target search problems  : shows how to train a RL agent similar to the previous, but with the ability to reset to the origin, an action that is learned along its spatial dynamics.\nImitation learning  : shows how to train a RL agent to imitate the policy of an expert equipped with a pre-trained policy. The latter is based on the benchmark strategies common in the literature.\nBenchmarks  : shows how launch various benchmark strategies with which to compare the trained RL agents.\n\n\n\nCite\nWe kindly ask you to cite our paper if any of the previous material was useful for your work:\n@article{munoz2024optimal,\n  title={Optimal foraging strategies can be learned},\n  author={Mu{\\~n}oz-Gil, Gorka and L{\\'o}pez-Incera, Andrea and Fiderer, Lukas J and Briegel, Hans J},\n  journal={New Journal of Physics},\n  volume={26},\n  number={1},\n  pages={013010},\n  year={2024},\n  publisher={IOP Publishing}\n}"
  },
  {
    "objectID": "lib_nbs/environments_numba.html",
    "href": "lib_nbs/environments_numba.html",
    "title": "Reinforcement learning environments",
    "section": "",
    "text": "This notebook gathers the functions creating different kinds of environments for foraging and target search in various scenarios, adapted for their use in the reinforcement learning paradigm."
  },
  {
    "objectID": "lib_nbs/environments_numba.html#isbetween",
    "href": "lib_nbs/environments_numba.html#isbetween",
    "title": "Reinforcement learning environments",
    "section": "isBetween",
    "text": "isBetween\n\nsource\n\nisBetween_c_Vec_numba\n\n isBetween_c_Vec_numba (a, b, c, r)\n\nChecks whether point c is crossing the line formed with point a and b.\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\na\ntensor, shape = (1,2)\nPrevious position.\n\n\nb\ntensor, shape = (1,2)\nCurrent position.\n\n\nc\ntensor, shape = (Nt,2)\nPositions of all targets.\n\n\nr\nint/float\nTarget radius.\n\n\nReturns\narray of boolean values\nTrue at the indices of found targets.\n\n\n\n\ncompiling = isBetween_c_Vec_numba(np.array([0.1,1]), np.array([1,3]), np.random.rand(100,2), 0.00001)\n\n\n\n\n4.65 μs ± 25.2 ns per loop (mean ± std. dev. of 7 runs, 100,000 loops each)\n\n\n\nfrom rl_opts.utils import isBetween_c_Vec as oldbetween\n\n\n\n\n40.4 μs ± 177 ns per loop (mean ± std. dev. of 7 runs, 10,000 loops each)"
  },
  {
    "objectID": "lib_nbs/environments_numba.html#pareto-sampling",
    "href": "lib_nbs/environments_numba.html#pareto-sampling",
    "title": "Reinforcement learning environments",
    "section": "Pareto sampling",
    "text": "Pareto sampling\n\nsource\n\npareto_sample\n\n pareto_sample (alpha, xm, size=1)"
  },
  {
    "objectID": "lib_nbs/environments_numba.html#random-sampling-from-array-with-probs",
    "href": "lib_nbs/environments_numba.html#random-sampling-from-array-with-probs",
    "title": "Reinforcement learning environments",
    "section": "Random sampling from array with probs",
    "text": "Random sampling from array with probs\n\nsource\n\nrand_choice_nb\n\n rand_choice_nb (arr, prob)\n\n:param arr: A 1D numpy array of values to sample from. :param prob: A 1D numpy array of probabilities for the given samples. :return: A random sample from the given array with a given probability."
  },
  {
    "objectID": "lib_nbs/environments_numba.html#d",
    "href": "lib_nbs/environments_numba.html#d",
    "title": "Reinforcement learning environments",
    "section": "1D",
    "text": "1D\n\nsource\n\nResetEnv_1D\n\n ResetEnv_1D (*args, **kwargs)\n\nInitialize self. See help(type(self)) for accurate signature.\n\n\nParallel search loops for Reset 1D\n\nsource\n\n\nparallel_Reset1D_exp\n\n parallel_Reset1D_exp (T, rates, L, D)\n\nRuns the Reset 1D loop in parallel for different exponential resetting rates.\n\nsource\n\n\nparallel_Reset1D_sharp\n\n parallel_Reset1D_sharp (T, resets, L, D)\n\nRuns the Reset 1D loop in parallel for different sharp resetting times."
  },
  {
    "objectID": "lib_nbs/environments_numba.html#d-1",
    "href": "lib_nbs/environments_numba.html#d-1",
    "title": "Reinforcement learning environments",
    "section": "2D",
    "text": "2D\n\nsource\n\nResetEnv_2D\n\n ResetEnv_2D (*args, **kwargs)\n\nInitialize self. See help(type(self)) for accurate signature."
  },
  {
    "objectID": "lib_nbs/environments_numba.html#parallel-search-loops-for-reset-2d",
    "href": "lib_nbs/environments_numba.html#parallel-search-loops-for-reset-2d",
    "title": "Reinforcement learning environments",
    "section": "Parallel search loops for Reset 2D",
    "text": "Parallel search loops for Reset 2D\n\nsource\n\nparallel_Reset2D_policies\n\n parallel_Reset2D_policies (T, reset_policies, dist_target, radius_target,\n                            D)\n\n\nsource\n\n\nparallel_Reset2D_exp\n\n parallel_Reset2D_exp (T, rates, dist_target, radius_target, D)\n\n\nsource\n\n\nparallel_Reset2D_sharp\n\n parallel_Reset2D_sharp (T, resets, dist_target, radius_target, D)"
  },
  {
    "objectID": "lib_nbs/environments_numba.html#search-loop-with-fixed-policy",
    "href": "lib_nbs/environments_numba.html#search-loop-with-fixed-policy",
    "title": "Reinforcement learning environments",
    "section": "Search loop with fixed policy",
    "text": "Search loop with fixed policy\n\nsource\n\nsearch_loop_turn_reset_sharp\n\n search_loop_turn_reset_sharp (T, reset, turn, env)\n\n*Runs a search loop of T steps. There is a single counter that works as follows:\n\nStarts at 0\nFor each turn or continue action gets +1\nIf reset or reach the target is set to 0*"
  },
  {
    "objectID": "lib_nbs/analytics.html",
    "href": "lib_nbs/analytics.html",
    "title": "Analytical functions",
    "section": "",
    "text": "source\n\n\n\n pdf_multimode (L:int, lambdas:list, probs:list)\n\n*Computes the discrete PDF of multi-mode exponential of the form\n\\[\n\\Pr(L) = \\sum_{i=1,2} \\omega_i (1-e^{-1/\\lambda_i}) e^{-(L-1)/\\lambda_i} \\, ,\n\\] where \\(\\omega\\) is the probability of each mode and \\(\\lambda\\) it’s scale.*\n\n\n\n\nType\nDetails\n\n\n\n\nL\nint\nEither int or array for which pdf is calculated\n\n\nlambdas\nlist\nScales of each modes\n\n\nprobs\nlist\nProbability weight of each mode\n\n\nReturns\narray\nArray with probability of each L\n\n\n\n\nlambdas = np.array([2,15])\nprobs = np.array([0.99, 0.01])\nL_max = 100\nplt.loglog(np.arange(1, L_max),\n           pdf_multimode(L = np.arange(1, L_max), lambdas=lambdas, probs=probs)\n          )\nplt.xlabel('L'); plt.ylabel('P(L)')\n\nText(0, 0.5, 'P(L)')\n\n\n\n\n\n\nsource\n\n\n\n\n pdf_powerlaw (L:float, beta:float=1)\n\nComputes the discrete PDF of a powerlaw of the form \\[\n\\Pr(L)\\sim L^{-1-\\mathrm{beta}}.\n\\]\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nL\nfloat\n\nEither int or array for which pdf is calculated\n\n\nbeta\nfloat\n1\nExponent of the power law\n\n\nReturns\narray\n\nArray with probability of each L\n\n\n\n\nplt.loglog(np.arange(1, 1000),\n           pdf_powerlaw(L = np.arange(1,1000), beta = 1)\n          )\nplt.xlabel('L'); plt.ylabel('P(L)')\n\nText(0, 0.5, 'P(L)')\n\n\n\n\n\n\nsource\n\n\n\n\n pdf_discrete_sample (pdf_func:object, num_samples:int, **args_func)\n\nSamples discrete values from a given PDF\n\n\n\n\nType\nDetails\n\n\n\n\npdf_func\nobject\nFunction generating the pdf\n\n\nnum_samples\nint\nNumber of samples to create\n\n\nargs_func\nVAR_KEYWORD\n\n\n\nReturns\narray\nSamples\n\n\n\n\nsamples = pdf_discrete_sample(pdf_func = pdf_multimode, \n                              num_samples=10000, \n                              L = np.arange(1, L_max), lambdas=lambdas, probs=probs)\n\ncounts = np.bincount(samples)[1:]\n\nplt.loglog(np.arange(1, len(counts)+1),\n           counts/counts.sum(), 'o', \n           label = 'Histogram samples')\n\nplt.loglog(np.arange(1, L_max),\n           pdf_multimode(L = np.arange(1, L_max), lambdas=lambdas, probs=probs),\n           label = 'Theory'\n          )\nplt.xlabel('L'); plt.ylabel('P(L)'); plt.legend()\n\n<matplotlib.legend.Legend>"
  },
  {
    "objectID": "lib_nbs/analytics.html#calculation-of-policy-from-step-length-pdf",
    "href": "lib_nbs/analytics.html#calculation-of-policy-from-step-length-pdf",
    "title": "Analytical functions",
    "section": "Calculation of policy from step length PDF",
    "text": "Calculation of policy from step length PDF\n\nsource\n\nget_policy_from_dist\n\n get_policy_from_dist (n_max, func, renorm=True, **args_func)\n\nGiven a PDF of step lengths, calculates the corresponding policy\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nn_max\n\n\nMaximum counter n_max for which the policy is calculated\n\n\nfunc\n\n\nFunction generating the pdf\n\n\nrenorm\nbool\nTrue\nIf true, we check whether the distribution has a boundary N, for which _n=N^Pr(L=nd) = 0\n\n\nargs_func\nVAR_KEYWORD\n\n\n\n\nReturns\narray\n\nPolicy at each counter value\n\n\n\n\nL = 100\nbetas = np.linspace(0.1, 2, 10)\ncolors = plt.cm.plasma(np.linspace(0,1,len(betas)+2))\n\nfig, ax = plt.subplots()\nfor beta, color in zip(betas, colors):\n    \n    policy = get_policy_from_dist(n_max = L, \n                                  func = pdf_powerlaw,\n                                  beta = beta)\n    ax.plot(np.arange(2, L+1), policy[1:], c = color)\n\n\n# Plot features    \nplt.setp(ax, xlabel =r'$n$', ylabel = r'$\\pi(\\uparrow|n)$', xscale = 'log')\ncbar = fig.colorbar(plt.cm.ScalarMappable(norm= mcolors.Normalize(vmin=betas.min(), \n                                                                  vmax=betas.max()),\n                                          cmap=plt.cm.plasma),\n                    ax = ax)\ncbar.set_label(r'$\\beta$')"
  },
  {
    "objectID": "lib_nbs/learning_and_benchmark.html",
    "href": "lib_nbs/learning_and_benchmark.html",
    "title": "Learning and benchmarking",
    "section": "",
    "text": "This notebook gathers the functions needed to train agents to forage optimally, as well as tools to calculate their foraging efficiency as well as their comparison to benchmark foraging strategies.\n\nLearning\n\nsource\n\nlearning\n\n learning (config, results_path, run)\n\nTraining of the RL agent\n\n\n\n\nType\nDetails\n\n\n\n\nconfig\ndict\nDictionary with all the parameters\n\n\nresults_path\nstr\nPath to save the results\n\n\nrun\nint\nAgent identifier\n\n\n\n\n\n\nGenerate walk from a policy\n\nsource\n\nwalk_from_policy\n\n walk_from_policy (policy, time_ep, n, L, Nt, r, lc, destructive=False,\n                   with_bound=False, bound=100)\n\nWalk of foragers given a policy. Performance is evaluated as the number of targets found in a fixed time time_ep.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\npolicy\nlist\n\nStarting from counter=1, prob of continuing for each counter value.\n\n\ntime_ep\nint\n\nNumber of steps (decisions).\n\n\nn\nint\n\nNumber of agents that walk in parallel (all with the same policy, they do not interact). This is “number of walks” in the paper.\n\n\nL\nint\n\nWorld size.\n\n\nNt\nint\n\nNumber of targets.\n\n\nr\nfloat\n\nTarget radius.\n\n\nlc\nfloat\n\nCutoff length. Agent is displaced a distance lc from the target when it finds it.\n\n\ndestructive\nbool\nFalse\nTrue if targets are destructive. The default is False.\n\n\nwith_bound\nbool\nFalse\nTrue if policy is cut. The default is False.\n\n\nbound\nint\n100\nBound of the policy (maximum value for the counter). The default is 20.\n\n\nReturns\nlist, len(rewards)=n\n\nNumber of targets found by each agent in time_ep steps of d=1.\n\n\n\n\n\n\nEfficiency computation\n\nsource\n\nagent_efficiency\n\n agent_efficiency (results_path, config, run, num_walks, episode_interval)\n\nComputes the agent’s average search efficiency over a number of walks where the agent follows a fixed policy. This is repeated with the policies at different stages of the training to analyze the evolution of its performance.\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\nresults_path\nstr\nPath to the results folder, from which to extract the agent’s policies\n\n\nconfig\ndict\nDictionary with all the parameters. It needs to be the same configuration file as the one used to train the agent.\n\n\nrun\nint\nId of the agent\n\n\nnum_walks\nint\nNumber of (independent) walks\n\n\nepisode_interval\nint\nEvery ‘episode_interval’ training episodes, the policy of the agent is taken and its performance is analyzed.\n\n\n\n\n\n\nBenchmarks\nCode to get the search efficiency of the benchmark models. We consider Lévy and bi-exponential distributions and obtain the model parameters that achieve the highest search efficiency. We use the library Tune for the efficiency optimization within given parameter ranges.\n\nsource\n\naverage_search_efficiency\n\n average_search_efficiency (config)\n\nGet the average search efficiency, considering the benchmark model defined in config.\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\nconfig\ndict\nDictionary with the configuration of the benchmark model.\n\n\n\n\n\nExample\nSet up the configuration, run and type of search\n\nfrom rl_opts.learn_and_bench import average_search_efficiency\nfrom ray import tune\nfrom ray.tune.search.bayesopt import BayesOptSearch\nfrom ray.tune.search import ConcurrencyLimiter\nimport numpy as np\n\n\n#### Minimal example #####\nrun = '0'\nsearch_type = 'Bayesian'\nconfig = {'d_int': tune.uniform(0.00001, 20.0),\n          'd_ext': 100.0,\n          'p': tune.uniform(0.0, 1.0),\n          'beta': None,\n          'model': 'double_exp',\n          'time_ep': 20,\n          'n': 10,\n          'lc': 3.0,\n          'Nt': 100,\n          'L': 100,\n          'r': 0.5,\n          'destructive': False,\n          'results_path': None,\n          'num_raytune_samples':10\n         }\n\nInitialize Tune\n\nif search_type == 'Bayesian': #Bayesian optimization\n    \n    bayesopt = BayesOptSearch(metric=\"mean_eff\", mode=\"max\")\n    bayesopt = ConcurrencyLimiter(bayesopt, max_concurrent=3)\n    tuner = tune.Tuner(average_search_efficiency, \n                        tune_config=tune.TuneConfig(search_alg=bayesopt, num_samples=config['num_raytune_samples']), \n                        param_space=config)\n    \nelif search_type == 'Grid': #Grid search\n\n    tuner = tune.Tuner(average_search_efficiency,\n                        tune_config=tune.TuneConfig(num_samples=1),\n                        param_space=config)\n\nRun the algorithm\n\nresult_grid = tuner.fit()"
  },
  {
    "objectID": "lib_nbs/imitation_learning.html",
    "href": "lib_nbs/imitation_learning.html",
    "title": "Imitation learning",
    "section": "",
    "text": "source"
  },
  {
    "objectID": "lib_nbs/imitation_learning.html#example",
    "href": "lib_nbs/imitation_learning.html#example",
    "title": "Imitation learning",
    "section": "Example",
    "text": "Example\nWe showcase how to imitate the policy based on a given step length distribution, an in particular of a Lévy distribution. For further examples, see the Tutorials section.\n\nfrom rl_opts.analytics import pdf_powerlaw, pdf_discrete_sample, get_policy_from_dist\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n\nNUM_STATES = 100 # size of the state space\nEPOCHS = 100 # number of epochs\nNUM_STEPS = 1000 # number of learning steps per episode\n\nsteps = pdf_discrete_sample(pdf_func = pdf_powerlaw,\n                            beta = 1,\n                            L = np.arange(1, NUM_STATES),  \n                            num_samples = (EPOCHS, NUM_STEPS))\n\nimitator = PS_imitation(num_states = NUM_STATES,\n                        eta = int(1e-7),\n                        gamma = 0)\n    \n                               \nfor e in tqdm(range(EPOCHS)):\n    imitator.reset()\n    for s in steps[e]:    \n        imitator.update(length = s)\n\n100%|██████████| 100/100 [00:01<00:00, 86.11it/s]\n\n\n\npolicy_theory = get_policy_from_dist(n_max = NUM_STATES,\n                                     func = pdf_powerlaw,\n                                     beta = 1)\npolicy_imitat = imitator.h_matrix[0,:]/imitator.h_matrix.sum(0)\n\n\n_ , ax = plt.subplots(figsize = (5,3))\nax.plot(policy_imitat ,'o')\nax.plot(np.arange(1, NUM_STATES), policy_theory[1:])\nplt.setp(ax, \n         xscale = 'log', xlim = (0.9, NUM_STATES/2), xlabel = r'Counter $n$',\n         ylim = (0.5, 1.1), ylabel = 'Policy');"
  },
  {
    "objectID": "lib_nbs/mfpt.html",
    "href": "lib_nbs/mfpt.html",
    "title": "Mean First Passage Times in 1D and 2D environments",
    "section": "",
    "text": "source\n\n\n\n constant_velocity_generator (N, T, time_sampler, velocity=1,\n                              **sample_args)\n\nGiven a sampler for length of time steps, generates a trajectory considering a constant velocity in the sampled times. After each time step, we sample a new direction.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nN\n\n\nNumber of trajectories\n\n\nT\n\n\nLength of trajectories\n\n\ntime_sampler\n\n\nSampler of time of constant velocity\n\n\nvelocity\nint\n1\nVelocity\n\n\nsample_args\nVAR_KEYWORD\n\n\n\n\n\n\nN = 5; T = 150\n\ntrajs_lw = constant_velocity_generator(N,\n                                       T, \n                                       time_sampler = pdf_discrete_sample, \n                                       pdf_func=pdf_powerlaw,\n                                       L = np.arange(1, T),                                       \n                                       beta = 1)\n\ndef single_steps(num_samples):\n    return np.ones(num_samples)\n\ntrajs_rw = constant_velocity_generator(N, T, time_sampler = single_steps, velocity = 1.2)\n\n\nfig, ax = plt.subplots(1, 2, figsize = (5,3))\nax[0].plot(trajs_lw.transpose())\nax[0].set_title('LW')\nax[1].plot(trajs_rw.transpose())\nax[1].set_title('RW');\n\n\n\n\n\n\n\n\n\n\nsource\n\n\n\n\n\n mfpt_rw (N:int, T:int, x0:float, Ls:list, traj_generator:Callable,\n          max_loop=5, save=None, **args_generator)\n\nCalculates MFPT to boundaries at 0 and L starting in x0\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nN\nint\n\nNumber of trajectories\n\n\nT\nint\n\nLengths of trajectories\n\n\nx0\nfloat\n\nStarting point of walk\n\n\nLs\nlist\n\nBoundary position\n\n\ntraj_generator\nCallable\n\nFunction generating the walk (must start in x0 = zero)\n\n\nmax_loop\nint\n5\nMax number of while loop if some walks do not reach the boundary\n\n\nsave\nNoneType\nNone\n\n\n\nargs_generator\nVAR_KEYWORD\n\n\n\n\n\n\n\n\nsource\n\n\n\n\n\n mfpt_informed_rw (N:int, T:int, x0:float, Ls:list,\n                   traj_generator:Callable, max_loop=5, time_sampler=None,\n                   save=None, **args_generator)\n\nCalculates MFPT to boundaries at 0 and L starting in x0 with a traj generator informed on the scales of the sysmte (x0 and L)\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nN\nint\n\nNumber of trajectories\n\n\nT\nint\n\nLengths of trajectories\n\n\nx0\nfloat\n\nStarting point of walk\n\n\nLs\nlist\n\nBoundary position\n\n\ntraj_generator\nCallable\n\nFunction generating the walk (must start in x0 = zero)\n\n\nmax_loop\nint\n5\nMax number of while loop if some walks do not reach the boundary\n\n\ntime_sampler\nNoneType\nNone\n\n\n\nsave\nNoneType\nNone\n\n\n\nargs_generator\nVAR_KEYWORD\n\n\n\n\n\n\n\n\n\n\nIn this case we are already doing constant velocities, hence we can use the generator below:\n\nsource\n\n\n\n\n\n rw_generator (N, T)\n\n\nN = int(1e2)\nT = int(1e6)\nx0 = 3\nLs = np.arange(15, 100)\n\nreps = 20\nresults = np.array(Parallel(n_jobs=20)(delayed(mfpt_rw)(N = N, T = T, x0 = x0, Ls = Ls, \n                                                        traj_generator = rw_generator)\n                for _ in tqdm(range(reps))), dtype = object)\n\n\nmean = results.mean(0)\n\nplt.plot(Ls, mean[0]*(Ls/Ls[0])**2,c = 'k', label = r'$\\sim L^2$')\nplt.plot(Ls, mean[0]*(Ls/Ls[0]), c = 'k',ls = '--', label = r'$\\sim L$')\nplt.loglog(Ls, mean,'o', alpha = 0.3)\nplt.legend()\n\n<matplotlib.legend.Legend>\n\n\n\n\n\n\n\n\nsource\n\n\n\n\n\n exp_time_generator (num_samples)\n\n\nN = int(1e2)\nT = int(1e4)\nx0 = 3\nLs = np.arange(15, 100)\n\nreps = 20\nresults = np.array(Parallel(n_jobs=20)(delayed(mfpt_rw)(N = N, T = T, x0 = x0, Ls = Ls,\n                                                        traj_generator = constant_velocity_generator,\n                                                        time_sampler = exp_time_generator)\n                for _ in tqdm(range(reps))), dtype = object)\n\n\nmean_exp = results.mean(0)\n\nplt.plot(Ls, mean_exp[0]*(Ls/Ls[0])**2,c = 'k', label = r'$\\sim L^2$')\nplt.plot(Ls, mean_exp[0]*(Ls/Ls[0]), c = 'k',ls = '--', label = r'$\\sim L$')\nplt.loglog(Ls, mean_exp,'o', alpha = 0.3, label = 'Exponential')\nplt.loglog(Ls, mean,'o', alpha = 0.3, label = 'Discrete')\nplt.legend()\nplt.xlabel('MFPT'); plt.ylabel('L'); plt.title(fr'$x_0 =$ {x0}') \nplt.ylim(ymax = 500)\n\n(23.655963721815578, 500)\n\n\n\n\n\n\n\n\nsource\n\n\n\n\n\n Biexp (informed=False, **args)\n\nInitialize self. See help(type(self)) for accurate signature.\n\nsource\n\n\n\n\n biexp_time_generator (num_samples, **sample_kwargs)\n\n\nbi = Biexp(informed = False, w1 = 0.1, d1 = 1, d2 = 200)\nbi_rng = bi.sample(int(1e6))\n\nh, e = np.histogram(bi_rng, bins = np.linspace(0.1,100, 2000), density=True)\nplt.loglog(e[:-1], h)\nplt.plot(e, bi.pdf(e)*(h[0]/bi.pdf(e[0])), alpha = 0.8)\n\n\n\n\n\n\n\nN = int(1e2)\nT = int(1e4)\nx0 = 3\nLs = np.arange(15, 100)\n\nreps = 20\nresults = np.array(Parallel(n_jobs=20)(delayed(mfpt_rw)(N = N, T = T, x0 = x0, Ls = Ls,\n                                                        traj_generator = constant_velocity_generator,\n                                                        time_sampler = biexp_time_generator, w1 = 0.5, d1 = 1, d2 = 5)\n                for _ in tqdm(range(reps))), dtype = object)\n\nmean_uninf_biexp = results.mean(0)\n\n\nplt.plot(Ls, (Ls/Ls[0])**(1/2),c = 'k', label = r'$\\sim \\sqrt{L}$')\nplt.plot(Ls, (Ls/Ls[0]), c = 'k',ls = '--', label = r'$\\sim L$')\n\nplt.loglog(Ls, mean_uninf_biexp/mean_uninf_biexp[0],'o', alpha = 0.3, label = 'Uninformed bi-exponential')\nplt.loglog(Ls, mean_exp/mean_exp[0],'o', alpha = 0.3, label = 'Exponential')\nplt.loglog(Ls, mean/mean[0],'o', alpha = 0.3, label = 'Discrete')\nplt.legend()\nplt.ylabel('MFPT'); plt.xlabel('L'); plt.title(fr'$x_0 =$ {x0}')\n\nText(0.5, 1.0, '$x_0 =$ 3')\n\n\n\n\n\n\n\n\n\nN = int(1e2)\nT = int(1e4)\nx0 = 3\nLs = np.arange(15, 100)\n\nreps = 20\nresults = np.array(Parallel(n_jobs=10)(delayed(mfpt_informed_rw)(N = N, T = T, x0 = x0, Ls = Ls, \n                                                                 traj_generator = constant_velocity_generator,\n                                                                 time_sampler = biexp_time_generator)\n                for _ in tqdm(range(reps))), dtype = object)\n\nmean_biexp = results.mean(0)\n\n\nplt.plot(Ls, (Ls/Ls[0])**(1/2),c = 'k', label = r'$\\sim \\sqrt{L}$')\nplt.plot(Ls, (Ls/Ls[0]), c = 'k',ls = '--', label = r'$\\sim L$')\n\n\nplt.loglog(Ls, mean_biexp/mean_biexp[0],'o', alpha = 0.3, label = 'Informed bi-exponential')\nplt.loglog(Ls, mean_uninf_biexp/mean_uninf_biexp[0],'o', alpha = 0.3, label = 'Uninformed bi-exponential')\nplt.loglog(Ls, mean_exp/mean_exp[0],'o', alpha = 0.3, label = 'Exponential')\nplt.loglog(Ls, mean/mean[0],'o', alpha = 0.3, label = 'Discrete')\nplt.legend()\nplt.ylabel('MFPT'); plt.xlabel('L'); plt.title(fr'$x_0 =$ {x0}')\n\nText(0.5, 1.0, '$x_0 =$ 3')\n\n\n\n\n\n\n\n\n\n\\(\\beta=1\\), multiple repetitions\n\nN = int(1e2)\nT = int(1e5)\nx0 = 3\nLs = np.arange(15, 100)\n\nbeta = 1\n\nresults = np.array(Parallel(n_jobs=10)(delayed(mfpt_rw)(N = N, T = T, x0 = x0, Ls = Ls, \n                                                        traj_generator = constant_velocity_generator,\n                                                        time_sampler = pdf_discrete_sample, \n                                                        pdf_func=pdf_powerlaw,\n                                                        L = np.arange(1, T),\n                                                        beta = beta)\n                for _ in tqdm(range(reps))), dtype = object)\n\nmean_pw = results.mean(0)\n\nMultiple \\(\\beta\\)\n\nN = int(1e3)\nT = int(1e5)\nx0 = 3\nLs = np.arange(15, 100)\n\nbetas = np.linspace(0.1, 1.5, 10)\n\n\nresults = np.array(Parallel(n_jobs=len(betas))(delayed(mfpt_rw)(N = N, T = T, x0 = x0, Ls = Ls, \n                                                                traj_generator = constant_velocity_generator,\n                                                                time_sampler = pdf_discrete_sample, \n                                                                pdf_func=pdf_powerlaw,\n                                                                L = np.arange(1, T),\n                                                                beta = beta)\n                for beta in tqdm(betas)), dtype = object)\n\n\nmean_pw_betas = results.mean(0)\n\n\nfig, ax = plt.subplots()\nax.plot(Ls, (Ls/Ls[0])**(1/2),c = 'k', label = r'$\\sim \\sqrt{L}$')\npw_scaling = np.log(Ls)*(Ls**(1/2))\nax.plot(Ls, pw_scaling/pw_scaling[0], c = 'k',ls = '--', label = r'$\\sim \\sqrt{L}\\ln{L}$')\n\n\ncolors = plt.cm.plasma(np.linspace(0,1,len(betas)+2))\nfor res, c in zip(results, colors):\n    ax.loglog(Ls, res/res[0], 'o', c = c, alpha = 0.3)\n\ncbar = fig.colorbar(plt.cm.ScalarMappable(norm= mcolors.Normalize(vmin=betas.min(), \n                                                                  vmax=betas.max()),\n                                          cmap=plt.cm.plasma),\n                    ax = ax)\ncbar.set_label(r'$\\beta$')\n\nax.legend()\nplt.setp(ax, ylabel = 'MFPT', xlabel = 'L', title = fr'$x_0 =$ {x0}')\n\n[Text(0, 0.5, 'MFPT'), Text(0.5, 0, 'L'), Text(0.5, 1.0, '$x_0 =$ 3')]\n\n\n\n\n\n\ncolors = plt.cm.plasma(np.linspace(0,1,len(betas)+2))\nfor res, c, beta in zip(results, colors, betas):\n    if round(beta, 1) == 1: c = 'k'\n    plt.loglog(Ls, res, 'o', c = c, alpha = 0.3)\nplt.ylabel('MFPT'); plt.xlabel('L'); plt.title(fr'$x_0 =$ {x0}')\n\nText(0.5, 1.0, '$x_0 =$ 3')\n\n\n\n\n\nSmaller \\(\\beta\\)\n\nfig, ax = plt.subplots()\nax.plot(Ls, (Ls/Ls[0])**(1/2),c = 'k', label = r'$\\sim \\sqrt{L}$')\npw_scaling = np.log(Ls)*(Ls**(1/2))\nax.plot(Ls, pw_scaling/pw_scaling[0], c = 'k',ls = '--', label = r'$\\sim \\sqrt{L}\\ln{L}$')\n\n\ncolors = plt.cm.plasma(np.linspace(0,1,len(betas)+2))\nfor res, c in zip(results, colors):\n    ax.loglog(Ls, res/res[0], 'o', c = c, alpha = 0.3)\n\ncbar = fig.colorbar(plt.cm.ScalarMappable(norm= mcolors.Normalize(vmin=betas.min(), \n                                                                  vmax=betas.max()),\n                                          cmap=plt.cm.plasma),\n                    ax = ax)\ncbar.set_label(r'$\\beta$')\nplt.setp(ax, ylabel = 'MFPT', xlabel = 'L', title = fr'$x_0 =$ {x0}')\n\n[Text(0, 0.5, 'MFPT'), Text(0.5, 0, 'L'), Text(0.5, 1.0, '$x_0 =$ 0.02')]\n\n\n\n\n\n\ncolors = plt.cm.plasma(np.linspace(0,1,len(betas)+2))\nfor res, c in zip(results, colors):\n    plt.loglog(Ls, res, 'o', c = c, alpha = 0.3)\nplt.ylabel('MFPT'); plt.xlabel('L'); plt.title(fr'$x_0 =$ {x0}')\n\nText(0.5, 1.0, '$x_0 =$ 0.02')\n\n\n\n\n\n  0%|          | 0/100 [00:00<?, ?it/s]\n  0%|          | 0/100 [00:00<?, ?it/s]\n  0%|          | 0/100 [00:00<?, ?it/s]\n  0%|          | 0/100 [00:00<?, ?it/s]\n  0%|          | 0/100 [00:00<?, ?it/s]\n  0%|          | 0/100 [00:00<?, ?it/s]\n  0%|          | 0/100 [00:00<?, ?it/s]\n  0%|          | 0/100 [00:00<?, ?it/s]\n  0%|          | 0/100 [00:00<?, ?it/s]\n  0%|          | 0/100 [00:00<?, ?it/s]\n  0%|          | 0/100 [00:00<?, ?it/s]\n  0%|          | 0/100 [00:00<?, ?it/s]\n  0%|          | 0/100 [00:00<?, ?it/s]\n  0%|          | 0/100 [00:00<?, ?it/s]\n  0%|          | 0/100 [00:00<?, ?it/s]\n\n\n\n\n\nSaving the data for future tests\n\na,b,c = [0]*3\n\n\nfor k in [a,b,c]:\n    print(k)\n    k = 12\n\n0\n0\n0\n\n\n\nb\n\n0\n\n\n\nnames = ['discrete', 'exp', 'inf_biexp', 'uninf_biexp', 'pw']\ntry: \n    datas = [mean, mean_exp, mean_biexp, mean_uninf_biexp, mean_pw]\n    for data, name in zip(datas, names):\n        np.save('scalings_mfpt/'+name+f'_N_{N}_T_{T}_x0_{x0}_L_{min(Ls)}-{max(Ls)}.npy', data)\nelse: \n    mean = np.load('scalings_mfpt/'+names[0]+f'_N_{N}_T_{T}_x0_{x0}_L_{min(Ls)}-{max(Ls)}.npy', data)\n    mean_exp = np.load('scalings_mfpt/'+names[1]+f'_N_{N}_T_{T}_x0_{x0}_L_{min(Ls)}-{max(Ls)}.npy', data)\n    mean_biexp = np.load('scalings_mfpt/'+names[2]+f'_N_{N}_T_{T}_x0_{x0}_L_{min(Ls)}-{max(Ls)}.npy', data)\n    mean_uninf_biexp = np.load('scalings_mfpt/'+names[3]+f'_N_{N}_T_{T}_x0_{x0}_L_{min(Ls)}-{max(Ls)}.npy', data)\n    mean_pw = np.load('scalings_mfpt/'+names[4]+f'_N_{N}_T_{T}_x0_{x0}_L_{min(Ls)}-{max(Ls)}.npy', data)\n\n\nfig, ax = plt.subplots(1, 2, figsize = (10, 5))\n\nax[0].plot(Ls, (Ls/Ls[0])**(1/2),c = 'k', label = r'$\\sim \\sqrt{L}$')\nax[0].plot(Ls, (Ls/Ls[0]), c = 'k',ls = '--', label = r'$\\sim L$')\npw_scaling = np.log(Ls)*Ls**(1/2)\nax[0].plot(Ls, pw_scaling/pw_scaling[0], c = 'k',ls = ':', label = r'$\\sim \\sqrt{L}\\ln{L}$')\n\nax[0].loglog(Ls, mean_pw/mean_pw[0],' o', alpha = 0.3, label = r'Power-law $\\alpha = 1$')\nax[0].loglog(Ls, mean_biexp/mean_biexp[0],'o', alpha = 0.3, label = 'Informed bi-exponential')\nax[0].loglog(Ls, mean_uninf_biexp/mean_uninf_biexp[0],'o', alpha = 0.3, label = 'Uninformed bi-exponential')\nax[0].loglog(Ls, mean_exp/mean_exp[0],'o', alpha = 0.3, label = 'Exponential')\nax[0].loglog(Ls, mean/mean[0],'o', alpha = 0.3, label = 'Discrete')\nax[0].legend()\n\n\nax[1].loglog(Ls, mean_pw,' o', alpha = 0.3, label = r'Power-law $\\alpha = 1$')\nax[1].loglog(Ls, mean_biexp,'o', alpha = 0.3, label = 'Informed bi-exponential')\nax[1].loglog(Ls, mean_uninf_biexp,'o', alpha = 0.3, label = 'Uninformed bi-exponential')\nax[1].loglog(Ls, mean_exp,'o', alpha = 0.3, label = 'Exponential')\nax[1].loglog(Ls, mean,'o', alpha = 0.3, label = 'Discrete')\n\n\n\nplt.setp(ax, ylabel = 'MFPT', xlabel = 'L', title = fr'$x_0 =$ {x0}')\n\n[Text(0, 0.5, 'MFPT'),\n Text(0.5, 0, 'L'),\n Text(0.5, 1.0, '$x_0 =$ 3'),\n Text(0, 0.5, 'MFPT'),\n Text(0.5, 0, 'L'),\n Text(0.5, 1.0, '$x_0 =$ 3')]\n\n\n\n\n\nReading results\n\nmean_fpt = []\nnames_read = names#[:3]+[names[-1]]*len(betas)\nidxbeta = 6\nfor idx, name in enumerate(names_read):\n    try: del collect_mean\n    except: pass    \n\n    if name == 'biexp_inf': reps = 5500\n    else: reps = 500\n    \n    for rep in range(reps):\n        if name == 'pw':\n            current_m = np.load(path+name+f'_beta{round(betas[idxbeta],3)}_N_{np.log10(Ns[-1]).astype(int)}_L_{np.log10(Ts[-1]).astype(int)}_{rep}.npy')\n                \n        else:\n            current_m = np.load(path+name+f'_N_{np.log10(Ns[idx]).astype(int)}_L_{np.log10(Ts[idx]).astype(int)}_{rep}.npy')\n            \n        # current_s = np.load(path+name+f'_N_{np.log10(Ns[idx]).astype(int)}_L_{np.log10(Ts[idx]).astype(int)}_{rep}_stats.npy')\n        try:\n            collect_mean = np.vstack((collect_mean, current_m))\n            # collect_stat = np.vstack((collect_stat, current_s))\n        except:\n            collect_mean = current_m\n            # collect_stat = current_s\n    mean_fpt.append(collect_mean)\n\n\nfig, ax = plt.subplots()\nminL = 60\nLsplot = Ls[minL:]\nfor m, n  in zip(mean_fpt, names_read):\n    if n == 'pw': n = r'pw $\\beta \\approx 1$'\n    mean = m.mean(0)#[minL:]\n    ax.plot(Ls, mean/mean[0], label = n)\n\n\nax.plot(Lsplot, 2.7*(Lsplot/Lsplot[0])**(1/2),c = 'k', label = r'$\\sim \\sqrt{L}$')\nax.plot(Lsplot, 5*(Lsplot/Lsplot[0]), c = 'k',ls = '--', label = r'$\\sim L$')\npw_scaling = np.log(Lsplot)*Lsplot**(1/2)\nax.plot(Lsplot, 3.5*pw_scaling/pw_scaling[0], c = 'k',ls = ':', label = r'$\\sim \\sqrt{L}\\ln{L}$')\n\nplt.setp(ax, xscale = 'log', yscale = 'log', xlabel = 'L', ylabel = 'MFPT', title = fr'$x_0 = {x0}$')\nax.legend()\n\n<matplotlib.legend.Legend>\n\n\n\n\n\n\nmean_fpt_pw = []\nname = 'pw'#[:3]+[names[-1]]*len(betas)\n\nfor idxbeta, beta in enumerate(betas):\n    try: del collect_mean\n    except: pass    \n\n        \n    for rep in range(reps):\n        current_m = np.load(path+name+f'_beta{round(betas[idxbeta],3)}_N_{np.log10(Ns[-1]).astype(int)}_L_{np.log10(Ts[-1]).astype(int)}_{rep}.npy')\n       \n        try:\n            collect_mean = np.vstack((collect_mean, current_m))\n            # collect_stat = np.vstack((collect_stat, current_s))\n        except:\n            collect_mean = current_m\n            # collect_stat = current_s\n    mean_fpt_pw.append(collect_mean)\n\n\nfig, ax = plt.subplots()\nminL = 60\nLsplot = Ls[minL:]\ncolors = plt.cm.plasma(np.linspace(0,1,len(betas)+2))\nfor m, c  in zip(mean_fpt_pw, colors):\n    mean = m.mean(0)[minL:]\n    ax.plot(Lsplot, mean/mean[0], c = c)\n\n\nax.plot(Lsplot, (Lsplot/Lsplot[0])**(1/2),c = 'k', label = r'$\\sim \\sqrt{L}$')\nax.plot(Lsplot, (Lsplot/Lsplot[0]), c = 'k',ls = '--', label = r'$\\sim L$')\npw_scaling = np.log(Lsplot)*Lsplot**(1/2)\nax.plot(Lsplot, pw_scaling/pw_scaling[0], c = 'k',ls = ':', label = r'$\\sim \\sqrt{L}\\ln{L}$')\n\nplt.setp(ax, xscale = 'log', yscale = 'log', xlabel = 'L', ylabel = 'MFPT', title = fr'$x_0 = {x0}$')\nax.legend()\ncbar = fig.colorbar(plt.cm.ScalarMappable(norm= mcolors.Normalize(vmin=betas.min(), \n                                                                  vmax=betas.max()),\n                                          cmap=plt.cm.plasma),\n                    ax = ax)\ncbar.set_label(r'$\\beta$')\n\n\n\n\n\nfit = []\nfor m, c  in zip(mean_fpt_pw, colors):\n    mean = m.mean(0)[minL:]\n    fit.append(np.polyfit(np.log(Lsplot), np.log(mean/mean[0]), deg=1)[0])\n\n\n# Approx fit log(L)\napp_log =  np.polyfit(np.log(Lsplot), np.log(pw_scaling/pw_scaling[0]), deg=1)[0]\n\n# Fit informed\nmftp_inf = mean_fpt[-2].copy()\nmean_inf = mftp_inf.mean(0)[minL:]\nfit_informed = np.polyfit(np.log(Lsplot), np.log(mean_inf/mean_inf[0]), deg=1)[0]\n\n# Plot\nfig, ax = plt.subplots()\n# plot powerlaw\nax.plot(betas, fit, c = 'k')\nax.scatter(betas, fit, c = colors[:len(betas)], zorder = 10)\nfor line, name, ls in zip([1, app_log, 1/2], [r'$L$', r'$\\sim \\sqrt{L}\\ln{L}$', r'$\\sqrt{L}$'], ['-', '--',':']):\n    ax.axhline(line, label = name, ls = ls, alpha = 0.5, c = 'k')\n# plot informed    \nax.axhline(fit_informed, label = 'Fit Exp informed', lw = 2)\n    \nax.legend()\nplt.setp(ax, xlabel = r'$\\beta$', ylabel = 'Fit long time MFPT')\n\n[Text(0.5, 0, '$\\\\beta$'), Text(0, 0.5, 'Fit long time MFPT')]\n\n\n\n\n\n\nminL = 60\nmean_inf = mftp_inf.mean(0)[minL:]\nLsplot = Ls[minL:]\nplt.loglog(Lsplot, mean_inf/mean_inf[0])\nplt.loglog(Lsplot, (Lsplot/Lsplot[0])**(0.6))"
  },
  {
    "objectID": "lib_nbs/mfpt.html#d-not-conclusive-results",
    "href": "lib_nbs/mfpt.html#d-not-conclusive-results",
    "title": "Mean First Passage Times in 1D and 2D environments",
    "section": "2D (not conclusive results)",
    "text": "2D (not conclusive results)\n\nsource\n\nconstant_velocity_generator_2D\n\n constant_velocity_generator_2D (N, T, time_sampler, velocity=1,\n                                 **sample_args)\n\nGiven a sampler for length of time steps, generates a 2D trajectory considering a constant velocity in the sampled times. After each time step, we sample a new direction.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nN\n\n\nNumber of trajectories\n\n\nT\n\n\nLength of trajectories\n\n\ntime_sampler\n\n\nSampler of time of constant velocity\n\n\nvelocity\nint\n1\nVelocity\n\n\nsample_args\nVAR_KEYWORD\n\n\n\n\n\n\nN = 5; T = 150\ntrajs_lw_x, trajs_lw_y = constant_velocity_generator_2D(N, T, time_sampler = lw_step, beta = 1)\n\ndef single_steps(num_samples):\n    return np.ones(num_samples)\n\ntrajs_rw_x, trajs_rw_y = constant_velocity_generator_2D(N, T, time_sampler = single_steps, velocity = 1.2)\n\nFinished generating trajectories\nFinished generating trajectories\n\n\n\nfig, ax = plt.subplots(1, 2)\nfor x, y in zip(trajs_lw_x, trajs_lw_y):\n    ax[0].plot(x, y)\nax[0].set_title('LW')\nfor x, y in zip(trajs_rw_x, trajs_rw_y):\n    ax[1].plot(x, y)\nax[1].set_title('RW');\n\n\n\n\n\n\nMFPT calculator\n\nsource\n\n\nmfpt_rw_2D\n\n mfpt_rw_2D (N:int, T:int, x0:list, Ls:list, traj_generator:Callable,\n             max_loop=5, **args_generator)\n\nCalculates MFPT to boundaries at 0 and L starting in x0 in 2D\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nN\nint\n\nNumber of trajectories\n\n\nT\nint\n\nLengths of trajectories\n\n\nx0\nlist\n\nStarting point of walk (in 2d)\n\n\nLs\nlist\n\nBoundary position\n\n\ntraj_generator\nCallable\n\nFunction generating the walk (must start in x0 = zero)\n\n\nmax_loop\nint\n5\nMax number of while loop if some walks do not reach the boundary\n\n\nargs_generator\nVAR_KEYWORD\n\n\n\n\n\n\n\nResults\n\nDiscrete walking\nIn this case we are already doing constant velocities, hence we can use the generator below:\n\nN = int(1e4)\nT = int(1e2)\nx0 = [1, 1]\nLs = np.linspace(5, 150, 20)\n\nreps = 10\nresults = np.array(Parallel(n_jobs=10)(delayed(mfpt_rw_2D)(N = N, T = T, x0 = x0, Ls = Ls, \n                                                           traj_generator = constant_velocity_generator_2D,\n                                                           time_sampler = single_steps)\n                for _ in tqdm(range(reps))), dtype = object)\n\n\nmean = results.mean(0)\n\n\nplt.plot(Ls, mean[0]*(Ls/Ls[0])**(1/2),c = 'k', label = r'$\\sim L^{1/2}$')\nplt.plot(Ls, mean[0]*(Ls/Ls[0]), c = 'k',ls = '--', label = r'$\\sim L$')\n# plt.loglog(Ls, mean,'o', alpha = 0.8)\nplt.errorbar(Ls, mean, yerr = results.astype(float).std(0))\n\nplt.legend()\n\n# plt.xlim(xmax = 150)\n\n<matplotlib.legend.Legend>\n\n\n\n\n\n\n\n\nExponential\n\nN = int(1e2)\nT = int(1e4)\nx0 = [0.5, 0.5]\nLs = np.arange(15, 100)\n\nreps = 20\nresults = np.array(Parallel(n_jobs=20)(delayed(mfpt_rw_2D)(N = N, T = T, x0 = x0, Ls = Ls,\n                                                           traj_generator = constant_velocity_generator_2D,\n                                                           time_sampler = exp_time_generator)\n                for _ in tqdm(range(reps))), dtype = object)\n\n\nmean_exp = results.mean(0)\n\nplt.plot(Ls, mean_exp[0]*(Ls/Ls[0])**(1/2),c = 'k', label = r'$\\sim L^2$')\nplt.plot(Ls, mean_exp[0]*(Ls/Ls[0]), c = 'k',ls = '--', label = r'$\\sim L$')\nplt.loglog(Ls, mean_exp,'o', alpha = 0.3, label = 'Exponential')\nplt.loglog(Ls, mean,'o', alpha = 0.3, label = 'Discrete')\nplt.legend()\nplt.xlabel('MFPT'); plt.ylabel('L'); plt.title(fr'$x_0 =$ {x0}')\n\nText(0.5, 1.0, '$x_0 =$ [0.5, 0.5]')\n\n\n\n\n\n\n\nBiexponential\n\nUninformed\n\nN = int(1e3)\nT = int(1e5)\nx0 = [0.5, 0.5]\nLs = np.arange(5, 30)\n\nreps = 20*4\nresults = np.array(Parallel(n_jobs=5)(delayed(mfpt_rw_2D)(N = N, T = T, x0 = x0, Ls = Ls,\n                                                        traj_generator = constant_velocity_generator_2D,\n                                                        time_sampler = biexp_time_generator, w1 = 0.5, d1 = 1, d2 = 5)\n                for _ in tqdm(range(reps))), dtype = object)\n\nmean_uninf_biexp = results.mean(0)\n\n\nplt.plot(Ls, (Ls/Ls[0])**(1/2),c = 'k', label = r'$\\sim \\sqrt{L}$')\n# plt.plot(Ls, (Ls/Ls[0]), c = 'k',ls = '--', label = r'$\\sim L$')\n\nplt.loglog(Ls, mean_uninf_biexp/mean_uninf_biexp[0],'o', alpha = 0.3, label = 'Uninformed bi-exponential')\nplt.loglog(Ls, mean_exp/mean_exp[0],'o', alpha = 0.3, label = 'Exponential')\nplt.loglog(Ls, mean/mean[0],'o', alpha = 0.3, label = 'Discrete')\nplt.legend()\nplt.ylabel('MFPT'); plt.xlabel('L'); plt.title(fr'$x_0 =$ {x0}')\n\nText(0.5, 1.0, '$x_0 =$ [0.5, 0.5]')\n\n\n\n\n\n\nplt.loglog(Ls, mean_uninf_biexp,'o', alpha = 0.3, label = 'Uninformed bi-exponential')\nplt.loglog(Ls, mean_exp,'o', alpha = 0.3, label = 'Exponential')\nplt.loglog(Ls, mean,'o', alpha = 0.3, label = 'Discrete')\nplt.legend()\nplt.ylabel('MFPT'); plt.xlabel('L'); plt.title(fr'$x_0 =$ {x0}')\n\nText(0.5, 1.0, '$x_0 =$ [0.5, 0.5]')\n\n\n\n\n\n\n\nInformed\n\nsource\n\n\n\nmfpt_informed_rw_2D\n\n mfpt_informed_rw_2D (N:int, T:int, x0:list, Ls:list,\n                      traj_generator:Callable, max_loop=5,\n                      **args_generator)\n\nCalculates MFPT to boundaries at 0 and L starting in x0 in 2D\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nN\nint\n\nNumber of trajectories\n\n\nT\nint\n\nLengths of trajectories\n\n\nx0\nlist\n\nStarting point of walk (in 2d)\n\n\nLs\nlist\n\nBoundary position\n\n\ntraj_generator\nCallable\n\nFunction generating the walk (must start in x0 = zero)\n\n\nmax_loop\nint\n5\nMax number of while loop if some walks do not reach the boundary\n\n\nargs_generator\nVAR_KEYWORD\n\n\n\n\n\n\nN = int(1e2)\nT = int(1e4)\nx0 = 3\nLs = np.arange(15, 100)\n\nreps = 20\nresults = np.array(Parallel(n_jobs=10)(delayed(mfpt_informed_rw)(N = N, T = T, x0 = x0, Ls = Ls, \n                                                                 traj_generator = constant_velocity_generator,\n                                                                 time_sampler = biexp_time_generator)\n                for _ in tqdm(range(reps))), dtype = object)\n\nmean_biexp = results.mean(0)\n\n\nplt.plot(Ls, (Ls/Ls[0])**(1/2),c = 'k', label = r'$\\sim \\sqrt{L}$')\nplt.plot(Ls, (Ls/Ls[0]), c = 'k',ls = '--', label = r'$\\sim L$')\n\n\nplt.loglog(Ls, mean_biexp/mean_biexp[0],'o', alpha = 0.3, label = 'Informed bi-exponential')\nplt.loglog(Ls, mean_uninf_biexp/mean_uninf_biexp[0],'o', alpha = 0.3, label = 'Uninformed bi-exponential')\nplt.loglog(Ls, mean_exp/mean_exp[0],'o', alpha = 0.3, label = 'Exponential')\nplt.loglog(Ls, mean/mean[0],'o', alpha = 0.3, label = 'Discrete')\nplt.legend()\nplt.ylabel('MFPT'); plt.xlabel('L'); plt.title(fr'$x_0 =$ {x0}')\n\n\n\nPower-law\n\nN = int(1e3)\nT = int(1e4)\nx0 = [0.5, 0.5]\nLs = np.arange(15, 100)\n\nreps = 20\nbeta = 1\n\nresults = np.array(Parallel(n_jobs=20)(delayed(mfpt_rw_2D)(N = N, T = T, x0 = x0, Ls = Ls, \n                                                        traj_generator = constant_velocity_generator_2D,\n                                                        time_sampler = pdf_discrete_sample, \n                                                           pdf_func=pdf_powerlaw,\n                                                           L = np.arange(1, T), beta = beta,)\n                for _ in tqdm(range(reps))), dtype = object)\n\nmean_pw = results.mean(0)\n\n\nfig, ax = plt.subplots(1, 2, figsize = (10, 5))\n\nax[0].plot(Ls, (Ls/Ls[0])**(1/2),c = 'k', label = r'$\\sim \\sqrt{L}$')\nax[0].plot(Ls, (Ls/Ls[0]), c = 'k',ls = '--', label = r'$\\sim L$')\npw_scaling = np.log(Ls)*Ls**(1/2)\nax[0].plot(Ls, pw_scaling/pw_scaling[0], c = 'k',ls = ':', label = r'$\\sim \\sqrt{L}\\ln{L}$')\n\nax[0].loglog(Ls, mean_pw/mean_pw[0],' o', alpha = 0.3, label = r'Power-law $\\alpha = 1$')\n# ax[0].loglog(Ls, mean_biexp/mean_biexp[0],'o', alpha = 0.3, label = 'Informed bi-exponential')\nax[0].loglog(Ls, mean_uninf_biexp/mean_uninf_biexp[0],'o', alpha = 0.3, label = 'Uninformed bi-exponential')\nax[0].loglog(Ls, mean_exp/mean_exp[0],'o', alpha = 0.3, label = 'Exponential')\nax[0].loglog(Ls, mean/mean[0],'o', alpha = 0.3, label = 'Discrete')\nax[0].legend()\n\n\nax[1].loglog(Ls, mean_pw,' o', alpha = 0.3, label = r'Power-law $\\alpha = 1$')\n# ax[1].loglog(Ls, mean_biexp,'o', alpha = 0.3, label = 'Informed bi-exponential')\nax[1].loglog(Ls, mean_uninf_biexp,'o', alpha = 0.3, label = 'Uninformed bi-exponential')\nax[1].loglog(Ls, mean_exp,'o', alpha = 0.3, label = 'Exponential')\nax[1].loglog(Ls, mean,'o', alpha = 0.3, label = 'Discrete')\n\n\n\nplt.setp(ax, ylabel = 'MFPT', xlabel = 'L', title = fr'$x_0 =$ {x0}')\n\n[Text(0, 0.5, 'MFPT'),\n Text(0.5, 0, 'L'),\n Text(0.5, 1.0, '$x_0 =$ [0.5, 0.5]'),\n Text(0, 0.5, 'MFPT'),\n Text(0.5, 0, 'L'),\n Text(0.5, 1.0, '$x_0 =$ [0.5, 0.5]')]"
  },
  {
    "objectID": "lib_nbs/dev_rl_framework_numba.html",
    "href": "lib_nbs/dev_rl_framework_numba.html",
    "title": "Numba / Dev Reinforcement learning framework",
    "section": "",
    "text": "This notebook gathers the functions creating different kinds of environments for foraging and target search in various scenarios."
  },
  {
    "objectID": "lib_nbs/dev_rl_framework_numba.html#isbetween",
    "href": "lib_nbs/dev_rl_framework_numba.html#isbetween",
    "title": "Numba / Dev Reinforcement learning framework",
    "section": "isBetween",
    "text": "isBetween\n\nsource\n\nisBetween_c_Vec_numba\n\n isBetween_c_Vec_numba (a, b, c, r)\n\nChecks whether point c is crossing the line formed with point a and b.\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\na\ntensor, shape = (1,2)\nPrevious position.\n\n\nb\ntensor, shape = (1,2)\nCurrent position.\n\n\nc\ntensor, shape = (Nt,2)\nPositions of all targets.\n\n\nr\nint/float\nTarget radius.\n\n\nReturns\narray of boolean values\nTrue at the indices of found targets.\n\n\n\n\ncompiling = isBetween_c_Vec_numba(np.array([0.1,1]), np.array([1,3]), np.random.rand(100,2), 0.00001)\n\n\n\n\n4.48 µs ± 16.7 ns per loop (mean ± std. dev. of 7 runs, 100,000 loops each)\n\n\n\nfrom rl_opts.utils import isBetween_c_Vec as oldbetween\n\n\n\n\n46.9 µs ± 442 ns per loop (mean ± std. dev. of 7 runs, 10,000 loops each)"
  },
  {
    "objectID": "lib_nbs/dev_rl_framework_numba.html#pareto-sampling",
    "href": "lib_nbs/dev_rl_framework_numba.html#pareto-sampling",
    "title": "Numba / Dev Reinforcement learning framework",
    "section": "Pareto sampling",
    "text": "Pareto sampling\n\nsource\n\npareto_sample\n\n pareto_sample (alpha, xm, size=1)"
  },
  {
    "objectID": "lib_nbs/dev_rl_framework_numba.html#random-sampling-from-array-with-probs",
    "href": "lib_nbs/dev_rl_framework_numba.html#random-sampling-from-array-with-probs",
    "title": "Numba / Dev Reinforcement learning framework",
    "section": "Random sampling from array with probs",
    "text": "Random sampling from array with probs\n\nsource\n\nrand_choice_nb\n\n rand_choice_nb (arr, prob)\n\n:param arr: A 1D numpy array of values to sample from. :param prob: A 1D numpy array of probabilities for the given samples. :return: A random sample from the given array with a given probability."
  },
  {
    "objectID": "lib_nbs/dev_rl_framework_numba.html#targetenv",
    "href": "lib_nbs/dev_rl_framework_numba.html#targetenv",
    "title": "Numba / Dev Reinforcement learning framework",
    "section": "TargetEnv",
    "text": "TargetEnv\n\nsource\n\nTargetEnv\n\n TargetEnv (*args, **kwargs)\n\nClass defining the foraging environment. It includes the methods needed to place several agents to the world.\n\nRuntime testing\n\nenv = TargetEnv(Nt = 1000,\n                 L = 123,\n                 r = 50,\n                 lc = np.array([[0.1],[1]]),\n                 lc_distribution = 'pareto')\ncompiling = env.check_encounter()\n\n\n\n\n15.5 µs ± 892 ns per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n\n\n\nenv = TargetEnv(Nt = 1000,\n                 L = 123,\n                 r = 50,\n                 lc = np.array([[0.1, 0.3, 0.5, 0.5],[0.8, 0.1, 0.05, 0.05]]))\ncompiling = env.check_encounter()\n\n\n\n\n12 µs ± 7.38 ns per loop (mean ± std. dev. of 7 runs, 100,000 loops each)\n\n\n\nfrom rl_opts.rl_framework.legacy import TargetEnv as oldEnv\n\n\noenv = oldEnv(Nt = 100,\n                 L = 123,\n                 r = 0.2,\n                 lc = 1)\n\n\n\n\n/home/gorka/github/fork_rl_opts/rl_opts/utils.py:36: RuntimeWarning: invalid value encountered in divide\n  mask[np.argwhere(np.abs(np.cross(b-a, c-a))/np.linalg.norm(b-a) > r)] = False\n\n\n232 µs ± 388 ns per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n\n\n\n@jitclass\nclass move():\n    env : TargetEnv\n\n    def __init__(self,\n                 Nt = 1000,\n                 L = 123.0,\n                 r = 0.2,\n                 lc = 1.0,\n                 TIME_EP = 10):\n\n        self.env = TargetEnv(Nt, L, r, np.array([[0.1, 0.3, 0.5, 0.5],[0.8, 0.1, 0.05, 0.05]]), 1, 1, False, 'power_law')\n    \n    def run(self, t):\n        for time in range(t):\n            self.env.update_pos(False)\n            self.env.check_encounter()\n            self.env.check_bc()\n\n\nk = move()\nonerun = k.run(5)\n\n\n\n\n123 ms ± 61.2 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n\n\n\nfrom rl_opts.rl_framework.legacy import TargetEnv as oldEnv\n\n\noenv = oldEnv(Nt = 1000,\n                 L = 123,\n                 r = 0.2,\n                 lc = 1)\n\n\ndef old_run(t):\n    for t in range(t):\n        oenv.update_pos(False)\n        oenv.check_encounter()\n        oenv.check_bc()\n\n\n\n\n2.85 s ± 48.2 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n\n\n\n\n\nWalk from policy\n\nsource\n\n\nsingle_agent_walk\n\n single_agent_walk (N_runs, time_ep, policy, env)\n\n\nsource\n\n\nmulti_agents_walk\n\n multi_agents_walk (N_runs, time_ep, N_agents, Nt=100, L=100, r=0.5,\n                    lc=array([[1.],        [1.]]), num_agents=1,\n                    agent_step=1, destructive_targets=False,\n                    lc_distribution='constant', policy=array([[1, 1, 1, 1,\n                    1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n                    1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n                    1, 1, 1, 1,         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n                    1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,         1, 1, 1, 1,\n                    1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n                    1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],        [0, 0, 0,\n                    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                    0,         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                    0, 0, 0, 0, 0, 0, 0, 0,         0, 0, 0, 0, 0, 0, 0,\n                    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                    0, 0, 0, 0,         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                    0]]))"
  },
  {
    "objectID": "lib_nbs/dev_rl_framework_numba.html#resetenv",
    "href": "lib_nbs/dev_rl_framework_numba.html#resetenv",
    "title": "Numba / Dev Reinforcement learning framework",
    "section": "ResetEnv",
    "text": "ResetEnv\n\n1D\n\nsource\n\n\nResetEnv_1D\n\n ResetEnv_1D (*args, **kwargs)\n\nInitialize self. See help(type(self)) for accurate signature.\n\nsource\n\n\nmulti_loop_exp\n\n multi_loop_exp (T, rates, L, D)\n\n\nsource\n\n\nmulti_loop_constant\n\n multi_loop_constant (T, resets, L, D)\n\n\nsource\n\n\nreset_search_loop\n\n reset_search_loop (T, reset_policy, env)\n\n\nL = 10.0; D = 1/2; T = int(1e3)\n\nresets = np.linspace(70, 150, 40).astype(np.int64)\n\n\n\n\n121 µs ± 46.6 µs per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n\n\n\n\n\n112 µs ± 27.3 µs per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n\n\n\n\n2D\n\nsource\n\n\nResetEnv_2D\n\n ResetEnv_2D (*args, **kwargs)\n\nInitialize self. See help(type(self)) for accurate signature.\n\nsource\n\n\nmulti_loop_policies_2D\n\n multi_loop_policies_2D (T, reset_policies, dist_target, radius_target, D)\n\n\nsource\n\n\nmulti_loop_exp_2D\n\n multi_loop_exp_2D (T, rates, dist_target, radius_target, D)\n\n\nsource\n\n\nmulti_loop_constant_2D\n\n multi_loop_constant_2D (T, resets, dist_target, radius_target, D)\n\n\n# from rl_opts.rl_framework_numba import multi_loop_constant_2D\n\nfrom rl_opts.rl_framework_numba import isBetween_c_Vec_numba, reset_search_loop\n\n\nresets = (3*np.arange(10)+1).astype(int)\nT = int(1e6); dist_target = 5; radius_target = 1; D = 1\nplt.plot(resets, multi_loop_constant_2D(T, resets, dist_target, radius_target, D))\n\n\n\n\n\nLoop saving position\n\nsource\n\n\n\nreset_loop_saving_positions_2D\n\n reset_loop_saving_positions_2D (n_agents, T, reset_policy, dist_target,\n                                 radius_target, D)\n\n\nfrom rl_opts.rl_framework_numba import ResetEnv_2D\n\n\nenv = ResetEnv_2D(dist_target = 5, radius_target = 1, D = 1)   \nreset = 1000\nreset_policy = np.zeros(reset)\nreset_policy[reset-1] = 1 \n# reset_policy[int(reset/2)] = 0.5\ndist_target = 2; radius_target = 0.5; D = 0.1\ntarget_position = np.array([dist_target*np.cos(np.pi/4), dist_target*np.sin(np.pi/4)])\n\n\nT = 50\npositions = reset_loop_saving_positions_2D(n_agents = int(1e6), T = T, reset_policy = reset_policy, \n                                     dist_target = dist_target, radius_target = radius_target, D = D)\n\n\nbins = np.linspace(-dist_target, dist_target, 200)\n\nfor t in range(T)[::10]:\n    h, _ = np.histogram(pos[pos[:,t,0] != 0,t,0], bins = bins);\n\n    plt.plot(bins[1:], h/h.max())\n\n\n\n\n\n# import matplotlib.patches as patches\nbins = np.linspace(-7, 7, 100)\nt = 3\n# plt.scatter(target_position[0], target_position[1], s = 10, c = 'r', zorder = 10)\nplt.hist2d(pos[pos[:,t,0] != 0,t,0], pos[pos[:,t,0] != 0,t,1], bins = bins, cmap = 'Oranges');\n\ncircle = patches.Circle(target_position, radius_target, edgecolor='r', facecolor='none')\nplt.gca().add_patch(circle)\n\n# Adjust the aspect ratio\nplt.gca().set_aspect('equal', adjustable='box')\n\n\n\n\n\nAnimation\n\nsource\n\n\n\nanimate_positions_with_target\n\n animate_positions_with_target (bins, positions, radius_target,\n                                target_position, cmap='Oranges')\n\n\nbins = np.linspace(-10, 10, 1000)\n\nani = animation(bins, positions, radius_target, target_position, cmap = 'Greys')\n# Display the animation\nHTML(ani.to_jshtml())"
  },
  {
    "objectID": "lib_nbs/dev_rl_framework_numba.html#move-reset-env",
    "href": "lib_nbs/dev_rl_framework_numba.html#move-reset-env",
    "title": "Numba / Dev Reinforcement learning framework",
    "section": "Move + Reset Env",
    "text": "Move + Reset Env\n\n1D\n\nsource\n\n\nMoveResetEnv_1D\n\n MoveResetEnv_1D (*args, **kwargs)\n\nInitialize self. See help(type(self)) for accurate signature.\n\nenv = MoveResetEnv_1D(L = 15, step_length = 1)\n\npos = []\nfor time in range(10000):\n    pos.append(env.position)\n    env.update_pos(False if np.random.rand() > 0.5 else True, \n                   True if time % 2500 == 0 else False)\n\n\nplt.plot(pos)\nplt.axhline(env.L)\n\n<matplotlib.lines.Line2D>\n\n\n\n\n\n\nNo-train search loops\n\nsource\n\n\n\nmulti_loop_MoveReset_allfixed\n\n multi_loop_MoveReset_allfixed (T, resets, turns, L, step_length)\n\n\nsource\n\n\nMoveReset_allfixed\n\n MoveReset_allfixed (T, reset, turn, env)\n\n\nenv = MoveResetEnv_1D(L = 5, step_length = 1)\n\nT = int(1e7)\nreset = 500\nturns = np.arange(10, 20)\nL = 5.0\nstep_length = 1.0\n\n\nenv = MoveResetEnv_1D(L = L, step_length = step_length)\n\n#rewards = MoveReset_allfixed(T = T, reset = reset, turn = turns[0], env = env)\n\n\nrews = multi_loop_MoveReset_allfixed(T = int(1e8), resets = np.array([500]), turns = np.arange(5, 10), L = 16.0, step_length = 1.0)\n\n\nplt.plot(rews[0]/T)\n\n\n\n\n\n\n2D\n\nsource\n\n\nMoveResetEnv_2D\n\n MoveResetEnv_2D (*args, **kwargs)\n\nOpen question: here we consider no boundaries, to properly replicate 1D. But in the MoveReset environment there will be boundaries?\n\nL = 100\nenv = MoveResetEnv_2D(dist_target = 2)\nT = 200\npos = np.zeros((2, T))\ntime_enc = []\nfor time in (range(T)):\n    pos[:, time] = env.position\n    rew = env.update_pos(False if np.random.rand() > 0.5 else True, True if time % 500 == 0 else False)\n    if rew == 1:\n        time_enc.append(time)\n\nprint(len(time_enc))\n\n1\n\n\n\nfig, ax = plt.subplots()\nax.plot(pos[0], pos[1])\n\ntarget = plt.Circle(env.target_position[0], env.r, color='C1')\nax.add_patch(target)\n\nfor t in time_enc:\n    plt.plot(pos[0, t:t+2], pos[1, t:t+2], c = 'k', lw = 2)\n\n\n'''\nSee that we are seeing here is the reseting step, because the real step in which we crossed the target is not recorded\nbecause we do a env.init_env() when crossing the target and the one that gets out of the function is init_position\n'''\n\n'\\nSee that we are seeing here is the reseting step, because the real step in which we crossed the target is not recorded\\nbecause we do a env.init_env() when crossing the target and the one that gets out of the function is init_position\\n'\n\n\n\n\n\n\nMulti-target\n\nsource\n\n\n\nMoveResetEnv_multiTarget_2D\n\n MoveResetEnv_multiTarget_2D (*args, **kwargs)\n\nInitialize self. See help(type(self)) for accurate signature.\n\nL = 100\nenv = MoveResetEnv_2D(L = L, Nt = 100, init_position=np.array([L/2, L/2]))\nT = 200\npos = np.zeros((2, T))\ntarget = []\nfor time in (range(T)):\n    pos[:, time] = env.position[0]\n    env.update_pos(False if np.random.rand() > 0.5 else True, True if time % 500 == 0 else False)\n    if env.check_encounter() == 1: target.append(time)\n    #env.check_encounter()\n    env.check_bc()\n\nIf you want to consider cases with l_c, use MoveResetEnv_withlc_2D\n\n\n\nplt.plot(pos[0])\nfor t in target: plt.axvline(t+1, c = 'k', alpha = 0.3)\nplt.axhline(50, c = 'k', alpha = 0.3)\n\n<matplotlib.lines.Line2D>\n\n\n\n\n\n\nWith l_c\n\nsource\n\n\n\nMoveResetEnv_withlc_2D\n\n MoveResetEnv_withlc_2D (*args, **kwargs)\n\nClass defining the foraging environment. It includes the methods needed to place several agents to the world.\n\nL = 100\nenv = MoveResetEnv_2D(L = L, Nt = 100, init_positions=np.array([[L/2, L/2]]), lc_distribution = 'none')\nT = 200\npos = np.zeros((2, T))\ntarget = []\nfor time in tqdm(range(T)):\n    pos[:, time] = env.positions[0]\n    env.update_pos(False if np.random.rand() > 0.5 else True, True if time % 500 == 0 else False)\n    if env.check_encounter() == 1: target.append(time)\n    #env.check_encounter()\n    env.check_bc()\n\n\n\n\n\nplt.plot(pos[0])\nfor t in target: plt.axvline(t+1, c = 'k', alpha = 0.3)\nplt.axhline(50, c = 'k', alpha = 0.3)\n\n<matplotlib.lines.Line2D>\n\n\n\n\n\n\nNo-train search loops\n\nBase MoveRest_2D\n\nsource\n\n\n\n\nmulti_loop_MoveReset2D_allfixed\n\n multi_loop_MoveReset2D_allfixed (T, resets, turns, dist_target,\n                                  radius_target, agent_step)\n\n\nsource\n\n\nMoveReset2D_allfixed\n\n MoveReset2D_allfixed (T, reset, turn, env)\n\n\nfrom rl_opts.rl_framework_numba import MoveResetEnv_2D\n\n\nT = 1000; reset = 5; turn = 2;\nenv = MoveResetEnv_2D()\nrews = MoveReset2D_allfixed(T, reset, turn, env)\n\n\nrews = multi_loop_MoveReset2D_allfixed(T = int(1e4), resets = np.array([500]), turns = np.arange(2, 10),  \n                                       dist_target = 5, radius_target = 1.5, agent_step = 1.3)\n\n\nplt.plot(rews[0]/T)\n\n\n\n\n\nMulti target\n\nsource\n\n\n\nmulti_loop_MoveReset2D_multitarget_allfixed\n\n multi_loop_MoveReset2D_multitarget_allfixed (T, resets, turns, L, Nt, r,\n                                              step_length, init_position)\n\n\nsource\n\n\nMoveReset2D_multitarget_allfixed\n\n MoveReset2D_multitarget_allfixed (T, reset, turn, env)\n\n\nT = 1000; reset = 5; turn = 2;\nenv = MoveResetEnv_2D()\nrews = MoveReset2D_allfixed(T, reset, turn, env)\n\n\nrews = multi_loop_MoveReset2D_multitarget_allfixed(T = int(1e7), resets = np.array([500]), turns = np.arange(2, 10), L = L, \n                                       Nt = Nt, r = 1, init_position = np.array([L/2, L/2]),step_length = step_length)\n\n\nplt.plot(rews[0]/T)"
  },
  {
    "objectID": "lib_nbs/dev_rl_framework_numba.html#base-agent",
    "href": "lib_nbs/dev_rl_framework_numba.html#base-agent",
    "title": "Numba / Dev Reinforcement learning framework",
    "section": "BASE agent",
    "text": "BASE agent\n\nsource\n\nPSAgent\n\n PSAgent (*args, **kwargs)\n\nBase class of a Reinforcement Learning agent based on Projective Simulation, with two-layered network. This class has been adapted from https://github.com/qic-ibk/projectivesimulation\n\nps = PSAgent(num_actions = 10, num_percepts_list = np.array([15]))\nps.percept_preprocess([0]*ps.num_percepts_list)\nps.probability_distr(0)\nobservation = [0]*ps.num_percepts_list[0]\nps.deliberate(np.array(observation))\nps.learn(1)\nps.reset_g()\nps.deliberate_fixed_policy(np.array(observation))\n\nNo fixed policy was given to the agent. The action will be selected randomly.\n\n\n7"
  },
  {
    "objectID": "lib_nbs/dev_rl_framework_numba.html#forager",
    "href": "lib_nbs/dev_rl_framework_numba.html#forager",
    "title": "Numba / Dev Reinforcement learning framework",
    "section": "Forager",
    "text": "Forager\n\nsource\n\nForager\n\n Forager (*args, **kwargs)\n\n*Same as PSAGENT but: num_percepts_list -> state_space\nstate_space : list List where each entry is the state space of each perceptual feature. E.g. [state space of step counter, state space of density of successful neighbours].*\n\nagent = Forager(num_actions = 2, state_space = np.array([np.arange(100)]))\nagent.percept_preprocess([0]*agent.num_percepts_list)\nagent.probability_distr(0)\nobservation = [0]*agent.num_percepts_list[0]\nagent.deliberate(np.array(observation))\nagent.learn(1)\nagent.reset_g()\nagent.deliberate_fixed_policy(np.array(observation))\nagent.act(0)\nagent.get_state()\n\nNo fixed policy was given to the agent. The action will be selected randomly.\n\n\narray([1])"
  },
  {
    "objectID": "lib_nbs/dev_rl_framework_numba.html#forager-with-efficient-h-update",
    "href": "lib_nbs/dev_rl_framework_numba.html#forager-with-efficient-h-update",
    "title": "Numba / Dev Reinforcement learning framework",
    "section": "Forager with efficient H update",
    "text": "Forager with efficient H update\nWe use the formula \\(H_{t+i} = (1-\\gamma)^i H_t + \\gamma H_0 \\sum_{j=1}^i(1-\\gamma)^{j-1}\\)\n\nsource\n\nForager_efficient_H\n\n Forager_efficient_H (*args, **kwargs)\n\n*Same as PSAGENT but: num_percepts_list -> state_space\nstate_space : list List where each entry is the state space of each perceptual feature. E.g. [state space of step counter, state space of density of successful neighbours].*\nTesting\n\ndef test_train_loop_Heff(efficient, agent, episodes):\n    \n    for i in range(episodes):\n        \n        if efficient:\n            agent.counter_upd += 1\n        \n        state = np.array([i])\n        \n        if i % 2 == 0:\n            action = 0\n        else: 1\n        \n        # here is where glow matrix updates:\n        agent.g_matrix = (1 - agent.eta_glow_damping) * agent.g_matrix\n        agent.g_matrix[action, i] += 1 #record latest decision in g_matrix\n        \n        if i == 2 or i == 6:\n            reward = 1\n        else: reward = 0\n        \n        if efficient:\n            if reward == 1:\n                agent._learn_post_reward(reward)\n        else:\n            agent.learn(reward)\n            \n    return agent\n\nValue testing\n\neps = 100\nagent_noopt = Forager(num_actions = 2,\n                    state_space = np.array([np.arange(eps)]))\ntrained_noopt = test_train_loop_Heff(efficient = False, agent = agent_noopt, episodes = eps)\ntrained_noopt.h_matrix\n\nagent_opt = Forager_efficient_H(num_actions = 2,\n                              state_space = np.array([np.arange(eps)]))\ntrained = test_train_loop_Heff(efficient=True, agent = agent_opt, episodes = eps)\n\nf'comparison old and efficient: {(trained.h_matrix-trained_noopt.h_matrix).sum()} ||||| IF value != 0, something is wrong!!!'\n\n'comparison old and efficient: 0.0 ||||| IF value != 0, something is wrong!!!'"
  },
  {
    "objectID": "lib_nbs/dev_rl_framework_numba.html#forager-with-efficient-g-and-h-update",
    "href": "lib_nbs/dev_rl_framework_numba.html#forager-with-efficient-g-and-h-update",
    "title": "Numba / Dev Reinforcement learning framework",
    "section": "Forager with efficient G and H update",
    "text": "Forager with efficient G and H update\n\nThis is full efficient, both w.r.t. G and H\n\n\nsource\n\nForager_efficient\n\n Forager_efficient (*args, **kwargs)\n\n*Updated version of the FORAGER class, with an efficient update both for the H-matrix and the G-matrix.\nsize_state_space : np.array num of percepts for each feature*\n\nValue testing\n\nWe replicate a training with the original agent and the efficient one to check that the resulting h-matrix is equal. Note that because the deliberate is random, the value \\(!=1\\) in h_matrix may be in different rows (actions) for the two agents (but always on the same column, i.e. state).\n\n\nfrom rl_opts.rl_framework_numba import Forager_efficient, Forager\n\n\ngamma, eta = 0.5, 0.5\nsteps = 5\nsize_state_space = 10\n\nag_og = Forager(num_actions = 2, state_space = np.array([np.arange(size_state_space)]),\n                gamma_damping=gamma, \n                eta_glow_damping=eta\n               )\n\nag_ef = Forager_efficient(num_actions = 2, size_state_space = np.array([size_state_space]),\n                              eta_glow_damping = eta, gamma_damping = gamma)\n\nfor i in range(steps-1):\n    ag_og.learn(0)\nag_og.deliberate(np.array([i]))\nag_og.learn(1)\nfor i in range(steps-1):\n    ag_og.learn(0)\n\nag_ef.N_upd_H = steps-1\nag_ef.N_upd_G = steps-1\nag_ef._learn_post_reward(0)\n\nag_ef.N_upd_H += 1\nag_ef.N_upd_G += 1\nag_ef.deliberate(np.array([i]))\nag_ef._learn_post_reward(1)\n\nag_ef.N_upd_H = steps-1\nag_ef.N_upd_G = steps-1\nag_ef._learn_post_reward(0)\n\nag_og.h_matrix, ag_ef.h_matrix\n\n(array([[1.    , 1.    , 1.    , 1.    , 1.    , 1.    , 1.    , 1.    ,\n         1.    , 1.    ],\n        [1.    , 1.    , 1.    , 1.0625, 1.    , 1.    , 1.    , 1.    ,\n         1.    , 1.    ]]),\n array([[1.    , 1.    , 1.    , 1.    , 1.    , 1.    , 1.    , 1.    ,\n         1.    , 1.    ],\n        [1.    , 1.    , 1.    , 1.0625, 1.    , 1.    , 1.    , 1.    ,\n         1.    , 1.    ]]))\n\n\nRuntime testing\n\neps = int(1e4); eta = 0.1\nagent_noopt = Forager(num_actions = 2,\n                      state_space = np.array([np.arange(eps)]), eta_glow_damping = eta)\nagent_opt = Forager_efficient(num_actions = 2,\n                              state_space = np.array([np.arange(eps)]), eta_glow_damping = eta)\n\n\n\n\n22.7 ms ± 78.8 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n\n\n\n\n\n367 ms ± 7.43 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)"
  },
  {
    "objectID": "lib_nbs/dev_rl_framework_numba.html#forager-with-action-dependent-glow-and-damping",
    "href": "lib_nbs/dev_rl_framework_numba.html#forager-with-action-dependent-glow-and-damping",
    "title": "Numba / Dev Reinforcement learning framework",
    "section": "Forager with action dependent glow and damping",
    "text": "Forager with action dependent glow and damping\n\nsource\n\nForager_multi_param\n\n Forager_multi_param (*args, **kwargs)\n\n*Same as Forager_efficient but with different glows and damping factors for each action\nsize_state_space : np.array num of percepts for each feature*\n\nTest prefactor calculation\n\neps = int(1e4); \netas = np.array([0.99,0.001])\ngammas = np.array([0.001, 0.001])\n\nagent = Forager_multi_param(num_actions = 2,\n                                 size_state_space = np.array([10]), \n                                 eta_glow_damping = etas, gamma_damping = gammas)\n                                 \nagent_og = Forager_efficient(num_actions = 2, size_state_space = np.array([10]),\n                              eta_glow_damping = etas[0], gamma_damping = gammas[0])\n\nassert (agent.prefactor_1[0,:] == agent_og.prefactor_1).all()\nassert (agent.prefactor_2[0,:] == agent_og.prefactor_2).all()\n\n\n\nTest update of h_matrix\n\nfrom rl_opts.rl_framework_numba import rand_choice_nb\n\n\netas = np.array([0.001]*2)\ngammas = np.array([0.001]*2)\n\nagent = Forager_multi_param(num_actions = 2,\n                                 size_state_space = np.array([10]), \n                                 eta_glow_damping = etas, gamma_damping = gammas)\n                                 \nagent_og = Forager_efficient(num_actions = 2, size_state_space = np.array([10]),\n                              eta_glow_damping = etas[0], gamma_damping = gammas[0])\n\n\nh_mat = np.zeros((2, agent.size_state_space[0]))\n\nh_mat[0,:] = 1\n\n\n# h_mat[0,:] = np.random.randint(2, size = agent.size_state_space[0])\n# h_mat[1,:] = np.abs(h_mat[0,:]-1)\n\n\nagent.h_matrix = h_mat.copy()\nagent_og.h_matrix = h_mat.copy()\n\n\n\nacs = []\nfor a in [agent, agent_og]:\n    \n\n    a.N_upd_H = 0\n    a.N_upd_G = 0\n    ac = []\n    for i in range(5):\n    \n        a.N_upd_H += 1\n        a.N_upd_G += 1\n    \n        action = a.deliberate(np.array([i]))\n        ac.append(action)\n    \n        if i == 2 or i == 6:\n            reward = 1\n        else: \n            reward = 0\n            \n        if reward == 1:\n            a._learn_post_reward(reward)\n            \n    a._G_upd_full()\n    acs.append(ac)\n\nif acs[0] == acs[1]:\n    assert np.sum(agent_og.h_matrix - agent.h_matrix) < 1e-10\nelse:\n    print('actions didnt match :( this may be because of luck')\n\n\n\nTest different gammas\n\netas = np.array([0.1]*2)\ngammas = np.array([0.1, 0.001])\n\nagent = Forager_multi_param(num_actions = 2,\n                                 size_state_space = np.array([10]), \n                                 eta_glow_damping = etas, gamma_damping = gammas)\n\nagent.h_matrix *= 5\n\nagent.N_upd_H = 10\nagent.N_upd_G = 10\nagent._learn_post_reward(0)\n\nagent.h_matrix\n\narray([[2.39471376, 2.39471376, 2.39471376, 2.39471376, 2.39471376,\n        2.39471376, 2.39471376, 2.39471376, 2.39471376, 2.39471376],\n       [4.96017952, 4.96017952, 4.96017952, 4.96017952, 4.96017952,\n        4.96017952, 4.96017952, 4.96017952, 4.96017952, 4.96017952]])"
  },
  {
    "objectID": "lib_nbs/dev_rl_framework_numba.html#for-targetenv",
    "href": "lib_nbs/dev_rl_framework_numba.html#for-targetenv",
    "title": "Numba / Dev Reinforcement learning framework",
    "section": "For TargetEnv",
    "text": "For TargetEnv\n\nsource\n\ntrain_loop\n\n train_loop (episodes, time_ep, agent, env)\n\n\nsource\n\n\ntrain_loop_h_efficient\n\n train_loop_h_efficient (episodes, time_ep, agent, env, h_mat_allT=False)\n\n\nsource\n\n\ntrain_loop_full_efficient\n\n train_loop_full_efficient (episodes, time_ep, agent, env,\n                            h_mat_allT=False)\n\n\nsource\n\n\nrun_agents\n\n run_agents (episodes, time_ep, N_agents, Nt=100, L=100, r=0.5,\n             lc=array([[1.],        [1.]]), num_agents=1, agent_step=1,\n             destructive_targets=False, lc_distribution='constant',\n             num_actions=2, state_space=array([[ 0.,  1.,  2.,  3.,  4.,\n             5.,  6.,  7.,  8.,  9., 10., 11., 12.,         13., 14., 15.,\n             16., 17., 18., 19., 20., 21., 22., 23., 24., 25.,\n             26., 27., 28., 29., 30., 31., 32., 33., 34., 35., 36., 37.,\n             38.,         39., 40., 41., 42., 43., 44., 45., 46., 47.,\n             48., 49., 50., 51.,         52., 53., 54., 55., 56., 57.,\n             58., 59., 60., 61., 62., 63., 64.,         65., 66., 67.,\n             68., 69., 70., 71., 72., 73., 74., 75., 76., 77.,\n             78., 79., 80., 81., 82., 83., 84., 85., 86., 87., 88., 89.,\n             90.,         91., 92., 93., 94., 95., 96., 97., 98., 99.]]),\n             gamma_damping=1e-05, eta_glow_damping=0.1,\n             initial_prob_distr=array([], shape=(2, 0), dtype=float64),\n             policy_type='standard', beta_softmax=3,\n             fixed_policy=array([], shape=(2, 0), dtype=float64),\n             max_no_H_update=1000, efficient_agent=False,\n             h_mat_allT=False)\n\n\n\nTesting\n\n# Other similar exps: 'previous', 'previous_long'\n# Current exp:\nEXP = 'previous_pol_t'\n\n# Training spec\nTIME_EP = 20\nEPISODES = 12\nmultiplier_agents = 5\n\n# Environment \nNt = 100; L = 100; r = 0.5; \nlc_distribution = 'constant'\nlcs = [0.6]+np.linspace(1,10,10).tolist()\n\n# Agent\nmax_counter = int(2e3)\nstate_space = np.array([np.arange(max_counter)])\ngamma_damping = 0.00001\neta_glow_damping = 0.1\ninitial_prob_distr = (np.array([0.99, 0.01])*np.ones((2, max_counter)).transpose()).transpose()\n\n\nfor lc_value in (lcs):\n    \n    lc = np.array([[lc_value],[1.0]])\n\n    print(f'starting lc = {lc_value}')\n    \n    rews, mats = run_agents(episodes = EPISODES, time_ep = TIME_EP, N_agents = 1,#multiplier_agents*numba.get_num_threads(),\n                            Nt = Nt, L = L, r = r, \n                            lc = lc, lc_distribution = lc_distribution,\n                            state_space = state_space,\n                            gamma_damping = gamma_damping,\n                            eta_glow_damping = eta_glow_damping, \n                            initial_prob_distr = initial_prob_distr,\n                            efficient_agent = True,\n                            h_mat_allT=True\n                           )\n    print(f'saving lc = {lc_value}')\n\n    # np.save(f'../../results/constant_lc/EXP_{EXP}/h_mats_lc_{lc_value}.npy', mats)\n    # np.save(f'../../results/constant_lc/EXP_{EXP}/rewards_lc_{lc_value}.npy', rews)\n\nstarting lc = 0.6\nsaving lc = 0.6\nstarting lc = 1.0\nsaving lc = 1.0\nstarting lc = 2.0\nsaving lc = 2.0\nstarting lc = 3.0\nsaving lc = 3.0\nstarting lc = 4.0\nsaving lc = 4.0\nstarting lc = 5.0\nsaving lc = 5.0\nstarting lc = 6.0\nsaving lc = 6.0\nstarting lc = 7.0\nsaving lc = 7.0\nstarting lc = 8.0\nsaving lc = 8.0\nstarting lc = 9.0\nsaving lc = 9.0\nstarting lc = 10.0\nsaving lc = 10.0\n\n\n\n\nRuntime testing\n\nfrom rl_opts.rl_framework.legacyimport run_agents\nimport numba\nimport numpy as np\ntime_ep = 10\n\n\n# For compiling and checking\nrun_agents(episodes = 10, time_ep = time_ep, N_agents = numba.get_num_threads(), state_space = np.array([np.linspace(0, time_ep-1, time_ep)]), efficient_agent=False);\n\n\ntime_ep = 12000\n\n9.86 s ± 44.6 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n\n\n\ntime_ep = 12000\n\n2.33 s ± 11.3 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n\n\n\ntime_ep = 12000\n\n1.06 s ± 14.1 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)"
  },
  {
    "objectID": "lib_nbs/dev_rl_framework_numba.html#for-resetenv",
    "href": "lib_nbs/dev_rl_framework_numba.html#for-resetenv",
    "title": "Numba / Dev Reinforcement learning framework",
    "section": "For ResetEnv",
    "text": "For ResetEnv\n\n1D\n\nsource\n\n\ntrain_loop_reset\n\n train_loop_reset (episodes, time_ep, agent, env, h_mat_allT=False,\n                   when_save_h_mat=1, reset_after_reward=True)\n\n\n# from rl_opts.rl_framework_numba import Forager_efficient, ResetEnv_1D, ResetEnv_2D\n# import numpy as np \n\nenv = ResetEnv_1D(L = 5, D = 1/2)\nagent = Forager_efficient(num_actions = 2,\n                          size_state_space = np.array([100]))\nres = train_loop_reset(100, 100, agent, env)\n\n\nres = train_loop_reset(100, 100, agent, env)\n\n\nTest\n\nfrom rl_opts.rl_framework_numba import Forager_efficient, ResetEnv_1D, ResetEnv_2D\nimport numpy as np\n\n\ntime_ep = 10\n\n\nenv = ResetEnv_1D(L = 5, D = 1/2)\nagent = Forager_efficient(num_actions = 2,\n                          size_state_space = np.array([time_ep+1]))\n\n\n#initialize environment and agent's counter and g matrix\nenv.init_env()\nagent.agent_state = 0\nagent.reset_g()\n\nfor t in range(time_ep):\n    agent.N_upd_H += 1\n    agent.N_upd_G += 1\n\n    #get perception\n    state = agent.get_state()\n\n    \n    action = 0 if t != 5 else 1\n\n    percept = agent.percept_preprocess(state) \n    agent._G_upd_single_percept(percept, action)\n        \n    #act (update counter)\n    agent.act(action)\n\n    \n    reward = 0 if t < time_ep -1 else 1\n\nagent._learn_post_reward(reward)\n\n\nagent.g_matrix\n\narray([[2., 2., 2., 2., 1., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.]])\n\n\n\ndef train_loop_reset(episodes, time_ep, agent, env, h_mat_allT = False, when_save_h_mat = 1, reset_after_reward = True):  \n\n    if h_mat_allT: \n        policy_t = np.zeros((int(np.ceil(episodes/when_save_h_mat)), \n                             agent.h_matrix.shape[-1]))\n        idx_policy_save = 0\n        \n    save_rewards = np.zeros(episodes)\n    gmats = []\n    counters = []\n    tuples = []\n    for ep in range(episodes):\n        \n        #initialize environment and agent's counter and g matrix\n        agent.agent_state = 0\n        agent.reset_g()\n\n        position = 0\n        for t in range(time_ep):\n            agent.N_upd_H += 1\n            agent.N_upd_G += 1\n\n            #get perception\n            state = agent.get_state()\n            \n            # if we reached the maximum state space, we perform turn action\n            if state == agent.h_matrix.shape[-1]:\n                action = 1\n            # else we do as normal    \n            else: \n                action = agent.deliberate(state)\n\n            \n                \n            #act (update counter)\n            agent.act(action)\n\n            \n\n            #update positions\n            if action == 0:\n                position += 2*np.random.randint(2)-1       \n            else: \n                position = 0\n\n            # Checking if reward\n            if position >= env.L: \n                position = 0\n                reward = 1\n            else:\n                reward = 0\n\n            tuples.append([state[0].copy(), action, reward])\n                          \n\n            if reward == 1 or agent.N_upd_H == agent.max_no_H_update-1:\n                \n                agent._learn_post_reward(reward)\n\n                gmats.append(agent.g_matrix.copy())\n                counters.append(state)\n            \n            if reset_after_reward == True and reward != 0:\n                agent.agent_state = 0\n\n            # Saving\n            save_rewards[ep] += reward\n        if h_mat_allT and ep % when_save_h_mat == 0:\n            policy_t[idx_policy_save] = agent.h_matrix[0,:] / agent.h_matrix.sum(0)\n            idx_policy_save += 1\n      \n    # return (save_rewards/time_ep, policy_t) if h_mat_allT else (save_rewards/time_ep, agent.h_matrix)\n    return gmats, counters, tuples\n\n\nagent._G_upd_full??\n\n\nSignature: agent._G_upd_full()\nSource:   \n    def _G_upd_full(self):\n        '''Given the current number of steps without an update, updates the whole G-matrix.\n        Then, resets all counters.'''\n        self.g_matrix = (1 - self.eta_glow_damping)**(self.N_upd_G - self.last_upd_G) * self.g_matrix\n        self.N_upd_G = 0\n        self.last_upd_G = np.zeros((self.num_actions, self.num_percepts))\nFile:      /tmp/ipykernel_2749/2916760148.py\nType:      method\n\n\n\n\nidxl = 0\n\nidxg, idxe = 9, 5\nD = 1/2; \n\nLs = np.arange(5, 11)\n\ngammas = np.logspace(-9, -5.5, 10)\n# first round (not all L finished)\netas1 = np.linspace(0.05, 0.18, 10)\n# second round\netas2 = np.linspace(0.18, 0.3, 10)\netas = np.append(etas1, etas2)\n\n\netas\n\narray([0.05      , 0.06444444, 0.07888889, 0.09333333, 0.10777778,\n       0.12222222, 0.13666667, 0.15111111, 0.16555556, 0.18      ,\n       0.18      , 0.19333333, 0.20666667, 0.22      , 0.23333333,\n       0.24666667, 0.26      , 0.27333333, 0.28666667, 0.3       ])\n\n\n\nenv = ResetEnv_1D(L = Ls[idxl], D = D)\nagent = Forager_efficient(num_actions = 2,\n                          size_state_space = np.array([50]),\n                          gamma_damping=gammas[idxg],\n                          eta_glow_damping=etas[idxe],\n                          g_update = 'r')\n\n\nfrom rl_opts.rl_framework_numba import rand_choice_nb\n\n\ngmats, counters, tuples = train_loop_reset(10, int(1e3), agent, env)\ngmats = np.array(gmats)\ntuples = np.array(tuples)\n\n\nidx_reward = np.argwhere(tuples[:,-1] == 1).flatten()\nround = tuples[:idx_reward[-1]+2,:]\nround[-10:]\n\narray([[0, 0, 0],\n       [1, 1, 0],\n       [0, 0, 0],\n       [1, 0, 0],\n       [2, 0, 0],\n       [3, 0, 0],\n       [4, 0, 0],\n       [5, 0, 0],\n       [6, 0, 1],\n       [0, 0, 0]])\n\n\n\nplt.plot(gmats[0, 1, :10], label = 'reset')\nplt.plot(gmats[0, 0, :10], label = 'continue')\nplt.legend()\n\n<matplotlib.legend.Legend>\n\n\n\n\n\n\nplt.plot(np.bincount(np.array(counters).flatten()))\n\n\n\n\n\nplt.plot(gmats[-100:,1].mean(0))\n\n\n\n\n\nplt.plot(agent.h_matrix[1]/agent.h_matrix.sum(0))\n\n\n\n\n\n\nEnd test\n\nsource\n\n\n\nrun_agents_reset\n\n run_agents_reset (episodes, time_ep, N_agents, D=0.5, L=10.0,\n                   num_actions=2, size_state_space=array([100]),\n                   gamma_damping=1e-05, eta_glow_damping=0.1,\n                   g_update='s', initial_prob_distr=array([], shape=(2,\n                   0), dtype=float64), policy_type='standard',\n                   beta_softmax=3, fixed_policy=array([], shape=(2, 0),\n                   dtype=float64), max_no_H_update=1000, h_mat_allT=False,\n                   reset_after_reward=True, num_runs=None)\n\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nepisodes\n\n\n\n\n\ntime_ep\n\n\n\n\n\nN_agents\n\n\n\n\n\nD\nfloat\n0.5\n\n\n\nL\nfloat\n10.0\nEnvironment props\n\n\nnum_actions\nint\n2\nAgent props\n\n\nsize_state_space\nndarray\n[100]\n\n\n\ngamma_damping\nfloat\n1e-05\n\n\n\neta_glow_damping\nfloat\n0.1\n\n\n\ng_update\nstr\ns\n\n\n\ninitial_prob_distr\n\n[]\n\n\n\npolicy_type\nstr\nstandard\n\n\n\nbeta_softmax\nint\n3\n\n\n\nfixed_policy\n\n[]\n\n\n\nmax_no_H_update\nint\n1000\n\n\n\nh_mat_allT\nbool\nFalse\n\n\n\nreset_after_reward\nbool\nTrue\n\n\n\nnum_runs\nNoneType\nNone\nWhen we want N_agent != number of max cores, we use this to make few runsover the selected number of cores, given by N_agents.\n\n\n\nRun test\n\nfrom rl_opts.rl_framework_numba import Forager_efficient, ResetEnv_1D, train_loop_reset\n\n\nrews, mats = run_agents_reset(5, 100, 5, L = 2, num_runs=2, eta_glow_damping=0);\n\n\nrews\n\narray([[0.01, 0.06, 0.04, 0.04, 0.03],\n       [0.  , 0.01, 0.02, 0.03, 0.04],\n       [0.03, 0.06, 0.05, 0.07, 0.02],\n       [0.05, 0.  , 0.02, 0.02, 0.04],\n       [0.03, 0.03, 0.  , 0.01, 0.01],\n       [0.06, 0.06, 0.02, 0.02, 0.03],\n       [0.02, 0.01, 0.03, 0.01, 0.03],\n       [0.01, 0.03, 0.03, 0.06, 0.03],\n       [0.01, 0.02, 0.01, 0.02, 0.04],\n       [0.05, 0.04, 0.03, 0.  , 0.02]])\n\n\n\n\n2D\nWe need to have the 2D as a separate function because of an unification problem of numba. You can’t have the following, because it fails to properly compile, env_1d and env_2d have different variables and are then different objects.\nif dim == 1:\n    env = env_1d\nelse:\n    env = env_2d\n\nsource\n\n\nrun_agents_reset_2D\n\n run_agents_reset_2D (episodes, time_ep, N_agents, dist_target=10.0,\n                      radius_target=1.0, D=0.5, num_actions=2,\n                      size_state_space=array([100]), gamma_damping=1e-05,\n                      eta_glow_damping=0.1, initial_prob_distr=array([],\n                      shape=(2, 0), dtype=float64),\n                      policy_type='standard', beta_softmax=3,\n                      fixed_policy=array([], shape=(2, 0), dtype=float64),\n                      max_no_H_update=1000, h_mat_allT=False,\n                      when_save_h_mat=1, reset_after_reward=True,\n                      g_update='s')\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nepisodes\n\n\n\n\n\ntime_ep\n\n\n\n\n\nN_agents\n\n\n\n\n\ndist_target\nfloat\n10.0\n\n\n\nradius_target\nfloat\n1.0\n\n\n\nD\nfloat\n0.5\nEnvironment props\n\n\nnum_actions\nint\n2\nAgent props\n\n\nsize_state_space\nndarray\n[100]\n\n\n\ngamma_damping\nfloat\n1e-05\n\n\n\neta_glow_damping\nfloat\n0.1\n\n\n\ninitial_prob_distr\n\n[]\n\n\n\npolicy_type\nstr\nstandard\n\n\n\nbeta_softmax\nint\n3\n\n\n\nfixed_policy\n\n[]\n\n\n\nmax_no_H_update\nint\n1000\n\n\n\nh_mat_allT\nbool\nFalse\n\n\n\nwhen_save_h_mat\nint\n1\n\n\n\nreset_after_reward\nbool\nTrue\n\n\n\ng_update\nstr\ns\n\n\n\n\n\n\nTesting\n\nfrom rl_opts.rl_framework_numba import Forager_efficient, ResetEnv_2D\n\n\nrun_agents_reset_2D(int(1e2),int(1e2), 15, dist_target = 10, radius_target = 1, D = 1, \n                           size_state_space=np.array([3]),\n                          h_mat_allT=True, when_save_h_mat=5);\n\n\nh.shape\n\n(15, 20, 3)\n\n\n\nnp.unique(r.flatten(), return_counts=True)\n\n(array([0.   , 0.001, 0.002, 0.003, 0.004, 0.005, 0.006, 0.007, 0.008,\n        0.009]),\n array([47569, 53276, 31957, 12400,  3745,   844,   173,    32,     3,\n            1]))\n\n\n\npol = []\nfor ha in h:\n    pol.append(ha[0]/ha.sum(0))\npol = np.array(pol)\n\nplt.plot(r.mean(0))\n\n\n\n\n\nplt.plot(r.mean(0))"
  },
  {
    "objectID": "lib_nbs/dev_rl_framework_numba.html#for-moveresetenv",
    "href": "lib_nbs/dev_rl_framework_numba.html#for-moveresetenv",
    "title": "Numba / Dev Reinforcement learning framework",
    "section": "For MoveResetEnv",
    "text": "For MoveResetEnv\n\n2D\n\nFor base env\n\nsource\n\n\n\ntrain_loop_MoveReset2D\n\n train_loop_MoveReset2D (episodes, time_ep, agent, env, h_mat_allT=False,\n                         turn_0_when_reset=False)\n\n\nfrom rl_opts.rl_framework_numba import Forager_efficient, MoveResetEnv_2D\n\n\nagent = Forager_efficient(num_actions = 3, size_state_space = np.array([100, 100]),\n                          gamma_damping = np.array([1e-5]*3),\n                          eta_glow_damping = np.array([0.1,0.1,0.1]),\n                          max_no_H_update = 10)\nenv = MoveResetEnv_2D(dist_target = 5.1, # Distance from init position and target\n                      radius_target = 1.0, # Radius of the target\n                      agent_step = 1)\n\n\nrews, hmat = train_loop_MoveReset2D(episodes = 100, time_ep = int(1e4), agent = agent, env = env, h_mat_allT = False, turn_0_when_reset=True)\n# hmat\n\n\nNormal h_matrix update\n\nsource\n\n\n\nrun_agents_MoveReset2D\n\n run_agents_MoveReset2D (episodes, time_ep, N_agents, dist_target=5.1,\n                         radius_target=1.0, agent_step=1.0, num_actions=3,\n                         size_state_space=array([100, 100]),\n                         gamma_damping=1e-05, eta_glow_damping=0.1,\n                         initial_prob_distr=array([], shape=(2, 0),\n                         dtype=float64), policy_type='standard',\n                         beta_softmax=3, fixed_policy=array([], shape=(2,\n                         0), dtype=float64), max_no_H_update=1000,\n                         efficient_agent=False, h_mat_allT=False)\n\n\nr, h = run_agents_MoveReset2D(episodes = 1000, time_ep = 1000, N_agents = 10)\n\n\nh.shape\n\n(10, 3, 10000)\n\n\n\nmat = h.mean(0)\nsize_state_space = (100, 100)\n\nfig, ax = plt.subplots(1, 3, figsize = (9, 3), tight_layout = True)\n_, ax2 = plt.subplots(1, 4, figsize = (9, 3), tight_layout = True)\nfor i, action in enumerate(['continue', 'reset', 'turn']):\n    \n    ax[i].matshow((mat[i]/mat.sum(0)).reshape(size_state_space).transpose())\n    ax[i].set_ylabel('Rotate counter')\n    ax[i].set_title(action)\n\n    ax2[i].plot((mat[i]/mat.sum(0))[:30])\n    ax2[i].axhline(1/3, c = 'k', ls = '--', alpha = 0.2, zorder = -1)\n    ax2[-1].plot((mat[i]/mat.sum(0))[:30])\n    \nax[-1].set_xlabel('Reset counter')\n\nText(0.5, 0, 'Reset counter')\n\n\n\n\n\n\n\n\n\nplt.plot(r.mean(0))\n\n\n\n\n\nMulti param h_matrix update\n\ngamma_damping = 0.00001,\n                           eta_glow_damping = 0.1,\n\n\ngamma_damping = np.array([1e-5,2e-5, 3e-5])\neta_glow_damping = np.array([0.1, 0.2, 0.3])\n\nassert gamma_damping.shape[0] == 3 and eta_glow_damping.shape[0] == 3, \"\n\n\nsource\n\n\n\nrun_agents_MoveReset2D_multiparam\n\n run_agents_MoveReset2D_multiparam (episodes, time_ep, N_agents,\n                                    dist_target=5.1, radius_target=1.0,\n                                    agent_step=1.0, num_actions=3,\n                                    size_state_space=array([100, 100]),\n                                    gamma_damping=array([1.e-05, 2.e-05,\n                                    3.e-05]), eta_glow_damping=array([0.1,\n                                    0.2, 0.3]),\n                                    initial_prob_distr=array([], shape=(2,\n                                    0), dtype=float64),\n                                    policy_type='standard',\n                                    beta_softmax=3, fixed_policy=array([],\n                                    shape=(2, 0), dtype=float64),\n                                    max_no_H_update=1000,\n                                    efficient_agent=False,\n                                    h_mat_allT=False,\n                                    turn_0_when_reset=False)\n\n\nfrom rl_opts.rl_framework_numba import Forager_multi_param, train_loop_MoveReset2D\n\n\nr, h = run_agents_MoveReset2D_multiparam(episodes = 1000, time_ep = 1000, N_agents = 10)\n\n\nh.shape\n\n(10, 3, 10000)\n\n\n\nmat = h.mean(0)\nsize_state_space = (100, 100)\n\nfig, ax = plt.subplots(1, 3, figsize = (9, 3), tight_layout = True)\n_, ax2 = plt.subplots(1, 4, figsize = (9, 3), tight_layout = True)\nfor i, action in enumerate(['continue', 'reset', 'turn']):\n    \n    ax[i].matshow((mat[i]/mat.sum(0)).reshape(size_state_space).transpose())\n    ax[i].set_ylabel('Rotate counter')\n    ax[i].set_title(action)\n\n    ax2[i].plot((mat[i]/mat.sum(0))[:30])\n    ax2[i].axhline(1/3, c = 'k', ls = '--', alpha = 0.2, zorder = -1)\n    ax2[-1].plot((mat[i]/mat.sum(0))[:30])\n    \nax[-1].set_xlabel('Reset counter')\n\nText(0.5, 0, 'Reset counter')\n\n\n\n\n\n\n\n\n\nplt.plot(r.mean(0))\n\n\n\n\n\nFor others\n\nsource\n\n\n\ntrain_loop_MoveReset\n\n train_loop_MoveReset (episodes, time_ep, agent, env, h_mat_allT=False)\n\n\nsource\n\n\nrun_agents_MoveReset\n\n run_agents_MoveReset (episodes, time_ep, N_agents, Nt=100, L=100, r=0.5,\n                       lc=array([[1.],        [1.]]), num_agents=1,\n                       agent_step=1, destructive_targets=False,\n                       lc_distribution='constant',\n                       init_positions=array([[0., 0.]]), num_actions=2,\n                       size_state_space=array([100, 100]),\n                       gamma_damping=1e-05, eta_glow_damping=0.1,\n                       initial_prob_distr=array([], shape=(2, 0),\n                       dtype=float64), policy_type='standard',\n                       beta_softmax=3, fixed_policy=array([], shape=(2,\n                       0), dtype=float64), max_no_H_update=1000,\n                       efficient_agent=False, h_mat_allT=False)\n\n\n#### For compiling and checking\nrews , mats = run_agents_MoveReset(episodes = int(1e2), time_ep = int(1e3), num_actions = 3,\n                                N_agents = 1, size_state_space = np.array([10, 10]), \n                                efficient_agent=False, init_positions = np.array([[10, 10.1]]));\n\n\nrews\n\narray([[ 4.,  0.,  1., 16.,  1.,  0.,  0.,  1.,  0.,  4.,  0.,  4.,  0.,\n        13.,  0.,  1.,  2.,  0.,  0.,  0., 11., 30.,  6.,  0., 12.,  7.,\n        11.,  4.,  6.,  2.,  2.,  5.,  4.,  4.,  5.,  0.,  0., 29.,  2.,\n         6.,  2.,  2., 10.,  0.,  4.,  2.,  0.,  0., 15., 46.,  2.,  3.,\n         0.,  0., 15.,  0.,  1.,  6.,  3.,  4.,  3., 13.,  2.,  2.,  4.,\n         4.,  0.,  8.,  0.,  2.,  2.,  3.,  4.,  5.,  4.,  2.,  3., 14.,\n         2., 32.,  0., 17.,  7., 26.,  3.,  2.,  2.,  0.,  2.,  6.,  0.,\n         4.,  8.,  0.,  3.,  1., 26.,  3.,  3.,  5.]])\n\n\n\nHow to read the h-matrix:\n\nagent = Forager_efficient(num_actions = 3, size_state_space = np.array([100, 100]))\n\nConvoluted way of doing it (see below for better option):\n\nsize_state_space = np.array([100, 100])\nmat_2d = np.zeros(size_state_space)\nfor c_rotate in range(size_state_space[0]):\n    for c_reset in range(size_state_space[1]):\n        mat_2d[c_rotate, c_reset] = (mat[0]/mat.sum(0))[agent.percept_preprocess(np.array([c_rotate, c_reset]))]\n\n\nplt.matshow(mat_2d[:10, :10])\nplt.ylabel('Rotate counter')\nplt.xlabel('Reset counter')\n\nText(0.5, 0, 'Reset counter')\n\n\n\n\n\nBetter way: reshape + tranpose!!\n\nmat = mats[0]\n\n\nfig, ax = plt.subplots(1, 3, figsize = (9, 3))\nfor i, action in enumerate(['turn', 'reset', 'continue']):\n    \n    ax[i].matshow((mat[i]/mat.sum(0)).reshape(10,10).transpose()[:10,:10])\n    ax[i].set_ylabel('Rotate counter')\n    ax[i].set_title(action)\n    \nax[-1].set_xlabel('Reset counter')\n\nText(0.5, 0, 'Reset counter')"
  },
  {
    "objectID": "lib_nbs/rl_framework.html",
    "href": "lib_nbs/rl_framework.html",
    "title": "Classic version",
    "section": "",
    "text": "This notebook gathers the functions creating the RL framework proposed in our work. Namely, it can be use to generate both the foraging environment as well as the agents moving on them.\n\nEnvironment\nClass that defines the foraging environment\n\nnp.random.rand()\n\n0.4958686188975374\n\n\n\nsource\n\nTargetEnv\n\n TargetEnv (Nt, L, r, lc, agent_step=1, boundary_condition='periodic',\n            num_agents=1, high_den=5, destructive=False)\n\nClass defining the foraging environment. It includes the methods needed to place several agents to the world.\n\n\n\nProjective Simulation agent\n\nsource\n\nPSAgent\n\n PSAgent (num_actions, num_percepts_list, gamma_damping=0.0,\n          eta_glow_damping=0.0, policy_type='standard', beta_softmax=3,\n          initial_prob_distr=None, fixed_policy=None)\n\nBase class of a Reinforcement Learning agent based on Projective Simulation, with two-layered network. This class has been adapted from https://github.com/qic-ibk/projectivesimulation\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nnum_actions\nint >=1\n\nNumber of actions.\n\n\nnum_percepts_list\nlist of integers >=1, not nested\n\nCardinality of each category/feature of percept space.\n\n\ngamma_damping\nfloat\n0.0\nForgetting/damping of h-values at the end of each interaction. The default is 0.0.\n\n\neta_glow_damping\nfloat\n0.0\nControls the damping of glow; setting this to 1 effectively switches off glow. The default is 0.0.\n\n\npolicy_type\nstr\nstandard\nToggles the rule used to compute probabilities from h-values. See probability_distr. The default is ‘standard’.\n\n\nbeta_softmax\nint\n3\nProbabilities are proportional to exp(beta*h_value). If policy_type != ‘softmax’, then this is irrelevant. The default is 3.\n\n\ninitial_prob_distr\nNoneType\nNone\nIn case the user wants to change the initialization policy for the agent. This list contains, per percept, a list with the values of the initial h values for each action. The default is None.\n\n\nfixed_policy\nNoneType\nNone\nIn case the user wants to fix a policy for the agent. This list contains, per percept, a list with the values of the probabilities for each action. Example: Percept 0: fixed_policy[0] = [p(a0), p(a1), p(a2)] = [0.2, 0.3, 0.5], where a0, a1 and a2 are the three possible actions. The default is None.\n\n\n\n\n\n\nGeneral forager agent\n\nsource\n\nForager\n\n Forager (state_space, num_actions, visual_cone=3.141592653589793,\n          visual_radius=1.0, **kwargs)\n\nThis class extends the general PSAgent class and adapts it to the foraging scenario·\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nstate_space\nlist\n\nList where each entry is the state space of each perceptual feature.E.g. [state space of step counter, state space of density of successful neighbours].\n\n\nnum_actions\nint\n\nNumber of actions.\n\n\nvisual_cone\nfloat\n3.141592653589793\nVisual cone (angle, in radians) of the forager, useful in scenarios with ensembles of agents. The default is np.pi.\n\n\nvisual_radius\nfloat\n1.0\nRadius of the visual region, useful in scenarious with ensembles of agents. The default is 1.0.\n\n\nkwargs\nVAR_KEYWORD"
  },
  {
    "objectID": "lib_nbs/utils.html",
    "href": "lib_nbs/utils.html",
    "title": "Utils",
    "section": "",
    "text": "This notebook gathers multiple functions used as helpers in the library.\n\nHelpers for the environments\n\nsource\n\nisBetween_c_Vec\n\n isBetween_c_Vec (a, b, c, r)\n\nChecks whether point c is crossing the line formed with point a and b.\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\na\ntensor, shape = (1,2)\nPrevious position.\n\n\nb\ntensor, shape = (1,2)\nCurrent position.\n\n\nc\ntensor, shape = (Nt,2)\nPositions of all targets.\n\n\nr\nint/float\nTarget radius.\n\n\nReturns\narray of boolean values\nTrue at the indices of found targets.\n\n\n\n\nsource\n\n\ncoord_mod\n\n coord_mod (coord1, coord2, mod)\n\nComputes the distance difference between two coordinates, in a world with size ‘mod’ and periodic boundary conditions.\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\ncoord1\nvalue, np.array, tensor (can be shape=(n,1))\nFirst coordinate.\n\n\ncoord2\nnp.array, tensor – shape=(1,1)\nSecond coordinate, substracted from coord1.\n\n\nmod\nint\nWorld size.\n\n\nReturns\nfloat\nDistance difference (with correct sign, not absolute value).\n\n\n\n\nsource\n\n\nisBetween_c_Vec_nAgents\n\n isBetween_c_Vec_nAgents (a, b, c, r)\n\nChecks whether point c is crossing the line formed with point a and b. Code to run several agents in parallel.\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\na\narray, shape = (n,2)\nPrevious position of all n agents.\n\n\nb\narray, shape = (n,2)\nCurrent position of all n agents.\n\n\nc\narray, shape = (Nt,2)\nPositions of all targets.\n\n\nr\nfloat\nTarget radius\n\n\nReturns\narray of boolean values, shape = (Nt, n)\nTrue at the indices of found targets.\n\n\n\n\nsource\n\n\nget_encounters\n\n get_encounters (agent_previous_pos, agent_pos, target_positions, L, r)\n\nConsidering the agent walks, it checks whether the agent finds a target while walking the current step. Code to run several agents in parallel.\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\nagent_previous_pos\narray, shape = (n,2)\nPosition of the n agents before taking the step.\n\n\nagent_pos\narray, shape = (n,2)\nPosition of the n agents.\n\n\ntarget_positions\narray, shape = (Nt,2)\nPositions of the targets.\n\n\nL\nint\nWorld size.\n\n\nr\nfloat\nRadius of the targets.\n\n\nReturns\narray of boolean values, shape = (Nt, n)\nTrue at the indices of found targets.\n\n\n\n\n\n\nLoading data\n\nsource\n\nget_config\n\n get_config (config, config_path='configurations/learning/')\n\nGet the configuration file of the given experiment and config name (e.g. exp_0).\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nconfig\nstr\n\nConfig name (e.g. exp_0)\n\n\nconfig_path\nstr\nconfigurations/learning/\npath to configurations\n\n\nReturns\ndict\n\nDictionary with the parameters of the loaded configuration.\n\n\n\n\nsource\n\n\nget_policy\n\n get_policy (results_path, agent, episode)\n\nGets the policy of an agent at a given episode.\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\nresults_path\nstr\nPath of the folder from which to extract the data.\n\n\nagent\nint\nAgent index.\n\n\nepisode\nint\nEpisode.\n\n\nReturns\nlist\nPolicy.\n\n\n\n\nsource\n\n\nget_performance\n\n get_performance (results_path, agent_list, episode_list)\n\nExtract data with the efficiencies obtained in the postlearning analysis.\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\nresults_path\nstr\nPath of the folder from which to extract the data.\n\n\nagent_list\nlist\nList with the agent indices.\n\n\nepisode_list\nlist\nList with the episodes.\n\n\nReturns\nnp.array, shape=(len(agent_list), len(episode_list))\nAverage performances obtained by the agents in the postlearning analysis.\n\n\n\n\nsource\n\n\nget_opt\n\n get_opt (path, df)\n\nGet the highest efficiency obtained by the benchmark models and the corresponding parameters.\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\npath\nstr\nPath from which to get the data.\n\n\ndf\npanda dataframe\nDataframe with the results from the optimization with Tune.\n\n\nReturns\nlist\nEfficiency of each walk."
  },
  {
    "objectID": "lib_nbs/agents_numba.html",
    "href": "lib_nbs/agents_numba.html",
    "title": "Reinforcement learning agents",
    "section": "",
    "text": "This notebook gathers the functions creating different kinds of agents for foraging and target search in various scenarios, adapted for their use in the reinforcement learning paradigm."
  },
  {
    "objectID": "lib_nbs/agents_numba.html#random-sampling-from-array-with-probs",
    "href": "lib_nbs/agents_numba.html#random-sampling-from-array-with-probs",
    "title": "Reinforcement learning agents",
    "section": "Random sampling from array with probs",
    "text": "Random sampling from array with probs\n\nsource\n\nrand_choice_nb\n\n rand_choice_nb (arr, prob)\n\n:param arr: A 1D numpy array of values to sample from. :param prob: A 1D numpy array of probabilities for the given samples. :return: A random sample from the given array with a given probability."
  },
  {
    "objectID": "lib_nbs/agents_numba.html#for-resetenv",
    "href": "lib_nbs/agents_numba.html#for-resetenv",
    "title": "Reinforcement learning agents",
    "section": "For ResetEnv",
    "text": "For ResetEnv\n\nSearch loop\n\nsource\n\n\ntrain_loop_reset\n\n train_loop_reset (episodes, time_ep, agent, env, h_mat_allT=False,\n                   when_save_h_mat=1, reset_after_reward=True)\n\n\n\nLaunchers\n\nNote: we have to separate the launchers in 1D and 2D because of numba compilation, which would give errors due to the enviroments asking for different inputs.\n\n\n1D\n\nsource\n\n\n\nrun_agents_reset_1D\n\n run_agents_reset_1D (episodes, time_ep, N_agents, D=0.5, L=10.0,\n                      num_actions=2, size_state_space=array([100]),\n                      gamma_damping=1e-05, eta_glow_damping=0.1,\n                      g_update='s', initial_prob_distr=array([], shape=(2,\n                      0), dtype=float64), policy_type='standard',\n                      beta_softmax=3, fixed_policy=array([], shape=(2, 0),\n                      dtype=float64), max_no_H_update=1000,\n                      h_mat_allT=False, reset_after_reward=True,\n                      num_runs=None)\n\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nepisodes\n\n\n\n\n\ntime_ep\n\n\n\n\n\nN_agents\n\n\n\n\n\nD\nfloat\n0.5\n\n\n\nL\nfloat\n10.0\nEnvironment props\n\n\nnum_actions\nint\n2\nAgent props\n\n\nsize_state_space\nndarray\n[100]\n\n\n\ngamma_damping\nfloat\n1e-05\n\n\n\neta_glow_damping\nfloat\n0.1\n\n\n\ng_update\nstr\ns\n\n\n\ninitial_prob_distr\n\n[]\n\n\n\npolicy_type\nstr\nstandard\n\n\n\nbeta_softmax\nint\n3\n\n\n\nfixed_policy\n\n[]\n\n\n\nmax_no_H_update\nint\n1000\n\n\n\nh_mat_allT\nbool\nFalse\n\n\n\nreset_after_reward\nbool\nTrue\n\n\n\nnum_runs\nNoneType\nNone\nWhen we want N_agent != number of max cores, we use this to make few runsover the selected number of cores, given by N_agents.\n\n\n\n\n2D\n\nsource\n\n\n\nrun_agents_reset_2D\n\n run_agents_reset_2D (episodes, time_ep, N_agents, dist_target=10.0,\n                      radius_target=1.0, D=0.5, num_actions=2,\n                      size_state_space=array([100]), gamma_damping=1e-05,\n                      eta_glow_damping=0.1, initial_prob_distr=array([],\n                      shape=(2, 0), dtype=float64),\n                      policy_type='standard', beta_softmax=3,\n                      fixed_policy=array([], shape=(2, 0), dtype=float64),\n                      max_no_H_update=1000, h_mat_allT=False,\n                      when_save_h_mat=1, reset_after_reward=True,\n                      g_update='s', num_runs=None)\n\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nepisodes\n\n\n\n\n\ntime_ep\n\n\n\n\n\nN_agents\n\n\n\n\n\ndist_target\nfloat\n10.0\n\n\n\nradius_target\nfloat\n1.0\n\n\n\nD\nfloat\n0.5\nEnvironment props\n\n\nnum_actions\nint\n2\nAgent props\n\n\nsize_state_space\nndarray\n[100]\n\n\n\ngamma_damping\nfloat\n1e-05\n\n\n\neta_glow_damping\nfloat\n0.1\n\n\n\ninitial_prob_distr\n\n[]\n\n\n\npolicy_type\nstr\nstandard\n\n\n\nbeta_softmax\nint\n3\n\n\n\nfixed_policy\n\n[]\n\n\n\nmax_no_H_update\nint\n1000\n\n\n\nh_mat_allT\nbool\nFalse\n\n\n\nwhen_save_h_mat\nint\n1\n\n\n\nreset_after_reward\nbool\nTrue\n\n\n\ng_update\nstr\ns\n\n\n\nnum_runs\nNoneType\nNone\nWhen we want N_agent != number of max cores, we use this to make few runsover the selected number of cores, given by N_agents."
  }
]